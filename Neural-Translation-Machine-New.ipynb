{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "from torch.utils.data import Dataset,RandomSampler\n",
    "import operator\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "vocab_size = 85000\n",
    "hidden_size = 256\n",
    "# emb_size = 256\n",
    "MAX_LENGTH_1 = 100 # since 99% source sentence is <= 100\n",
    "# MAX_LENGTH_1 = max(len(pair[0].split(\" \")) for pair in pairs)\n",
    "# MAX_LENGTH_2 = max(len(pair[1].split(\" \")) for pair in pairs)\n",
    "dropout_p = 0.1\n",
    "teacher_forcing_ratio = 0.5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 88919\n",
      "chi 69127\n",
      "Build vocabulary by top 85000 frequent word...\n",
      "eng 85004\n",
      "['它 能 临近 96863 英尺   即 高 雷达 能 达到 的 最高 海拔    下午 四点 十二 十二分 二分   太阳 太阳神 阳神 号 已经 在 地球 大气 大气层 气层 的 百分 百分之 百分之九 百分之九十 百分之九十八 九十 九十八 十八 的 高度 了  ', 'Approaching a peak radar altitude of 96,863 feet , at 4 : 12 p.m. , Helios is standing on top of 98 percent of the Earth &apos;s atmosphere .']\n"
     ]
    }
   ],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0:\"PAD\", 1: \"SOS\", 2: \"EOS\",3:\"UNK\"}\n",
    "        self.n_words = 4  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        \n",
    "            \n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lang1_lines = open('data/iwslt-zh-en/train.tok.zh', encoding = 'utf-8').read().\\\n",
    "                    strip().split('\\n')\n",
    "    lang2_lines = open('data/iwslt-zh-en/train.tok.en', encoding = 'utf-8').read().\\\n",
    "                    strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [(lang1_lines[i] +',' + lang2_lines[i]).split(',',1) for i in range(len(lang1_lines))]\n",
    "\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def build_topwordVocab(lang, vocab_size):\n",
    "    print(\"Build vocabulary by top {} frequent word...\".format(vocab_size))\n",
    "    sorted_word2Count = sorted(lang.word2count.items(),\n",
    "        key=operator.itemgetter(1),\n",
    "        reverse=True)\n",
    "    sorted_words = [x[0] for x in sorted_word2Count[:vocab_size]]\n",
    "    \n",
    "    \n",
    "    lang.index2word = {}\n",
    "    lang.index2word[0] = \"PAD\"\n",
    "    lang.index2word[1] = \"SOS\"\n",
    "    lang.index2word[2] = \"EOS\"\n",
    "    lang.index2word[3] = \"UNK\"\n",
    "    \n",
    "    for ind, word in enumerate(sorted_words):\n",
    "            lang.index2word[ind + 4] = word\n",
    "            \n",
    "\n",
    "    lang.word2index = {}\n",
    "    for ind, word in enumerate(sorted_words):\n",
    "        lang.word2index[word] = ind + 4\n",
    "    \n",
    "    lang.n_words = len(lang.index2word)\n",
    "    \n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'chi')\n",
    "\n",
    "input_lang = build_topwordVocab(input_lang,vocab_size)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_word2Count = sorted(input_lang.word2count.items(),\n",
    "    key=operator.itemgetter(1),\n",
    "    reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 1429891),\n",
       " ('的', 272264),\n",
       " ('我', 83174),\n",
       " ('是', 71290),\n",
       " ('我们', 68696),\n",
       " ('在', 55727),\n",
       " ('了', 52197),\n",
       " ('你', 40824),\n",
       " ('这', 36049),\n",
       " ('一个', 33590),\n",
       " ('他们', 25256),\n",
       " ('和', 24670),\n",
       " ('有', 23982),\n",
       " ('它', 18993),\n",
       " ('就', 18825),\n",
       " ('这个', 18294),\n",
       " ('他', 17734),\n",
       " ('会', 17319),\n",
       " ('可以', 17137),\n",
       " ('都', 16825),\n",
       " ('也', 15945),\n",
       " ('这些', 15501),\n",
       " ('人', 12741),\n",
       " ('说', 12586),\n",
       " ('做', 12416),\n",
       " ('什么', 12362),\n",
       " ('就是', 12343),\n",
       " ('中', 11481),\n",
       " ('因为', 11107),\n",
       " ('让', 10825),\n",
       " ('对', 10766),\n",
       " ('很', 10459),\n",
       " ('如果', 10402),\n",
       " ('但', 10311),\n",
       " ('上', 10295),\n",
       " ('没有', 9852),\n",
       " ('把', 9841),\n",
       " ('到', 9490),\n",
       " ('而', 9428),\n",
       " ('所以', 9324),\n",
       " ('现在', 9296),\n",
       " ('能', 9207),\n",
       " ('被', 8608),\n",
       " ('这样', 8510),\n",
       " ('知道', 8472),\n",
       " ('问题', 8444),\n",
       " ('想', 8259),\n",
       " ('来', 8238),\n",
       " ('要', 8109),\n",
       " ('将', 8049),\n",
       " ('但是', 8025),\n",
       " ('她', 7786),\n",
       " ('不是', 7708),\n",
       " ('当', 7708),\n",
       " ('去', 7602),\n",
       " ('那', 7585),\n",
       " ('你们', 7544),\n",
       " ('世界', 7446),\n",
       " ('从', 7432),\n",
       " ('自己', 7293),\n",
       " ('不', 7257),\n",
       " ('非常', 7100),\n",
       " ('它们', 7049),\n",
       " ('个', 6861),\n",
       " ('一些', 6844),\n",
       " ('开始', 6719),\n",
       " ('需要', 6586),\n",
       " ('时候', 6568),\n",
       " ('并', 6564),\n",
       " ('看到', 6473),\n",
       " ('可能', 6285),\n",
       " ('然后', 6123),\n",
       " ('给', 6079),\n",
       " ('人们', 6014),\n",
       " ('好', 5768),\n",
       " ('里', 5734),\n",
       " ('着', 5720),\n",
       " ('那些', 5614),\n",
       " ('像', 5601),\n",
       " ('还', 5578),\n",
       " ('所有', 5525),\n",
       " ('用', 5492),\n",
       " ('与', 5455),\n",
       " ('一种', 5312),\n",
       " ('东西', 5216),\n",
       " ('已经', 5146),\n",
       " ('很多', 5090),\n",
       " ('工作', 5060),\n",
       " ('时', 4727),\n",
       " ('这种', 4724),\n",
       " ('年', 4691),\n",
       " ('认为', 4685),\n",
       " ('为', 4655),\n",
       " ('过', 4616),\n",
       " ('呢', 4589),\n",
       " ('地', 4559),\n",
       " ('更', 4545),\n",
       " ('发现', 4462),\n",
       " ('个人', 4359),\n",
       " ('这里', 4313),\n",
       " ('事情', 4297),\n",
       " ('研究', 4283),\n",
       " ('如何', 4272),\n",
       " ('发生', 4172),\n",
       " ('们', 4162),\n",
       " ('不同', 4151),\n",
       " ('或者', 4127),\n",
       " ('孩子', 4005),\n",
       " ('时间', 3997),\n",
       " ('一样', 3969),\n",
       " ('国家', 3943),\n",
       " ('生活', 3910),\n",
       " ('其他', 3815),\n",
       " ('大家', 3768),\n",
       " ('重要', 3761),\n",
       " ('最', 3710),\n",
       " ('还有', 3680),\n",
       " ('看', 3659),\n",
       " ('吗', 3657),\n",
       " ('那么', 3625),\n",
       " ('地方', 3618),\n",
       " ('通过', 3576),\n",
       " ('起来', 3545),\n",
       " ('美国', 3533),\n",
       " ('设计', 3533),\n",
       " ('而且', 3437),\n",
       " ('第一', 3411),\n",
       " ('所', 3393),\n",
       " ('告诉', 3362),\n",
       " ('改变', 3353),\n",
       " ('方式', 3277),\n",
       " ('部分', 3268),\n",
       " ('人类', 3209),\n",
       " ('为什么', 3202),\n",
       " ('能够', 3200),\n",
       " ('使用', 3180),\n",
       " ('任何', 3155),\n",
       " ('以及', 3144),\n",
       " ('并且', 3124),\n",
       " ('事实', 3119),\n",
       " ('又', 3117),\n",
       " ('关于', 3092),\n",
       " ('得', 3082),\n",
       " ('故事', 3057),\n",
       " ('或', 3021),\n",
       " ('不会', 3016),\n",
       " ('事', 2978),\n",
       " ('系统', 2968),\n",
       " ('吧', 2947),\n",
       " ('只是', 2937),\n",
       " ('一起', 2870),\n",
       " ('技术', 2859),\n",
       " ('怎么', 2852),\n",
       " ('新', 2846),\n",
       " ('以', 2840),\n",
       " ('进行', 2804),\n",
       " ('社会', 2770),\n",
       " ('应该', 2769),\n",
       " ('一下', 2752),\n",
       " ('下', 2746),\n",
       " ('数据', 2735),\n",
       " ('有人', 2708),\n",
       " ('因此', 2704),\n",
       " ('希望', 2695),\n",
       " ('多', 2694),\n",
       " ('科学', 2689),\n",
       " ('得到', 2685),\n",
       " ('不能', 2678),\n",
       " ('来说', 2672),\n",
       " ('那个', 2624),\n",
       " ('成为', 2622),\n",
       " ('必须', 2615),\n",
       " ('实际', 2614),\n",
       " ('觉得', 2593),\n",
       " ('真的', 2566),\n",
       " ('大', 2560),\n",
       " ('一次', 2557),\n",
       " ('信息', 2545),\n",
       " ('正在', 2529),\n",
       " ('方法', 2429),\n",
       " ('还是', 2426),\n",
       " ('一点', 2421),\n",
       " ('比', 2415),\n",
       " ('只', 2397),\n",
       " ('对于', 2372),\n",
       " ('为了', 2360),\n",
       " ('其中', 2353),\n",
       " ('跟', 2343),\n",
       " ('创造', 2339),\n",
       " ('生物', 2331),\n",
       " ('学家', 2325),\n",
       " ('发展', 2302),\n",
       " ('今天', 2293),\n",
       " ('喜欢', 2271),\n",
       " ('公司', 2269),\n",
       " ('同时', 2240),\n",
       " ('大脑', 2239),\n",
       " ('当然', 2226),\n",
       " ('每个', 2216),\n",
       " ('那里', 2207),\n",
       " ('这么', 2205),\n",
       " ('出来', 2182),\n",
       " ('生命', 2162),\n",
       " ('后', 2161),\n",
       " ('完全', 2157),\n",
       " ('向', 2118),\n",
       " ('存在', 2111),\n",
       " ('事实上', 2100),\n",
       " ('真正', 2094),\n",
       " ('解决', 2094),\n",
       " ('影响', 2092),\n",
       " ('使', 2091),\n",
       " ('经济', 2088),\n",
       " ('却', 2083),\n",
       " ('更多', 2082),\n",
       " ('另', 2077),\n",
       " ('帮助', 2077),\n",
       " ('一', 2073),\n",
       " ('甚至', 2058),\n",
       " ('再', 2056),\n",
       " ('作为', 2056),\n",
       " ('最后', 2048),\n",
       " ('每', 2047),\n",
       " ('之后', 2040),\n",
       " ('实际上', 2038),\n",
       " ('谢谢', 2036),\n",
       " ('来自', 2029),\n",
       " ('一直', 2009),\n",
       " ('下来', 1998),\n",
       " ('简单', 1984),\n",
       " ('想要', 1981),\n",
       " ('机器', 1972),\n",
       " ('了解', 1968),\n",
       " ('实验', 1968),\n",
       " ('的话', 1953),\n",
       " ('不知', 1948),\n",
       " ('城市', 1945),\n",
       " ('找到', 1938),\n",
       " ('决定', 1938),\n",
       " ('看看', 1924),\n",
       " ('情况', 1923),\n",
       " ('也许', 1918),\n",
       " ('其实', 1913),\n",
       " ('两个', 1908),\n",
       " ('由', 1891),\n",
       " ('问', 1876),\n",
       " ('于', 1871),\n",
       " ('组织', 1867),\n",
       " ('结果', 1863),\n",
       " ('动物', 1855),\n",
       " ('想法', 1847),\n",
       " ('过去', 1832),\n",
       " ('只有', 1830),\n",
       " ('如此', 1828),\n",
       " ('比如', 1826),\n",
       " ('艺术', 1809),\n",
       " ('意识', 1807),\n",
       " ('选择', 1790),\n",
       " ('细胞', 1785),\n",
       " ('才', 1785),\n",
       " ('相信', 1781),\n",
       " ('许多', 1779),\n",
       " ('环境', 1767),\n",
       " ('照片', 1766),\n",
       " ('整个', 1762),\n",
       " ('展示', 1757),\n",
       " ('一切', 1757),\n",
       " ('过程', 1749),\n",
       " ('有些', 1747),\n",
       " ('项目', 1745),\n",
       " ('仅仅', 1745),\n",
       " ('之前', 1734),\n",
       " ('不可', 1731),\n",
       " ('变得', 1723),\n",
       " ('产生', 1719),\n",
       " ('感觉', 1711),\n",
       " ('拥有', 1704),\n",
       " ('未来', 1699),\n",
       " ('同样', 1692),\n",
       " ('当时', 1691),\n",
       " ('政府', 1675),\n",
       " ('想象', 1671),\n",
       " ('方面', 1666),\n",
       " ('原因', 1653),\n",
       " ('年前', 1650),\n",
       " ('演讲', 1650),\n",
       " ('于是', 1639),\n",
       " ('是因为', 1622),\n",
       " ('全球', 1614),\n",
       " ('自然', 1613),\n",
       " ('之间', 1611),\n",
       " ('小', 1608),\n",
       " ('地球', 1602),\n",
       " ('不过', 1591),\n",
       " ('之', 1584),\n",
       " ('例子', 1583),\n",
       " ('理解', 1574),\n",
       " ('怎样', 1574),\n",
       " ('网络', 1558),\n",
       " ('出现', 1558),\n",
       " ('学习', 1557),\n",
       " ('不仅', 1552),\n",
       " ('之一', 1550),\n",
       " ('教育', 1533),\n",
       " ('建筑', 1526),\n",
       " ('无法', 1515),\n",
       " ('运动', 1515),\n",
       " ('能力', 1514),\n",
       " ('提供', 1511),\n",
       " ('朋友', 1504),\n",
       " ('而是', 1497),\n",
       " ('音乐', 1488),\n",
       " ('行为', 1486),\n",
       " ('非洲', 1485),\n",
       " ('变化', 1480),\n",
       " ('变成', 1480),\n",
       " ('其', 1466),\n",
       " ('计算', 1456),\n",
       " ('有趣', 1441),\n",
       " ('第二', 1436),\n",
       " ('学生', 1430),\n",
       " ('是否', 1425),\n",
       " ('成功', 1411),\n",
       " ('意思', 1405),\n",
       " ('关系', 1404),\n",
       " ('一位', 1393),\n",
       " ('思考', 1389),\n",
       " ('基本', 1384),\n",
       " ('中国', 1383),\n",
       " ('10', 1376),\n",
       " ('电脑', 1367),\n",
       " ('美元', 1367),\n",
       " ('科技', 1366),\n",
       " ('声音', 1362),\n",
       " ('岁', 1358),\n",
       " ('学校', 1347),\n",
       " ('令人', 1341),\n",
       " ('叫', 1329),\n",
       " ('一部', 1325),\n",
       " ('利用', 1325),\n",
       " ('20', 1319),\n",
       " ('身体', 1303),\n",
       " ('谁', 1302),\n",
       " ('高', 1298),\n",
       " ('控制', 1296),\n",
       " ('印度', 1291),\n",
       " ('历史', 1289),\n",
       " ('空间', 1287),\n",
       " ('意义', 1285),\n",
       " ('一天', 1280),\n",
       " ('安全', 1277),\n",
       " ('基因', 1273),\n",
       " ('带来', 1270),\n",
       " ('那样', 1267),\n",
       " ('经历', 1264),\n",
       " ('游戏', 1258),\n",
       " ('花', 1253),\n",
       " ('一件', 1251),\n",
       " ('可', 1243),\n",
       " ('复杂', 1242),\n",
       " ('女性', 1225),\n",
       " ('医生', 1219),\n",
       " ('制造', 1218),\n",
       " ('建立', 1215),\n",
       " ('各种', 1213),\n",
       " ('意味', 1211),\n",
       " ('世纪', 1210),\n",
       " ('月', 1208),\n",
       " ('我会', 1207),\n",
       " ('机会', 1207),\n",
       " ('更好', 1204),\n",
       " ('小时', 1194),\n",
       " ('完成', 1189),\n",
       " ('健康', 1187),\n",
       " ('特别', 1187),\n",
       " ('TED', 1180),\n",
       " ('讲', 1176),\n",
       " ('巨大', 1168),\n",
       " ('文化', 1164),\n",
       " ('语言', 1162),\n",
       " ('写', 1155),\n",
       " ('多少', 1153),\n",
       " ('宇宙', 1149),\n",
       " ('尝试', 1148),\n",
       " ('神经', 1144),\n",
       " ('确实', 1143),\n",
       " ('多数', 1139),\n",
       " ('意味着', 1134),\n",
       " ('感到', 1132),\n",
       " ('些', 1124),\n",
       " ('食物', 1123),\n",
       " ('没', 1123),\n",
       " ('解释', 1122),\n",
       " ('合作', 1122),\n",
       " ('治疗', 1121),\n",
       " ('主义', 1117),\n",
       " ('几个', 1116),\n",
       " ('一定', 1116),\n",
       " ('分享', 1115),\n",
       " ('大学', 1113),\n",
       " ('参与', 1112),\n",
       " ('保护', 1112),\n",
       " ('计划', 1110),\n",
       " ('活动', 1110),\n",
       " ('他人', 1110),\n",
       " ('最终', 1109),\n",
       " ('政治', 1103),\n",
       " ('进入', 1102),\n",
       " ('张', 1102),\n",
       " ('啊', 1098),\n",
       " ('请', 1090),\n",
       " ('模式', 1086),\n",
       " ('才能', 1081),\n",
       " ('机器人', 1080),\n",
       " ('中心', 1080),\n",
       " ('做到', 1079),\n",
       " ('有关', 1077),\n",
       " ('1', 1076),\n",
       " ('获得', 1075),\n",
       " ('太', 1075),\n",
       " ('注意', 1075),\n",
       " ('另外', 1072),\n",
       " ('5', 1072),\n",
       " ('大多', 1070),\n",
       " ('回到', 1069),\n",
       " ('价值', 1062),\n",
       " ('前', 1062),\n",
       " ('她们', 1059),\n",
       " ('工具', 1054),\n",
       " ('继续', 1054),\n",
       " ('重新', 1053),\n",
       " ('疾病', 1051),\n",
       " ('有点', 1050),\n",
       " ('地区', 1050),\n",
       " ('联系', 1050),\n",
       " ('作用', 1050),\n",
       " ('市场', 1043),\n",
       " ('接受', 1041),\n",
       " ('那时', 1040),\n",
       " ('第一个', 1039),\n",
       " ('三个', 1039),\n",
       " ('大概', 1037),\n",
       " ('有时', 1036),\n",
       " ('看起', 1035),\n",
       " ('内', 1034),\n",
       " ('里面', 1034),\n",
       " ('电影', 1033),\n",
       " ('看起来', 1031),\n",
       " ('一部分', 1030),\n",
       " ('目标', 1028),\n",
       " ('大约', 1025),\n",
       " ('该', 1019),\n",
       " ('曾', 1016),\n",
       " ('听', 1015),\n",
       " ('作品', 1014),\n",
       " ('听到', 1010),\n",
       " ('带', 1009),\n",
       " ('然而', 1009),\n",
       " ('叫做', 1006),\n",
       " ('家庭', 1005),\n",
       " ('更加', 1001),\n",
       " ('分钟', 1000),\n",
       " ('长', 998),\n",
       " ('想到', 997),\n",
       " ('生产', 994),\n",
       " ('是非', 991),\n",
       " ('包括', 990),\n",
       " ('2', 986),\n",
       " ('十年', 984),\n",
       " ('3', 979),\n",
       " ('不到', 978),\n",
       " ('病人', 977),\n",
       " ('领域', 977),\n",
       " ('几乎', 976),\n",
       " ('数字', 975),\n",
       " ('努力', 969),\n",
       " ('死亡', 967),\n",
       " ('来看', 967),\n",
       " ('首先', 967),\n",
       " ('答案', 964),\n",
       " ('力量', 963),\n",
       " ('处理', 963),\n",
       " ('产品', 962),\n",
       " ('在于', 960),\n",
       " ('做出', 954),\n",
       " ('制作', 951),\n",
       " ('实现', 948),\n",
       " ('大多数', 948),\n",
       " ('十分', 948),\n",
       " ('钱', 947),\n",
       " ('观察', 947),\n",
       " ('自我', 945),\n",
       " ('总是', 945),\n",
       " ('等', 944),\n",
       " ('像是', 943),\n",
       " ('曾经', 942),\n",
       " ('最好', 942),\n",
       " ('反应', 942),\n",
       " ('年代', 941),\n",
       " ('讨论', 938),\n",
       " ('关注', 934),\n",
       " ('最大', 934),\n",
       " ('爱', 931),\n",
       " ('正', 929),\n",
       " ('年轻', 925),\n",
       " ('汽车', 921),\n",
       " ('持续', 917),\n",
       " ('点', 915),\n",
       " ('吃', 914),\n",
       " ('人口', 913),\n",
       " ('相当', 913),\n",
       " ('每天', 912),\n",
       " ('一张', 911),\n",
       " ('结构', 910),\n",
       " ('精神', 909),\n",
       " ('别人', 908),\n",
       " ('事物', 906),\n",
       " ('自由', 903),\n",
       " ('有意', 901),\n",
       " ('一年', 900),\n",
       " ('太阳', 900),\n",
       " ('即使', 899),\n",
       " ('准备', 899),\n",
       " ('百万', 898),\n",
       " ('挑战', 897),\n",
       " ('发明', 896),\n",
       " ('感谢', 896),\n",
       " ('区域', 896),\n",
       " ('受到', 895),\n",
       " ('某种', 891),\n",
       " ('战争', 890),\n",
       " ('容易', 890),\n",
       " ('时代', 890),\n",
       " ('正是', 889),\n",
       " ('观众', 889),\n",
       " ('不要', 885),\n",
       " ('行动', 884),\n",
       " ('身上', 883),\n",
       " ('不断', 882),\n",
       " ('出', 881),\n",
       " ('认识', 881),\n",
       " ('大量', 881),\n",
       " ('服务', 876),\n",
       " ('或是', 876),\n",
       " ('兴趣', 876),\n",
       " ('病毒', 872),\n",
       " ('接下', 870),\n",
       " ('资源', 868),\n",
       " ('显示', 866),\n",
       " ('比较', 865),\n",
       " ('以前', 860),\n",
       " ('直到', 860),\n",
       " ('则', 859),\n",
       " ('程度', 850),\n",
       " ('现实', 848),\n",
       " ('一条', 847),\n",
       " ('接下来', 846),\n",
       " ('找', 845),\n",
       " ('视频', 844),\n",
       " ('明白', 838),\n",
       " ('实是', 838),\n",
       " ('用来', 838),\n",
       " ('真实', 836),\n",
       " ('代表', 836),\n",
       " ('社区', 835),\n",
       " ('分子', 834),\n",
       " ('主要', 833),\n",
       " ('模型', 832),\n",
       " ('或许', 831),\n",
       " ('别的', 831),\n",
       " ('目的', 830),\n",
       " ('女孩', 829),\n",
       " ('寻找', 829),\n",
       " ('知识', 829),\n",
       " ('观点', 826),\n",
       " ('增长', 826),\n",
       " ('类似', 826),\n",
       " ('工程', 825),\n",
       " ('一段', 822),\n",
       " ('放在', 820),\n",
       " ('能源', 820),\n",
       " ('根本', 818),\n",
       " ('越来', 818),\n",
       " ('很大', 817),\n",
       " ('不仅仅', 817),\n",
       " ('通常', 814),\n",
       " ('越来越', 813),\n",
       " ('难', 808),\n",
       " ('数学', 807),\n",
       " ('一只', 807),\n",
       " ('超过', 805),\n",
       " ('唯一', 805),\n",
       " ('30', 805),\n",
       " ('成', 804),\n",
       " ('创新', 804),\n",
       " ('是从', 802),\n",
       " ('理论', 802),\n",
       " ('其它', 800),\n",
       " ('基础', 799),\n",
       " ('一步', 799),\n",
       " ('科学家', 799),\n",
       " ('虽然', 797),\n",
       " ('算机', 797),\n",
       " ('计算机', 795),\n",
       " ('支持', 794),\n",
       " ('第一次', 793),\n",
       " ('大部', 792),\n",
       " ('考虑', 792),\n",
       " ('纽约', 789),\n",
       " ('海洋', 789),\n",
       " ('材料', 789),\n",
       " ('概念', 788),\n",
       " ('50', 782),\n",
       " ('办法', 782),\n",
       " ('上面', 781),\n",
       " ('传统', 780),\n",
       " ('有着', 778),\n",
       " ('一名', 777),\n",
       " ('国人', 777),\n",
       " ('目前', 775),\n",
       " ('大部分', 774),\n",
       " ('拿', 770),\n",
       " ('结束', 769),\n",
       " ('母亲', 768),\n",
       " ('电话', 766),\n",
       " ('百分', 763),\n",
       " ('女人', 763),\n",
       " ('图片', 762),\n",
       " ('充满', 761),\n",
       " ('形成', 760),\n",
       " ('进化', 757),\n",
       " ('哪里', 755),\n",
       " ('达到', 755),\n",
       " ('投资', 755),\n",
       " ('只要', 754),\n",
       " ('画', 752),\n",
       " ('接着', 752),\n",
       " ('机构', 750),\n",
       " ('医疗', 748),\n",
       " ('应用', 747),\n",
       " ('书', 747),\n",
       " ('有效', 747),\n",
       " ('记录', 746),\n",
       " ('到底', 740),\n",
       " ('飞行', 740),\n",
       " ('看着', 739),\n",
       " ('不管', 739),\n",
       " ('清楚', 738),\n",
       " ('人生', 737),\n",
       " ('实验室', 736),\n",
       " ('媒体', 736),\n",
       " ('来到', 735),\n",
       " ('尽管', 734),\n",
       " ('最近', 730),\n",
       " ('关键', 730),\n",
       " ('民主', 730),\n",
       " ('谈论', 728),\n",
       " ('英国', 728),\n",
       " ('个例', 728),\n",
       " ('手机', 728),\n",
       " ('男人', 726),\n",
       " ('测试', 725),\n",
       " ('不再', 725),\n",
       " ('角度', 725),\n",
       " ('错误', 724),\n",
       " ('物质', 723),\n",
       " ('电视', 722),\n",
       " ('愿意', 719),\n",
       " ('生在', 719),\n",
       " ('无论', 718),\n",
       " ('速度', 718),\n",
       " ('看见', 717),\n",
       " ('后来', 714),\n",
       " ('如', 714),\n",
       " ('买', 712),\n",
       " ('离开', 711),\n",
       " ('领导', 711),\n",
       " ('正确', 710),\n",
       " ('所有人', 707),\n",
       " ('水', 706),\n",
       " ('具有', 705),\n",
       " ('站', 705),\n",
       " ('移动', 704),\n",
       " ('第三', 704),\n",
       " ('儿童', 703),\n",
       " ('坐在', 701),\n",
       " ('设备', 700),\n",
       " ('等等', 700),\n",
       " ('不想', 699),\n",
       " ('电子', 697),\n",
       " ('仍然', 696),\n",
       " ('走', 696),\n",
       " ('能量', 696),\n",
       " ('提出', 695),\n",
       " ('几年', 694),\n",
       " ('记得', 693),\n",
       " ('面对', 691),\n",
       " ('相关', 688),\n",
       " ('者', 688),\n",
       " ('导致', 686),\n",
       " ('用于', 686),\n",
       " ('探索', 684),\n",
       " ('的确', 684),\n",
       " ('试图', 684),\n",
       " ('证明', 684),\n",
       " ('癌症', 683),\n",
       " ('不得', 683),\n",
       " ('住', 682),\n",
       " ('名字', 682),\n",
       " ('物种', 681),\n",
       " ('连接', 681),\n",
       " ('4', 680),\n",
       " ('数量', 678),\n",
       " ('以为', 678),\n",
       " ('父亲', 678),\n",
       " ('回答', 677),\n",
       " ('伟大', 676),\n",
       " ('内容', 675),\n",
       " ('互联', 674),\n",
       " ('管理', 669),\n",
       " ('图', 669),\n",
       " ('样子', 666),\n",
       " ('手术', 665),\n",
       " ('造成', 662),\n",
       " ('共同', 661),\n",
       " ('而言', 660),\n",
       " ('百分之', 659),\n",
       " ('已', 657),\n",
       " ('增加', 656),\n",
       " ('感受', 655),\n",
       " ('保持', 654),\n",
       " ('15', 653),\n",
       " ('建造', 651),\n",
       " ('例如', 650),\n",
       " ('联网', 650),\n",
       " ('植物', 650),\n",
       " ('这儿', 647),\n",
       " ('足够', 645),\n",
       " ('这件', 645),\n",
       " ('鼓掌', 645),\n",
       " ('至少', 643),\n",
       " ('父母', 642),\n",
       " ('那种', 641),\n",
       " ('范围', 641),\n",
       " ('企业', 641),\n",
       " ('开发', 639),\n",
       " ('很重', 639),\n",
       " ('老师', 639),\n",
       " ('宗教', 637),\n",
       " ('功能', 636),\n",
       " ('团队', 635),\n",
       " ('过来', 634),\n",
       " ('飞机', 634),\n",
       " ('利亚', 633),\n",
       " ('人员', 631),\n",
       " ('形式', 631),\n",
       " ('方向', 630),\n",
       " ('事件', 630),\n",
       " ('一项', 629),\n",
       " ('上去', 628),\n",
       " ('当地', 628),\n",
       " ('减少', 628),\n",
       " ('网站', 627),\n",
       " ('交流', 627),\n",
       " ('本身', 626),\n",
       " ('好像', 626),\n",
       " ('现象', 626),\n",
       " ('直接', 625),\n",
       " ('先', 623),\n",
       " ('分析', 622),\n",
       " ('物理', 621),\n",
       " ('公共', 621),\n",
       " ('以后', 620),\n",
       " ('随着', 619),\n",
       " ('快乐', 619),\n",
       " ('理学', 617),\n",
       " ('这项', 617),\n",
       " ('要求', 617),\n",
       " ('全世界', 616),\n",
       " ('明显', 616),\n",
       " ('一场', 615),\n",
       " ('互联网', 615),\n",
       " ('某些', 613),\n",
       " ('基本上', 612),\n",
       " ('气候', 612),\n",
       " ('使得', 611),\n",
       " ('危险', 610),\n",
       " ('表现', 608),\n",
       " ('商业', 608),\n",
       " ('正如', 605),\n",
       " ('只能', 604),\n",
       " ('么', 603),\n",
       " ('什么样', 603),\n",
       " ('很快', 603),\n",
       " ('突然', 603),\n",
       " ('药物', 602),\n",
       " ('不用', 601),\n",
       " ('其他人', 599),\n",
       " ('不得不', 598),\n",
       " ('石油', 598),\n",
       " ('方案', 597),\n",
       " ('心理', 596),\n",
       " ('除了', 595),\n",
       " ('规模', 593),\n",
       " ('小孩', 593),\n",
       " ('100', 593),\n",
       " ('下面', 593),\n",
       " ('不一', 592),\n",
       " ('最重', 592),\n",
       " ('每年', 590),\n",
       " ('拍摄', 589),\n",
       " ('似的', 589),\n",
       " ('经过', 589),\n",
       " ('词', 589),\n",
       " ('道德', 589),\n",
       " ('永远', 587),\n",
       " ('周围', 587),\n",
       " ('尼亚', 586),\n",
       " ('从来', 582),\n",
       " ('一般', 582),\n",
       " ('现代', 580),\n",
       " ('肯定', 578),\n",
       " ('失去', 578),\n",
       " ('起', 577),\n",
       " ('刚刚', 577),\n",
       " ('确定', 576),\n",
       " ('困难', 575),\n",
       " ('组成', 574),\n",
       " ('亿', 574),\n",
       " ('同一', 573),\n",
       " ('暴力', 573),\n",
       " ('试', 572),\n",
       " ('万', 568),\n",
       " ('讲述', 568),\n",
       " ('正常', 567),\n",
       " ('天', 567),\n",
       " ('消息', 567),\n",
       " ('难以', 566),\n",
       " ('收入', 566),\n",
       " ('可是', 565),\n",
       " ('经常', 564),\n",
       " ('化学', 564),\n",
       " ('位置', 563),\n",
       " ('上帝', 562),\n",
       " ('传播', 560),\n",
       " ('眼睛', 560),\n",
       " ('程序', 559),\n",
       " ('哦', 558),\n",
       " ('如今', 558),\n",
       " ('DNA', 557),\n",
       " ('地图', 557),\n",
       " ('真是', 557),\n",
       " ('由于', 556),\n",
       " ('表达', 555),\n",
       " ('严重', 554),\n",
       " ('预测', 553),\n",
       " ('相同', 553),\n",
       " ('时刻', 551),\n",
       " ('6', 548),\n",
       " ('学到', 548),\n",
       " ('定义', 547),\n",
       " ('欧洲', 547),\n",
       " ('系列', 545),\n",
       " ('似乎', 545),\n",
       " ('说明', 543),\n",
       " ('是不是', 543),\n",
       " ('英里', 543),\n",
       " ('思想', 541),\n",
       " ('会议', 541),\n",
       " ('幸福', 540),\n",
       " ('根据', 538),\n",
       " ('所说', 537),\n",
       " ('加入', 537),\n",
       " ('状态', 536),\n",
       " ('那儿', 536),\n",
       " ('体验', 535),\n",
       " ('造出', 534),\n",
       " ('软件', 533),\n",
       " ('多年', 532),\n",
       " ('遇到', 532),\n",
       " ('描述', 532),\n",
       " ('常有', 530),\n",
       " ('担心', 529),\n",
       " ('不在', 528),\n",
       " ('特', 528),\n",
       " ('及', 527),\n",
       " ('不错', 527),\n",
       " ('生态', 526),\n",
       " ('相互', 526),\n",
       " ('哪', 524),\n",
       " ('面临', 524),\n",
       " ('表演', 523),\n",
       " ('提高', 523),\n",
       " ('生存', 523),\n",
       " ('以来', 522),\n",
       " ('快', 521),\n",
       " ('想想', 520),\n",
       " ('称为', 519),\n",
       " ('医院', 519),\n",
       " ('阶段', 519),\n",
       " ('效果', 518),\n",
       " ('介绍', 518),\n",
       " ('值得', 517),\n",
       " ('新闻', 516),\n",
       " ('非常感谢', 515),\n",
       " ('上有', 515),\n",
       " ('革命', 515),\n",
       " ('一半', 515),\n",
       " ('信号', 515),\n",
       " ('试验', 515),\n",
       " ('便', 514),\n",
       " ('回来', 514),\n",
       " ('感兴', 514),\n",
       " ('感兴趣', 514),\n",
       " ('要是', 512),\n",
       " ('绿色', 512),\n",
       " ('梦想', 511),\n",
       " ('参加', 510),\n",
       " ('任务', 510),\n",
       " ('各位', 508),\n",
       " ('国际', 507),\n",
       " ('嗯', 507),\n",
       " ('平均', 505),\n",
       " ('视觉', 505),\n",
       " ('聪明', 504),\n",
       " ('法律', 504),\n",
       " ('妈妈', 503),\n",
       " ('调查', 502),\n",
       " ('40', 502),\n",
       " ('专家', 501),\n",
       " ('里斯', 500),\n",
       " ('家', 500),\n",
       " ('谈', 500),\n",
       " ('多么', 499),\n",
       " ('笑', 499),\n",
       " ('训练', 498),\n",
       " ('标准', 497),\n",
       " ('术家', 497),\n",
       " ('尤其', 497),\n",
       " ('艺术家', 496),\n",
       " ('光', 496),\n",
       " ('提到', 495),\n",
       " ('恐惧', 495),\n",
       " ('自身', 495),\n",
       " ('全部', 494),\n",
       " ('本书', 494),\n",
       " ('一旦', 494),\n",
       " ('奇怪', 493),\n",
       " ('图像', 493),\n",
       " ('下去', 493),\n",
       " ('发出', 493),\n",
       " ('间', 492),\n",
       " ('风险', 492),\n",
       " ('一家', 491),\n",
       " ('各地', 491),\n",
       " ('有机', 489),\n",
       " ('房间', 489),\n",
       " ('开放', 489),\n",
       " ('收集', 488),\n",
       " ('即', 487),\n",
       " ('车', 487),\n",
       " ('完美', 485),\n",
       " ('经验', 485),\n",
       " ('斯', 483),\n",
       " ('危机', 483),\n",
       " ('水平', 482),\n",
       " ('8', 481),\n",
       " ('某个', 481),\n",
       " ('帮', 481),\n",
       " ('死', 479),\n",
       " ('银行', 479),\n",
       " ('普通', 479),\n",
       " ('运用', 478),\n",
       " ('记忆', 478),\n",
       " ('创作', 476),\n",
       " ('玩', 476),\n",
       " ('吸引', 475),\n",
       " ('妇女', 475),\n",
       " ('运作', 474),\n",
       " ('性', 472),\n",
       " ('还要', 471),\n",
       " ('星期', 471),\n",
       " ('这次', 471),\n",
       " ('到达', 471),\n",
       " ('森林', 471),\n",
       " ('您', 471),\n",
       " ('兴奋', 470),\n",
       " ('种', 470),\n",
       " ('出生', 469),\n",
       " ('一代', 469),\n",
       " ('不多', 469),\n",
       " ('12', 469),\n",
       " ('男孩', 468),\n",
       " ('生物学', 468),\n",
       " ('失败', 468),\n",
       " ('刚', 467),\n",
       " ('物体', 467),\n",
       " ('英尺', 466),\n",
       " ('土地', 465),\n",
       " ('医学', 464),\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_word2Count ###标点符号排第一 之后要改掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    idxs = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            idxs.append(lang.word2index[word])\n",
    "        except KeyError:\n",
    "            idxs.append(3)  # 3 is the id of 'UNK'\n",
    "    return idxs\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "\n",
    "\n",
    "class VocabDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "#         pairs = [tensorsFromPair(pair) for pair in pairs]\n",
    "#         self.source_sent_list = [i[0] for i in pairs]\n",
    "#         self.target_sent_list = [i[1] for i in pairs]\n",
    "\n",
    "        self.source_sent_list = [indexesFromSentence(input_lang,pair[0]) for pair in pairs]\n",
    "        self.target_sent_list = [indexesFromSentence(output_lang,pair[1]) for pair in pairs]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_sent_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        token1_idx = self.source_sent_list[key][:MAX_LENGTH_1]\n",
    "        token2_idx = self.target_sent_list[key][:MAX_LENGTH_1]\n",
    "        return [token1_idx,token2_idx, len(token1_idx), len(token2_idx)]\n",
    "\n",
    "    \n",
    "def Vocab_collate_func(batch):\n",
    "    source_sent_list = []\n",
    "    target_sent_list = []\n",
    "    source_len_list = []\n",
    "    target_len_list = []\n",
    "\n",
    "    for datum in batch:   ### batch = sample\n",
    "        source_len_list.append(datum[2])\n",
    "        target_len_list.append(datum[3])\n",
    "\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        \n",
    "        # source sentence processing\n",
    "        padded_source = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_LENGTH_1-datum[2])),          ### 0代表左边没有pad,右边的值代表右边pad的个数\n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        source_sent_list.append(padded_source)\n",
    "        \n",
    "        # target sentence processing\n",
    "        padded_target = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_LENGTH_1-datum[3])),          ### 0代表左边没有pad,右边的值代表右边pad的个数\n",
    "                                mode=\"constant\", constant_values=PAD_token)\n",
    "        target_sent_list.append(padded_target)\n",
    "        \n",
    "    return [torch.tensor(source_sent_list,device = device), \n",
    "            torch.tensor(target_sent_list,device = device),\n",
    "            torch.LongTensor(source_len_list,device = device), \n",
    "            torch.LongTensor(target_len_list,device = device)]\n",
    "\n",
    "train_dataset = VocabDataset(pairs)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=Vocab_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True) # embedding size = hidden size\n",
    "        self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "    def initHidden(self,BATCH_SIZE):\n",
    "        return torch.zeros(2, BATCH_SIZE, self.hidden_size, device=device) # return (2,1,hidden_size) 2 due to bidirection\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, BATCH_SIZE, -1)  # input is just one token at timpstep t\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)  \n",
    "        # (seq_len, batch, num_directions * hidden_size) and (num_layers * num_directions, batch, hidden_size)\n",
    "        output = self.fc1(output)\n",
    "        return output, hidden\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH_1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, BATCH_SIZE, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "#         embedded: torch.Size([1, 32, 256])\n",
    "#         hidden: torch.Size([1, 32, 256])\n",
    "        \n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)   \n",
    "#         attn_weights:torch.Size([32, 100])\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1),\n",
    "                                 encoder_outputs.transpose(0,1))\n",
    "#         encoder_outputs: 100*32*512 attn_applied: 32*1*512\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied.transpose(0,1)[0]), 1)\n",
    "        # output: 32*768\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        # output 1*32*256\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "#         output: torch.Size([32, 69126])\n",
    "#         hidden: torch.Size([1, 32, 256])\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is just one sentence input, could be batchlized \n",
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion, mask = None):\n",
    "    encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
    "    encoder_optimizer.zero_grad()  # zero out the accumulated gradient over mini-batch\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0) # length of source sentence\n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(target_length, BATCH_SIZE, encoder.hidden_size, device=device) \n",
    "    # (seq_length, BATCH_SIZE,hidden_size*2) 2 due to bidirection\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # feed-forward layer resulting encoder outputs, ei refers to each word token in input sentence\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)  \n",
    "        # encoder_output: torch.Size([1, 32, 512]) encoder_hidden: torch.Size([2, 32, 256])\n",
    "        encoder_outputs[ei] = encoder_output[0] \n",
    "    # change the shape of encoder output to fit into decoder \n",
    "    encoder_hidden = nn.Linear(2*hidden_size,hidden_size)(\n",
    "        torch.cat((encoder_hidden[0],encoder_hidden[1]),dim = 1)).unsqueeze(0)\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]*32], device=device)  # decoder_input: torch.Size([1, 32])\n",
    "    # init decoder hidden \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "            loss += temp_loss * mask[di:di+1].float()  \n",
    "            ave_loss = loss.sum()/BATCH_SIZE \n",
    "            \n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # decoder_input: torch.Size([1, 32])\n",
    "            # decoder_hidden: torch.Size([1, 32, 256]) 1 token * batch * hidden size\n",
    "            # encoder_outputs: torch.Size([100, 32, 512])\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            # topv: 32*1\n",
    "            # topi: 32*1\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            # decoder_input: 32\n",
    "            # target_tensor: 100*32\n",
    "            # decoder_output: 32*69127 \n",
    "            temp_loss = criterion(decoder_output, target_tensor[di])\n",
    "            loss += temp_loss * mask[di:di+1].float()\n",
    "            # loss size 1*32\n",
    "            ave_loss = loss.sum()/BATCH_SIZE  \n",
    "            \n",
    "    ave_loss.backward()\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.step()   # update parameters\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return ave_loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=100, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss(reduce = False) ##!!!!!!!!!!1 这个loss是否要换成crossentropy\n",
    "\n",
    "    for epoch in range(1, n_iters + 1):\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        for i, (input_sentences, target_sentences,len1,len2) in enumerate(train_loader): \n",
    "            input_tensor = input_sentences.transpose(0,1)   # 32*100 to 100*32\n",
    "            target_tensor = target_sentences.transpose(0,1)\n",
    "            mask = target_tensor.ge(1)   # 100 * 32\n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion, mask = mask)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "            if i > 0 and i % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}'.format(\n",
    "                    timeSince(start, i + 1/len(train_loader)), epoch, n_iters, i, \n",
    "                    len(train_loader),print_loss_avg))\n",
    "\n",
    "            if i > 0 and i % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "                \n",
    "        print(plot_losses)\n",
    "        showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH_1):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden(BATCH_SIZE)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "    \n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hp/anaconda/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-89085091c904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saved_model/encoder_hiddenSize{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-43056b66820c>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 100 * 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 17\u001b[0;31m                          decoder, encoder_optimizer, decoder_optimizer, criterion, mask = mask)\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-4132a1bccf9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, mask)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mave_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mave_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(input_lang.n_words,hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 3, print_every=1,plot_every=1)\n",
    "\n",
    "torch.save(encoder1.state_dict(), \"saved_model/encoder_hiddenSize{}\".format(hidden_size))\n",
    "torch.save(attn_decoder1.state_dict(), \"saved_model/attn_decoder_hiddenSize{}\".format(hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 我们 谈 的 不是 哈利 哈利波 波特 的 尾巴 下   它 是 在 左边  \n",
      "= We &apos;re not talking about the Harry Potter end , right at the left side .\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 256, got 8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-22fdad7aedca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-bedbf4c1b306>\u001b[0m in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bedbf4c1b306>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             encoder_output, encoder_hidden = encoder(input_tensor[ei],\n\u001b[0;32m---> 11\u001b[0;31m                                                      encoder_hidden)\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-3e7432d50ff1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# input is just one token at timpstep t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# (seq_len, batch, num_directions * hidden_size) and (num_layers * num_directions, batch, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    128\u001b[0m             raise RuntimeError(\n\u001b[1;32m    129\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[0;32m--> 130\u001b[0;31m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_input_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 256, got 8"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
