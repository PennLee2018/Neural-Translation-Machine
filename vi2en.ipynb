{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vi2en.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "code",
        "id": "QQTKmCqMLfv2",
        "outputId": "c254b3b0-09e4-4abb-945c-290d2c4c1a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Tvqp0ME6R1u1",
        "outputId": "2029d7ab-415c-4616-f196-f4cc3499ddc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install sacrebleu"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.2.12)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O47m-P3N1DJw",
        "outputId": "c6043a9c-bdab-464d-af84-94891877e34d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.1\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xGw8DDZT0d5X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import operator\n",
        "from torch.utils.data import Dataset\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from sacrebleu import corpus_bleu, TOKENIZERS, DEFAULT_TOKENIZER\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "from torch.nn import functional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EYDlc9JJ0d5Z",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "hidden_size = 1024\n",
        "dropout_p = 0.1\n",
        "teacher_forcing_ratio = 1\n",
        "BATCH_SIZE = 64\n",
        "MIN_LENGTH = 1\n",
        "MAX_LENGTH = 55\n",
        "source_vocab_size = 19000\n",
        "target_vocab_size = 22000\n",
        "n_layers = 4\n",
        "lr_rate_en = 0.0001\n",
        "lr_rate_de = 0.0005\n",
        "lr_decay = False\n",
        "gamma_encoder = 0.9\n",
        "gamma_decoder = 0.9\n",
        "n_epochs = 20\n",
        "plot_every = 100\n",
        "print_every = 100\n",
        "evaluate_every = 500\n",
        "attn_model = 'dot'\n",
        "Attention = True\n",
        "search_method = 'greedy'\n",
        "beam_size = 10\n",
        "n_best = 5\n",
        "dynamic_sentence_length = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UWhY1ARnf0mM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def filter_pairs(pairs):\n",
        "    filtered_pairs = []\n",
        "    for pair in pairs:\n",
        "        if len(pair[0].split()) >= MIN_LENGTH and len(pair[0].split()) <= MAX_LENGTH \\\n",
        "            and len(pair[1].split()) >= MIN_LENGTH and len(pair[1].split()) <= MAX_LENGTH:\n",
        "            filtered_pairs.append(pair)\n",
        "    return filtered_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0-6BiXr1MaHT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    \"\"\"\n",
        "    Code paraphrased from \n",
        "    https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/masked_cross_entropy.py\n",
        "    \"\"\"\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    batch_size = sequence_length.size(0)\n",
        "    seq_range = torch.arange(0, max_len).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len).contiguous()\n",
        "    seq_range_expand = seq_range_expand.to(device)\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand\n",
        "\n",
        "\n",
        "\n",
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = torch.LongTensor(length).to(device)\n",
        "    \"\"\"\n",
        "    Code paraphrased from \n",
        "    https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/masked_cross_entropy.py\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    log_probs_flat = functional.log_softmax(logits_flat, dim=1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum()\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OESV2zjg0d5c"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "X0G2eDBP0d5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4403fcd5-4213-459d-a0cd-022163991662"
      },
      "cell_type": "code",
      "source": [
        "class Lang:\n",
        "    '''\n",
        "    Part of the codes are paraphrased from\n",
        "    https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "    '''\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
        "        self.n_words = 4  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "\n",
        "other_punctuations = string.punctuation.replace('!','').replace('.','').replace('?','').replace(',','').replace('-','')\n",
        "\n",
        "def normalizeEnString(s):\n",
        "#     s = unicodeToAscii(s.strip())\n",
        "    s = s.replace(\"&apos\", \"\").replace(\"&quot\",\"\")\n",
        "#    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z,.!?0-9]+\", r\" \", s)\n",
        "    s = re.sub( '\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def normalizeViString(s):\n",
        "    s = s.replace(\"&apos\", \"\").replace(\"&quot\",\"\").replace(\"_\",\"\").replace('-','')\n",
        "    s = re.sub(r'[{}]'.format(other_punctuations), '', s)\n",
        "    s = re.sub( '\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "print(normalizeEnString(\"It &apos;s very pretty , and it has rapidly started to overgrow the \\\n",
        "                  once very rich biodiversity of the northwestern Mediterranean .\"))\n",
        "normalizeViString('húng_ta đã đạt được điều này qua công_nghệ đến_mức mili - giây . và điều này cho_phép Peter nhìn_thấy não_bộ anh ấy dưới thời_gian thực khi anh ta ở trong máy_quét .')               "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It s very pretty , and it has rapidly started to overgrow the once very rich biodiversity of the northwestern Mediterranean .\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'húngta đã đạt được điều này qua côngnghệ đếnmức mili giây . và điều này chophép Peter nhìnthấy nãobộ anh ấy dưới thờigian thực khi anh ta ở trong máyquét .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "qj85Ak3cpsf9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# open('/content/drive/My Drive/iwslt-vi-en/{}.tok.vi'.format('train')).read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdkyHn8Ff0mX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b6a92b5c-1215-4212-8ef0-b01c9870cf57"
      },
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, data='train'):\n",
        "    '''\n",
        "    Part of the codes are paraphrased from\n",
        "    https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "    '''\n",
        "    #data: train/dev/test\n",
        "    print(\"Reading lines...\")\n",
        "    # Read the file and split into lines\n",
        "    zh_lines = open('/content/drive/My Drive/iwslt-vi-en/{}.tok.vi'.format(data)).read().split('\\n')\n",
        "    en_lines = open('/content/drive/My Drive/iwslt-vi-en/{}.tok.en'.format(data)).read().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeViString(element[0]), normalizeEnString(element[1])] for element in zip(zh_lines, en_lines)]\n",
        "\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def prepareData(lang1, lang2):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filter_pairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "def build_topwordVocab(lang, vocab_size):\n",
        "    print(\"Build vocabulary by top {} frequent word...\".format(vocab_size))\n",
        "    sorted_word2Count = sorted(lang.word2count.items(),\n",
        "        key=operator.itemgetter(1),\n",
        "        reverse=True)\n",
        "    sorted_words = [x[0] for x in sorted_word2Count[:vocab_size]]\n",
        "    \n",
        "    lang.word2index = {}\n",
        "\n",
        "    for ind, word in enumerate(sorted_words):\n",
        "            lang.word2index[word] = ind + 4\n",
        "    lang.index2word = {}\n",
        "    lang.index2word[0] = \"<PAD>\"\n",
        "    lang.index2word[1] = \"<SOS>\"\n",
        "    lang.index2word[2] = \"<EOS>\"\n",
        "    lang.index2word[3] = \"<UNK>\"\n",
        "\n",
        "    for ind, word in enumerate(sorted_words):\n",
        "        lang.index2word[ind + 4] = word\n",
        "    \n",
        "    lang.n_words = len(lang.index2word)\n",
        "    \n",
        "    print(lang.name, lang.n_words)\n",
        "    return lang\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('vi', 'eng')\n",
        "\n",
        "input_lang = build_topwordVocab(input_lang,vocab_size=source_vocab_size)\n",
        "output_lang = build_topwordVocab(output_lang, vocab_size=target_vocab_size)\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 133318 sentence pairs\n",
            "Trimmed to 128908 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "vi 40038\n",
            "eng 46550\n",
            "Build vocabulary by top 19000 frequent word...\n",
            "vi 19004\n",
            "Build vocabulary by top 22000 frequent word...\n",
            "eng 22004\n",
            "['Mọi việc tiếntriển tốtđẹp từ đó , và chúngtôi trởthành những nhà khoahọc tuyệtvời .', 'Everything went well from there , and we became amazing scientists .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "I_PL2_xh0d5y",
        "outputId": "7ebf0f55-6556-48b6-a48b-447603d192e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "_, _, val_pairs = readLangs('vi', 'eng', 'dev')\n",
        "val_pairs = filter_pairs(val_pairs[:-1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PMl_EFTVu1F8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61d81d17-6caa-4dc8-cfdf-1c0ccd40dae6"
      },
      "cell_type": "code",
      "source": [
        "_, _, test_pairs = readLangs('vi', 'eng', 'test')\n",
        "test_pairs = filter_pairs(test_pairs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FJv4sN4W0d52",
        "outputId": "e5da207d-9c20-4de8-c4fe-bd7065cb7b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(random.choice(test_pairs))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Giờ là câuhỏi đầutiên của ngày hômnay , Bạn có sẵnsàng để nghe về vấnđề quátải trong lựachọn ?', 'So for my first question for you today Are you guys ready to hear about the choice overload problem ?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h0KF3UmvulaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vmMewXau0d6E"
      },
      "cell_type": "markdown",
      "source": [
        "# Preparing Training Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SPnqKF_F0d6F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    idxs = []\n",
        "    for word in sentence.split(' '):\n",
        "        try:\n",
        "            idxs.append(lang.word2index[word])\n",
        "        except KeyError:\n",
        "            idxs.append(3)  # 3 is the id of 'UNK'\n",
        "    idxs.append(EOS_token)\n",
        "    return idxs\n",
        "\n",
        "\n",
        "class VocabDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        \n",
        "        self.source_sent_list = [indexesFromSentence(input_lang,pair[0]) for pair in pairs]\n",
        "        self.target_sent_list = [indexesFromSentence(output_lang,pair[1]) for pair in pairs]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_sent_list)\n",
        "        \n",
        "    def __getitem__(self, key):\n",
        "        token1_idx = self.source_sent_list[key]\n",
        "        token2_idx = self.target_sent_list[key]\n",
        "        return [token1_idx,token2_idx, len(token1_idx), len(token2_idx)]\n",
        "\n",
        "    \n",
        "def Vocab_collate_func(batch):\n",
        "    source_sent_list = []\n",
        "    target_sent_list = []\n",
        "    source_len_list = []\n",
        "    target_len_list = []\n",
        "\n",
        "    for datum in batch:   ### batch = sample\n",
        "        source_len_list.append(datum[2])\n",
        "        target_len_list.append(datum[3])\n",
        "    \n",
        "    max_len_src = max(source_len_list)\n",
        "    max_len_trg = max(target_len_list)\n",
        "    \n",
        "    # padding\n",
        "    for datum in batch:\n",
        "        \n",
        "        # source sentence processing\n",
        "        padded_source = np.pad(np.array(datum[0]), \n",
        "                                pad_width=((0,max_len_src-datum[2])),          \n",
        "                                mode=\"constant\", constant_values=PAD_token)\n",
        "        source_sent_list.append(padded_source)\n",
        "        \n",
        "        # target sentence processing\n",
        "        padded_target = np.pad(np.array(datum[1]), \n",
        "                                pad_width=((0,max_len_trg-datum[3])),        \n",
        "                                mode=\"constant\", constant_values=PAD_token)\n",
        "        target_sent_list.append(padded_target)\n",
        "        \n",
        "    #sort sentences for the batch\n",
        "    sort_idx = sorted(range(len(source_len_list)), key=source_len_list.__getitem__, reverse=True)\n",
        "    source_sent_list = np.array(source_sent_list)[sort_idx]\n",
        "    target_sent_list = np.array(target_sent_list)[sort_idx]\n",
        "    source_len_list = np.array(source_len_list)[sort_idx]\n",
        "    target_len_list = np.array(target_len_list)[sort_idx]\n",
        "        \n",
        "    return [torch.tensor(source_sent_list).to(device), \n",
        "            torch.tensor(target_sent_list).to(device),\n",
        "            torch.LongTensor(source_len_list), \n",
        "            torch.LongTensor(target_len_list)]\n",
        "\n",
        "train_dataset = VocabDataset(pairs)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           collate_fn=Vocab_collate_func,\n",
        "                                           shuffle=True)\n",
        "\n",
        "val_dataset = VocabDataset(val_pairs)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        collate_fn=Vocab_collate_func,\n",
        "                                        shuffle=False)\n",
        "\n",
        "\n",
        "test_dataset = VocabDataset(test_pairs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                        batch_size=BATCH_SIZE,\n",
        "                                        collate_fn=Vocab_collate_func,\n",
        "                                        shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "PhwZoZz_0d6Q"
      },
      "cell_type": "markdown",
      "source": [
        "# Build Encoder-Decoder"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "E4lH5BjW10Jj",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
        "        \n",
        "    def forward(self, input_seqs, input_lengths, hidden):\n",
        "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
        "        embedded = self.embedding(input_seqs)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs, hidden = self.lstm(packed, hidden)\n",
        "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
        "        return outputs, hidden, output_lengths\n",
        "      \n",
        "    def initHidden(self,batch_size):\n",
        "        return (torch.zeros(2 * self.n_layers, batch_size, self.hidden_size, device=device),torch.zeros(2 * self.n_layers, batch_size, self.hidden_size, device=device))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EF4Fj5_pN9Lv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size,dropout_p=0.1, n_layers=1, max_length=MAX_LENGTH):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        \n",
        "        self.embedding = nn.Embedding(output_size, hidden_size,padding_idx=PAD_token)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, n_layers, dropout=dropout_p)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input_seqs, hidden, batch_size):\n",
        "        embedded = self.embedding(input_seqs).view(1, batch_size, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        output = F.relu(embedded)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SbhB0-fFEd8Q",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    '''\n",
        "    Part of the codes are paraphrased from\n",
        "    https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "    '''\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        max_len = encoder_outputs.size(0)\n",
        "        this_batch_size = encoder_outputs.size(1)\n",
        "\n",
        "        # Create variable to store attention energies\n",
        "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)).to(device) # B x S\n",
        "        \n",
        "        if self.method == 'dot':\n",
        "\n",
        "            attn_energies = torch.matmul(encoder_outputs.permute(1,0,2), hidden.permute(1,2,0)).squeeze()\n",
        "            \n",
        "        if self.method == 'concat':\n",
        "            hidden_expand = hidden.expand(max_len, -1, -1).permute(1, 0, 2)  # shape of (B, S, N)\n",
        "            enc_cat_hid = torch.cat([encoder_outputs.permute(1,0,2), hidden_expand], dim=-1)  # shape of (B, S, 2*N)\n",
        "            # After nn.Linear(2*N, N), enc_cat_hid with shape (B, S, N)\n",
        "            # v is shape of (N)\n",
        "            attn_energies = torch.matmul(self.attn(enc_cat_hid), self.v)  # shape of (B, S)\n",
        "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
        "        # Dangerous\n",
        "        if attn_energies.dim() == 1:\n",
        "            attn_energies = attn_energies.unsqueeze(0)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LqklCWyTEnpG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    '''\n",
        "    Part of the codes are paraphrased from\n",
        "    https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "    '''\n",
        "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size).to(device)\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # Choose attention model\n",
        "        if attn_model != 'none':\n",
        "            self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "\n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        batch_size = input_seq.size(0)\n",
        "        embedded = self.embedding(input_seq)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
        "\n",
        "        # Get current hidden state from input word and last hidden state\n",
        "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
        "\n",
        "        # Calculate attention from current RNN state and all encoder outputs;\n",
        "        # apply to encoder outputs to get weighted average\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
        "\n",
        "        # Attentional vector using the RNN hidden state and context vector\n",
        "        # concatenated together (Luong eq. 5)\n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "        # Finally predict next token (Luong eq. 6, without softmax)\n",
        "        output = self.out(concat_output)\n",
        "\n",
        "        # Return final output, hidden state, and attention weights (for visualization)\n",
        "        return output, hidden, attn_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "d1GxY0qw0d6V"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4ICvoIhH0d6W",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qiiPjqOy0d6Y",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, input_lengths, target_lengths, encoder, decoder, \n",
        "          encoder_optimizer, decoder_optimizer, clip=10.0):\n",
        "    encoder_optimizer.zero_grad()  # zero out the accumulated gradient over mini-batch\n",
        "    decoder_optimizer.zero_grad()\n",
        "    \n",
        "    \n",
        "    batch_size = input_tensor.size(1)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    target_tensor = target_tensor.to(device)\n",
        "\n",
        "    encoder_hidden = encoder.initHidden(batch_size)\n",
        "    encoder_outputs = torch.zeros(input_lengths.max(), batch_size, encoder.hidden_size, device=device) \n",
        " \n",
        "\n",
        "    encoder_outputs, encoder_hidden, encoder_output_lengths = encoder(input_tensor, input_lengths, encoder_hidden)\n",
        "    #encoder_outputs:  # max_len x batch_size x hidden_size\n",
        "    #hidden: n_layers * 2 x batch_size x hidden_size\n",
        "    loss = 0\n",
        "\n",
        "    \n",
        "    decoder_input = torch.tensor([SOS_token]*batch_size).to(device)  # decoder_input: torch.Size([1, 32])\n",
        "    decoder_hidden = (encoder_hidden[0][:decoder.n_layers], encoder_hidden[-1][:decoder.n_layers]) # Use last (forward) hidden state from encoder\n",
        "    all_decoder_outputs = Variable(torch.zeros(target_lengths.max(), batch_size, decoder.output_size)).to(device)\n",
        "    \n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    \n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_lengths.max()):\n",
        "            if Attention:\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "     \n",
        "            else:\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden, batch_size)\n",
        "            \n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "            all_decoder_outputs[di] = decoder_output\n",
        "            \n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_lengths.max()):\n",
        "          \n",
        "            if Attention:\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "     \n",
        "            else:\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden, batch_size)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            all_decoder_outputs[di] = decoder_output\n",
        "           \n",
        "            \n",
        "    # Loss calculation and backpropagation\n",
        "\n",
        "    loss = masked_cross_entropy(\n",
        "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
        "        target_tensor.transpose(0, 1).contiguous(), # -> batch x seq\n",
        "        target_lengths\n",
        "    )\n",
        "#     loss = loss.sum()/batch_size \n",
        "    loss.backward()\n",
        "    #    ave_loss.backward()\n",
        "    \n",
        "    # Clip gradient norms\n",
        "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "    \n",
        "    encoder_optimizer.step()   # update parameters\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HmHvipVe0d6a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, lr_decay=True, gamma_encoder=0.9, gamma_decoder=0.9, print_every=100, plot_every=100, learning_rate_encoder=0.0005, learning_rate_decoder=0.002,evaluate_every=3000):\n",
        "    start = time.time()\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate_encoder)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate_decoder)\n",
        "    \n",
        "    scheduler_encoder = ExponentialLR(encoder_optimizer, gamma_encoder, last_epoch=-1) \n",
        "    scheduler_decoder = ExponentialLR(decoder_optimizer, gamma_decoder, last_epoch=-1) \n",
        "    \n",
        "    encoder.to(device)\n",
        "    decoder.to(device)\n",
        "    score_max = 0\n",
        "    plot_losses = []\n",
        "    validation_scores = []\n",
        "    \n",
        "    for epoch in range(1, n_iters + 1):\n",
        "        print_loss_total = 0  # Reset every print_every\n",
        "        plot_loss_total = 0  # Reset every plot_every\n",
        "        if lr_decay:\n",
        "            scheduler_encoder.step()\n",
        "            scheduler_decoder.step()\n",
        "        \n",
        "        for i, (input_sentences, target_sentences,len1,len2) in enumerate(train_loader): \n",
        "            encoder.train()\n",
        "            decoder.train()\n",
        "            \n",
        "            input_tensor = input_sentences.transpose(0,1)   # 13*100 to 100*13\n",
        "            target_tensor = target_sentences.transpose(0,1)\n",
        "            loss = train(input_tensor, target_tensor, len1, len2, encoder,\n",
        "                         decoder, encoder_optimizer, decoder_optimizer)\n",
        "            print_loss_total += loss\n",
        "            plot_loss_total += loss\n",
        "            \n",
        "            if i > 0 and i % evaluate_every == 0:\n",
        "                bleu_score, (src_sents, sys_sents, ref_sents) = test_model(encoder, decoder, val_loader)\n",
        "                print('Validation Score: {} \\n source sentence {} \\n predicted sentence {} \\n Reference sentence: {}'.format(bleu_score,src_sents, sys_sents, ref_sents))\n",
        "                validation_scores.append(bleu_score)\n",
        "                \n",
        "                if bleu_score > score_max:\n",
        "                    score_max = bleu_score\n",
        "                \n",
        "                    torch.save({\n",
        "                                'epoch': epoch,\n",
        "                                'encoder': encoder.state_dict(),\n",
        "                                'encoder_optimizer': encoder_optimizer.state_dict(),\n",
        "                                'decoder': decoder.state_dict(),\n",
        "                                'decoder_optimizer': decoder_optimizer.state_dict()\n",
        "                                }, \"/content/drive/My Drive/saved_model/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                        .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size,\n",
        "                                target_vocab_size,lr_decay,teacher_forcing_ratio))   \n",
        "                    \n",
        "            if i > 0 and i % print_every == 0:\n",
        "                print_loss_avg = print_loss_total / print_every\n",
        "                print_loss_total = 0\n",
        "#                bleu_score, (sys_sents, ref_sents) = test_model(encoder, decoder, val_loader)\n",
        "                print('Time: {}, Epoch: [{}/{}], Step: [{}/{}], Train Loss: {}'.format(\n",
        "                    timeSince(start, i + 1/len(train_loader)), epoch, n_iters, i, \n",
        "                    len(train_loader),print_loss_avg))\n",
        "\n",
        "            if i > 0 and i % plot_every == 0:\n",
        "                plot_loss_avg = plot_loss_total / plot_every\n",
        "                plot_losses.append(plot_loss_avg)\n",
        "                plot_loss_total = 0\n",
        "                torch.save({\n",
        "                            'plot_losses': plot_losses,\n",
        "                            'validation_scores': validation_scores\n",
        "                            }, \"/content/drive/My Drive/saved_scores/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                    .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size,\n",
        "                            target_vocab_size,lr_decay,teacher_forcing_ratio))   \n",
        "                \n",
        "            torch.cuda.empty_cache()    \n",
        "        print(\"plot_losses:\",plot_losses)\n",
        "        print(\"validation_scores:\",validation_scores)\n",
        "    showPlot(plot_losses)\n",
        "    showPlot(validation_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mj8OwEU_0d6d"
      },
      "cell_type": "markdown",
      "source": [
        "# Plotting results"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bDqy1HHH0d6e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L_-Y9XII0d6g"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "rwNlj-Hi9JTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class beam_search(object):\n",
        "    \"\"\"\n",
        "    Some code is paraphrased from\n",
        "    https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/translate/beam.py\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, max_length, beam_size, attention = True,sentence_ratio = False): \n",
        "        super(beam_search, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.attention = attention\n",
        "        self.max_length = max_length\n",
        "        self.beam_size = beam_size\n",
        "        self.sentence_ratio = sentence_ratio\n",
        "        \n",
        "    def search(self, encoder_outputs, decoder_input, decoder_hidden, src_len):\n",
        "\n",
        "        prob = {k:0 for k in range(self.beam_size)}\n",
        "        bestSent = []\n",
        "        bestScore = []       \n",
        "        decoder_word_choices = {k:[] for k in range(self.beam_size)}\n",
        "        decoder_hidden_choices = {}\n",
        "        decoder_input_choices = {}\n",
        "        decoder_output_choices = {}\n",
        "        \n",
        "        # Initialize beam serach\n",
        "        if self.attention == True:\n",
        "            decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input.contiguous(), decoder_hidden, encoder_outputs)\n",
        "            decoder_output = F.log_softmax(decoder_output, dim=1)\n",
        "            topv, topi = decoder_output.data.topk(self.beam_size)\n",
        "        else: \n",
        "            print(\"Only available when attention = True\")\n",
        "        \n",
        "        # Initialize beam candidates \n",
        "        for i in range(self.beam_size):\n",
        "            decoder_word_choices[i].append(topi.squeeze()[i].item())\n",
        "            decoder_input_choices[i] = topi.squeeze()[i].detach()\n",
        "            decoder_hidden_choices[i] = decoder_hidden\n",
        "            prob[i] += topv.squeeze()[i].detach()\n",
        "            \n",
        "        ## running beam search\n",
        "        cur_len = 0\n",
        "        max_length = 2*src_len if self.sentence_ratio else self.max_length\n",
        "        # delete\n",
        "#         print(self.sentence_ratio)\n",
        "#         print(src_len)\n",
        "#         print(max_length)\n",
        "        \n",
        "        while decoder_hidden_choices and cur_len <= max_length:\n",
        "            cur_len += 1\n",
        "            topi = {}\n",
        "            key_list = list(decoder_hidden_choices.keys())\n",
        "            scores = []\n",
        "            for key in key_list:\n",
        "                    \n",
        "                decoder_output, decoder_hidden_choices[key],decoder_attn  = self.decoder(decoder_input_choices[key].unsqueeze(0), decoder_hidden_choices[key],encoder_outputs)\n",
        "                decoder_output_choices[key] = F.log_softmax(decoder_output, dim=1)\n",
        "                topv, topi[key] = decoder_output_choices[key].data.topk(len(decoder_hidden_choices))\n",
        "                scores.extend((topv+prob[key]).tolist()[0])\n",
        "                \n",
        "            scores = np.array(scores)   \n",
        "            max_candidate_score = scores.argsort()[-len(decoder_hidden_choices):][::-1]\n",
        "            decoded_sent_score = scores[max_candidate_score]\n",
        "\n",
        "            choice_sentence = {}\n",
        "            choiceHidden = {}\n",
        "            trashOfKeys = []\n",
        "            \n",
        "            for j in range(len(max_candidate_score)):\n",
        "                prev_choice_idx = key_list[int(np.floor(max_candidate_score[j]/len(decoder_hidden_choices)))]\n",
        "                if topi[prev_choice_idx].squeeze().dim() == 0:\n",
        "                    next_idx = topi[prev_choice_idx].squeeze()\n",
        "                else:\n",
        "                    next_idx = topi[prev_choice_idx].squeeze()[max_candidate_score[j] % len(decoder_hidden_choices)]\n",
        "                \n",
        "                s_choice = decoder_word_choices[prev_choice_idx].copy()\n",
        "                s_choice.append(next_idx.item())\n",
        "                choice_sentence[j] = s_choice\n",
        "                h_choice = decoder_hidden_choices[prev_choice_idx]\n",
        "                choiceHidden[j] = h_choice\n",
        "                decoder_input_choices[j] = next_idx.detach()   \n",
        "                prob[j] = decoded_sent_score[j] \n",
        "   \n",
        "            decoder_word_choices = choice_sentence\n",
        "            decoder_hidden_choices = choiceHidden\n",
        "            \n",
        "            for key, s in decoder_word_choices.items():\n",
        "                if EOS_token in s:\n",
        "                    bestSent.append(s)\n",
        "                    bestScore.append(prob[key]) \n",
        "                    trashOfKeys.append(key)\n",
        "                    \n",
        "            for k in trashOfKeys:\n",
        "                decoder_hidden_choices.pop(k)\n",
        "                decoder_word_choices.pop(k)\n",
        "\n",
        "        if len(bestScore) == 0:\n",
        "            max_prob = prob[0]\n",
        "            max_prob_idx = 0\n",
        "            for k in prob.keys():\n",
        "                if prob[k] > max_prob: \n",
        "                    max_prob_idx = k\n",
        "                    max_prob = prob[k]\n",
        "            bestScore.append(max_prob)\n",
        "            bestSent.append(decoder_word_choices[max_prob_idx])\n",
        "                \n",
        "        return bestSent, bestScore"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "muc68_Oz0d6h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batch_outputs(encoder, decoder, input_sentences, input_lengths, output_lengths): \n",
        "    with torch.no_grad():\n",
        "        input_tensor = input_sentences.transpose(0,1).to(device)   # 32*100 to 100*32\n",
        "        batch_size = input_tensor.size(1)\n",
        "        encoder_hidden = encoder.initHidden(batch_size)\n",
        "        encoder_outputs, encoder_hidden,encoder_output_lengths = encoder(input_tensor, input_lengths, encoder_hidden)\n",
        "       \n",
        "        decoder_hidden = (encoder_hidden[0][:decoder.n_layers], encoder_hidden[-1][:decoder.n_layers]) # Use last (forward) hidden state from encoder\n",
        "\n",
        "        decoder_input = Variable(torch.tensor([SOS_token]*batch_size)).to(device)  # decoder_input: torch.Size([1, 32])\n",
        "        decoded_words = np.empty((output_lengths.max(), batch_size), dtype=object)\n",
        "\n",
        "        for di in range(output_lengths.max()):\n",
        "            if Attention:\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            else:\n",
        "                decoder_output, decoder_hidden = decoder(\n",
        "                    decoder_input, decoder_hidden, batch_size)\n",
        "\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach().to(device)  # detach from history as input\n",
        "\n",
        "            decoded_words[di:] = np.array(['<EOS>' if idx==EOS_token else output_lang.index2word[idx] for idx in decoder_input.tolist()])\n",
        "\n",
        "        return decoded_words.transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5-3nvuRwf0nD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_ratio = True\n",
        "def pad(l, max_length):\n",
        "    while len(l) < max_length + 2:\n",
        "        l.append(PAD_token)\n",
        "    return l\n",
        "  \n",
        "  \n",
        "def get_beam_batch_outputs(encoder, decoder, input_sentences, input_lengths): #####\n",
        "    with torch.no_grad():\n",
        "        input_tensor = input_sentences.transpose(0,1).to(device)   # 32*100 to 100*32\n",
        "        batch_size = input_tensor.size(1)\n",
        "        encoder_hidden = encoder.initHidden(batch_size)\n",
        "        encoder_outputs, encoder_hidden,encoder_output_lengths = encoder(input_tensor, input_lengths, encoder_hidden)\n",
        "\n",
        "        decoder_hidden = (encoder_hidden[0][:decoder.n_layers], encoder_hidden[-1][:decoder.n_layers])\n",
        "        my_beam_search = beam_search(encoder, decoder,input_sentences.max().item(), beam_size, True, sentence_ratio)\n",
        "        beam_search_result = []\n",
        "        for i in range(batch_size):\n",
        "            decoder_input = torch.tensor([SOS_token], device=device, requires_grad=False).unsqueeze(0)#.view(1,-1) # take care of different input shape\n",
        "            sentences, probs = my_beam_search.search(encoder_outputs[:,i,:].unsqueeze(1), decoder_input, \n",
        "                                                     (decoder_hidden[0][:,i,:].unsqueeze(1).contiguous(),decoder_hidden[1][:,i,:].unsqueeze(1).contiguous()), input_lengths[i].item())\n",
        "\n",
        "            beam_search_result.append(sentences[probs.index(max(probs))])\n",
        "\n",
        "        padded_beam_search_result = []\n",
        "\n",
        "        max_length = 0\n",
        "        for sentence in beam_search_result:\n",
        "            if len(sentence) > max_length:\n",
        "                max_length = len(sentence)\n",
        "\n",
        "        for sentence in beam_search_result:\n",
        "            while len(sentence) < max_length + 2:\n",
        "                sentence.append(PAD_token)\n",
        "            padded_beam_search_result.append(sentence)\n",
        "\n",
        "        batch_sentences = []\n",
        "\n",
        "        for sentence in padded_beam_search_result:\n",
        "            sentence = [output_lang.index2word[k] for k in sentence]\n",
        "            try:\n",
        "                end_idx = sentence.index('<EOS>')\n",
        "                batch_sentences.append(' '.join(sentence[:end_idx]))\n",
        "            except ValueError:\n",
        "                batch_sentences.append(' '.join(sentence))\n",
        "\n",
        "    return batch_sentences\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ce-QyODD0d6l",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_model(encoder, decoder, loader, search_method = 'greedy'):\n",
        "    \n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    \n",
        "    score = []\n",
        "    src_sentences = []\n",
        "    sys_sentences = []\n",
        "    ref_sentences = []\n",
        "    encoder.train(False)\n",
        "    decoder.train(False)\n",
        "    for i, (input_sentences, target_sentences, len1, len2) in enumerate(loader):\n",
        "        for sentence in target_sentences:\n",
        "            trg_list = []\n",
        "            for idx in sentence:\n",
        "                if idx.item() == EOS_token:\n",
        "                    break\n",
        "                else:\n",
        "                    trg_list.append(output_lang.index2word[idx.item()])\n",
        "            ref_sentences.append(' '.join(trg_list))\n",
        "        for sentence in input_sentences:\n",
        "            src_list = []\n",
        "            for idx in sentence:\n",
        "                if idx.item() == EOS_token:\n",
        "                    break\n",
        "                else:\n",
        "                    src_list.append(input_lang.index2word[idx.item()])\n",
        "            src_sentences.append(' '.join(src_list))\n",
        "\n",
        "        batch_size = input_sentences.size(0)\n",
        "        if search_method == 'greedy':\n",
        "            for sentence in get_batch_outputs(encoder, decoder, input_sentences, len1, len2):\n",
        "                try:\n",
        "                    end_idx = sentence.tolist().index('<EOS>')\n",
        "                    sys_sentences.append(' '.join(sentence[:end_idx]))\n",
        "                except ValueError:\n",
        "                    sys_sentences.append(' '.join(sentence))\n",
        "                    \n",
        "        elif search_method == 'beam':\n",
        "            translation_output = get_beam_batch_outputs(encoder, decoder, input_sentences, len1)\n",
        "            sys_sentences.extend(translation_output)\n",
        "            \n",
        "    encoder.train(True)\n",
        "    decoder.train(True) \n",
        "    \n",
        "    score = corpus_bleu(sys_sentences,[ref_sentences], smooth=\"floor\", smooth_floor=0.01, lowercase=False, use_effective_order=True, tokenize=DEFAULT_TOKENIZER).score\n",
        "    return score, (src_sentences[0:5], sys_sentences[0:5], ref_sentences[0:5])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hjafDuaU0d6s"
      },
      "cell_type": "markdown",
      "source": [
        "# TRAINING AND EVALUATING  (Only one example is shown )"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "T3iWCiIQ0d6t",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17371
        },
        "outputId": "866cf661-c960-442e-885b-433b8d4c761d"
      },
      "cell_type": "code",
      "source": [
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size, n_layers=n_layers).to(device)\n",
        "if Attention:\n",
        "    decoder1 = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, dropout=dropout_p, n_layers=n_layers).to(device)\n",
        "else:\n",
        "    decoder1 = DecoderRNN(hidden_size, output_lang.n_words, dropout_p=0)\n",
        "\n",
        "trainIters(encoder1, decoder1, n_iters=n_epochs, print_every=print_every, plot_every=plot_every, evaluate_every=evaluate_every, learning_rate_encoder=lr_rate_en, learning_rate_decoder=lr_rate_de, lr_decay=lr_decay, gamma_encoder=gamma_decoder,\n",
        "          gamma_decoder=gamma_decoder)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time: 1m 47s (- -2m 13s), Epoch: [1/20], Step: [100/2015], Train Loss: 6.619141621589661\n",
            "Time: 3m 34s (- -4m 27s), Epoch: [1/20], Step: [200/2015], Train Loss: 5.680650668144226\n",
            "Time: 5m 22s (- -6m 38s), Epoch: [1/20], Step: [300/2015], Train Loss: 5.354693050384522\n",
            "Time: 7m 8s (- -8m 52s), Epoch: [1/20], Step: [400/2015], Train Loss: 5.147682757377624\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 3.61856925760017 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['And I m going to be a lot of the same , and I m going to be a lot of the same thing , and I m going to be a lot of the same thing , and I m going to be a lot of', 'And so we re going to be a lot of the same thing that we re going to be a lot of the world , and the most thing that s the same thing that s the same thing that s the same thing .', 'And the <UNK> of the <UNK> , and the <UNK> of the <UNK> , and the <UNK> of the <UNK> of the <UNK> , and the <UNK> of the <UNK> of the <UNK> , and the <UNK> of the <UNK> of the <UNK> .', 'And I m going to be a lot of the <UNK> of the <UNK> of the <UNK> of the world , and I m going to be a lot of the world , and I m going to be a lot of the world .', 'And I m going to be a lot of the <UNK> , and I m going to be a lot of the <UNK> , and I m going to be a lot of the <UNK> , and I m going to be a lot of the world'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 9m 7s (- -10m 53s), Epoch: [1/20], Step: [500/2015], Train Loss: 5.000887427330017\n",
            "Time: 10m 57s (- -11m 4s), Epoch: [1/20], Step: [600/2015], Train Loss: 4.875171036720276\n",
            "Time: 12m 45s (- -13m 15s), Epoch: [1/20], Step: [700/2015], Train Loss: 4.784134893417359\n",
            "Time: 14m 32s (- -15m 28s), Epoch: [1/20], Step: [800/2015], Train Loss: 4.674350028038025\n",
            "Time: 16m 20s (- -17m 40s), Epoch: [1/20], Step: [900/2015], Train Loss: 4.570793943405151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 7.007239430993643 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was going to be able to do with a little bit of the time , but I m going to do it , but I m going to do it , but I m going to do it , but I m going to do it', 'And that s the only thing that the <UNK> of the <UNK> of the <UNK> of the <UNK> , the <UNK> of the <UNK> , and the <UNK> was not going to be a little bit of the <UNK> .', 'So we re very much more than the world , but we re very much about the world , but the world is the world of the world , and the same thing is , the same thing is , the same thing .', 'I m going to talk about the <UNK> of the <UNK> that I ve got to do that in a <UNK> with a <UNK> with a <UNK> with a <UNK> with a <UNK> with a <UNK> with a <UNK> .', 'And I was not going to be a little bit like that , but I m going to do that , but I m going to do that , and I m going to do that , in the world , but I m going to do'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 18m 18s (- -19m 42s), Epoch: [1/20], Step: [1000/2015], Train Loss: 4.425696096420288\n",
            "Time: 20m 4s (- -21m 56s), Epoch: [1/20], Step: [1100/2015], Train Loss: 4.341190090179444\n",
            "Time: 21m 52s (- -22m 8s), Epoch: [1/20], Step: [1200/2015], Train Loss: 4.23148211479187\n",
            "Time: 23m 37s (- -24m 23s), Epoch: [1/20], Step: [1300/2015], Train Loss: 4.1684769344329835\n",
            "Time: 25m 22s (- -26m 38s), Epoch: [1/20], Step: [1400/2015], Train Loss: 4.08700380563736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 8.60345393722872 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been able to tell what the life of a man who was going to be a very simple , but I m going to tell you how it was very , because I was very hard , because I was very difficult to be the', 'And in fact , there s a few years of the people who had this little bit of the people who had no longer on this , because the <UNK> has never been nothing to be able to get to the two two .', 'So we ve been a lot of time to talk about the <UNK> of the <UNK> , but the <UNK> <UNK> , but the more importantly , the <UNK> , the <UNK> , the <UNK> and the <UNK> of our life .', 'I can t tell you about my own <UNK> , how can you say that in the <UNK> of the <UNK> , because I was able to be able to be able to be able to be able to a <UNK> with a man .', 'And I ve been able to know the world in the world as I think I m going to be in <UNK> , but I don t think I ll be able to be able to change the <UNK> , for the time .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 27m 21s (- -28m 39s), Epoch: [1/20], Step: [1500/2015], Train Loss: 4.019237139225006\n",
            "Time: 29m 7s (- -30m 53s), Epoch: [1/20], Step: [1600/2015], Train Loss: 3.9547698307037353\n",
            "Time: 30m 52s (- -31m 8s), Epoch: [1/20], Step: [1700/2015], Train Loss: 3.8860162687301636\n",
            "Time: 32m 37s (- -33m 23s), Epoch: [1/20], Step: [1800/2015], Train Loss: 3.842606201171875\n",
            "Time: 34m 24s (- -35m 36s), Epoch: [1/20], Step: [1900/2015], Train Loss: 3.771799964904785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 11.345141070743452 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was born in the life of a human life , but I m sure that it s very difficult , but I m sure that it s very difficult , because the people from China to China is the <UNK> of the China .', 'In the time , the <UNK> had read these <UNK> , the <UNK> of the people who didn t have to be on the same , because the <UNK> was not there to be two weeks in the two weeks .', 'In the school , we ve been a lot of time to learn about the <UNK> of the <UNK> <UNK> , but it s not a lot of the first time , the United States , the United States and the <UNK> of our minds .', 'I can t tell about what I ve been doing is how I ve been able to get that in the <UNK> of the <UNK> because he was born in the United States , which is the <UNK> to live with a person who s going to', 'And I had a lot of time that I didn t know the world , but I still think I m going to live in the life in <UNK> <UNK> , for all the things that we have to change .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 36m 23s (- -37m 37s), Epoch: [1/20], Step: [2000/2015], Train Loss: 3.73540559053421\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452]\n",
            "Time: 38m 25s (- -39m 57s), Epoch: [2/20], Step: [100/2015], Train Loss: 3.562603545188904\n",
            "Time: 40m 12s (- -41m 59s), Epoch: [2/20], Step: [200/2015], Train Loss: 3.4861389136314394\n",
            "Time: 41m 57s (- -42m 11s), Epoch: [2/20], Step: [300/2015], Train Loss: 3.486001591682434\n",
            "Time: 43m 44s (- -44m 22s), Epoch: [2/20], Step: [400/2015], Train Loss: 3.4452125430107117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 12.912563664463216 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been told what happens to my life with my life , and I said , I m sorry that it was very hard , but I said , It s very hard to realize that the Chinese from China was <UNK> , and the Chinese', 'In the case of the time , when the <UNK> read this little guy , the family who didn t have to do on this , because the house was not there .', 'In the school , we ve been many time to learn about the life of the <UNK> of the <UNK> <UNK> , but I don t learn a lot of the world , and the United States is our own .', 'I can t say about how I had to do that in the same time , how could you say that in the Chinese because I was sent to the Chinese <UNK> because he lived with a person who live in China .', 'And I had ever heard that I had no idea the world as a way , but I still think I m going to live in <UNK> , until all of everything .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 45m 42s (- -46m 22s), Epoch: [2/20], Step: [500/2015], Train Loss: 3.4079955887794493\n",
            "Time: 47m 28s (- -48m 35s), Epoch: [2/20], Step: [600/2015], Train Loss: 3.379240102767944\n",
            "Time: 49m 14s (- -50m 49s), Epoch: [2/20], Step: [700/2015], Train Loss: 3.342082092761993\n",
            "Time: 50m 59s (- -51m 4s), Epoch: [2/20], Step: [800/2015], Train Loss: 3.3137521028518675\n",
            "Time: 52m 47s (- -53m 16s), Epoch: [2/20], Step: [900/2015], Train Loss: 3.280788285732269\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 14.413157602219174 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been interested in what happens to life with my life , and I realized that it was very difficult , but I realized that it was very difficult to be very difficult , because people from the Chinese <UNK> , the Chinese from the Chinese', 'In this case , when they read these little <UNK> , the other five people had no longer on the top , because he didn t have to eat what to eat in two weeks .', 'In school , we spent a lot of time to learn about the life of the life of the <UNK> <UNK> , but I didn t learn a lot of the world , and the United States is our own own , and the next place .', 'I can t talk about how I ve been able to do that in the same time , like how to be able to say that in China , because he was sent to live in China with a person who live to live with a person', 'And I ve been asked to wonder the world as a way of how much life is , but I still think I would live in the life in <UNK> , until everything is going to be changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 54m 45s (- -55m 17s), Epoch: [2/20], Step: [1000/2015], Train Loss: 3.2668155312538145\n",
            "Time: 56m 32s (- -57m 30s), Epoch: [2/20], Step: [1100/2015], Train Loss: 3.2598270320892335\n",
            "Time: 58m 19s (- -59m 42s), Epoch: [2/20], Step: [1200/2015], Train Loss: 3.201135313510895\n",
            "Time: 60m 7s (- -61m 55s), Epoch: [2/20], Step: [1300/2015], Train Loss: 3.1885945987701416\n",
            "Time: 61m 53s (- -62m 9s), Epoch: [2/20], Step: [1400/2015], Train Loss: 3.1689483237266542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 15.239655385296158 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been learned to what happened to the life of a person who s been going to be , but I said , It s very difficult , but I said , It s very difficult , because the people from North America were <UNK> .', 'And in that , when they read these little reading , the people of the child was not in this way , because the house was not there to eat this , because the house was not going to eat in two weeks .', 'In the school , we spent a lot of time to learn about the rest of the <UNK> , but I don t learn more about the world , except the United States , the United States , and Japan is our own .', 'I can t talk about myself from the <UNK> , which is just saying that in the <UNK> because the <UNK> of the <UNK> , I was sent to live in China , to live with a person .', 'And I ve been wondering that I didn t know the world s world , but I still think I m going to live in the life of the <UNK> <UNK> , until everything else .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 63m 52s (- -64m 9s), Epoch: [2/20], Step: [1500/2015], Train Loss: 3.159568283557892\n",
            "Time: 65m 42s (- -66m 20s), Epoch: [2/20], Step: [1600/2015], Train Loss: 3.126814968585968\n",
            "Time: 67m 28s (- -68m 33s), Epoch: [2/20], Step: [1700/2015], Train Loss: 3.100325117111206\n",
            "Time: 69m 14s (- -70m 47s), Epoch: [2/20], Step: [1800/2015], Train Loss: 3.102081768512726\n",
            "Time: 71m 1s (- -71m 0s), Epoch: [2/20], Step: [1900/2015], Train Loss: 3.0608720707893373\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 16.318305424244944 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was born in what happens to life , and I realized that it s very difficult to the life of the <UNK> , but I realized that it s very difficult , because the people from North Korea in China is being <UNK> to the Chinese', 'In this case , when you read these little children , the other people s children didn t have to be on the top of this , because the house was not in the two weeks .', 'In school , we spend a lot of time to learn about the rest of the <UNK> of the <UNK> <UNK> , but not to learn a lot of the world , except the United States , and Japan is our enemy .', 'I can t talk about myself in the <UNK> of the <UNK> , how to do that in the <UNK> of the <UNK> of the <UNK> , because he can say that in the Chinese <UNK> of life with a <UNK> .', 'And so I ve been asking , I ve been asking , but I still think I m going to live in the life in <UNK> , until everything is going to be when everything s gone .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 73m 1s (- -73m 0s), Epoch: [2/20], Step: [2000/2015], Train Loss: 3.0492928194999696\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944]\n",
            "Time: 75m 4s (- -75m 40s), Epoch: [3/20], Step: [100/2015], Train Loss: 2.778776776790619\n",
            "Time: 76m 51s (- -77m 31s), Epoch: [3/20], Step: [200/2015], Train Loss: 2.733776319026947\n",
            "Time: 78m 37s (- -79m 38s), Epoch: [3/20], Step: [300/2015], Train Loss: 2.7686165308952333\n",
            "Time: 80m 25s (- -81m 47s), Epoch: [3/20], Step: [400/2015], Train Loss: 2.7396368527412416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 17.655139184606092 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been interested in my life that s going to life a refugee , but I m saying that it s very hard to get hard , but I m saying that it s very hard to get the people from China to China s Chinese', 'And in that time , when you read these little bits of these little children , both of the baby who didn t have any of these , because the house was not going to eat in two weeks .', 'In school , we spend a lot of time to learn about the rest of the <UNK> , but also , there s no idea of the world , except the United States , the United States , the United States , the Japanese and Japan is', 'I can t talk about myself to be able to get out of the <UNK> , how to do that in the <UNK> of the <UNK> because I was sent to live in China because I was sent to live in a way to live with a', 'Although I ve been wondering before the world outside , how much other I m going to live in the past , but I still think I m going to live in the life in <UNK> , until everything s going to change .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 82m 23s (- -83m 46s), Epoch: [3/20], Step: [500/2015], Train Loss: 2.7224786949157713\n",
            "Time: 84m 9s (- -85m 59s), Epoch: [3/20], Step: [600/2015], Train Loss: 2.7188721823692323\n",
            "Time: 85m 55s (- -86m 11s), Epoch: [3/20], Step: [700/2015], Train Loss: 2.7184589552879332\n",
            "Time: 87m 41s (- -88m 25s), Epoch: [3/20], Step: [800/2015], Train Loss: 2.71845632314682\n",
            "Time: 89m 27s (- -90m 38s), Epoch: [3/20], Step: [900/2015], Train Loss: 2.715833249092102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 17.53099017656127 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was in the spirit of a refugee refugee , and then I realized that it was very difficult , but I realized that it was difficult , because people from North Korea , because the refugee came from North Korea , was considered the <UNK> of', 'And so there s a five <UNK> of the baby , and he read the five people of the baby , because the house was not going to eat in two weeks .', 'In school , we spend a lot of time about the rest of the <UNK> <UNK> <UNK> , but not to learn a lot of the outside world , except the United States , and Japan is our enemy .', 'I can t talk about myself to the idea of how I ve ever been to get out of the <UNK> , how much of the time I ve been sending to live in China , because the Chinese is to live to live with a <UNK>', 'Although I had ever wondered the world outside of the other , but I still think I m going to live in the life of the <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 91m 23s (- -92m 42s), Epoch: [3/20], Step: [1000/2015], Train Loss: 2.705645899772644\n",
            "Time: 93m 11s (- -94m 53s), Epoch: [3/20], Step: [1100/2015], Train Loss: 2.7103427267074585\n",
            "Time: 94m 57s (- -95m 7s), Epoch: [3/20], Step: [1200/2015], Train Loss: 2.665152521133423\n",
            "Time: 96m 43s (- -97m 21s), Epoch: [3/20], Step: [1300/2015], Train Loss: 2.662909860610962\n",
            "Time: 98m 29s (- -99m 34s), Epoch: [3/20], Step: [1400/2015], Train Loss: 2.667311899662018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 17.716875052278873 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was born in what happened to the life of a refugee , and then I realized that it was very difficult , but I soon realized that it was very difficult , because people from North Korea , and the refugee of the North China was', 'And in that time , when you read these little <UNK> , the family who didn t have been on the top of this , because the house was not there to eat in two weeks .', 'In school , we spent a lot of time to learn about the rest of the <UNK> <UNK> , but not to learn a lot of the world outside of the outside of the outside , except and Japan is our enemy .', 'I can t talk about myself in the North , and how long it s not about how long it s been only possible for the <UNK> of the <UNK> , because that s what I ve been sent to China with a professional person .', 'Although I had wondered , I ve been wondering that I didn t know the world outside , but I still think I would live in the entire life in <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 100m 28s (- -101m 35s), Epoch: [3/20], Step: [1500/2015], Train Loss: 2.669655153751373\n",
            "Time: 102m 15s (- -103m 48s), Epoch: [3/20], Step: [1600/2015], Train Loss: 2.6586928224563597\n",
            "Time: 104m 2s (- -104m 1s), Epoch: [3/20], Step: [1700/2015], Train Loss: 2.668819844722748\n",
            "Time: 105m 48s (- -106m 14s), Epoch: [3/20], Step: [1800/2015], Train Loss: 2.641269471645355\n",
            "Time: 107m 33s (- -108m 29s), Epoch: [3/20], Step: [1900/2015], Train Loss: 2.634045372009277\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.17102691009252 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so excited about what happened to the human life of the <UNK> , and I realized , but I soon realize that it was very difficult , but I soon realize that it was very difficult , because the refugees came from North Korea to', 'In this case when you read these <UNK> , the family who s not going to be on the top of the same , because the house was not having to eat in two weeks .', 'In school , we spend a lot of time to learn about the rest of the <UNK> <UNK> , but not to learn a lot about the outside world , except the United States , and Japan is our enemies .', 'I can t talk about how I had to get out of the <UNK> , how much of the time I could say in the <UNK> of the <UNK> , because I was sent to live in China to live with a <UNK> .', 'Though I ve been wondering , I ve wondered the world outside of the outside , but I still think I m going to live in the life of <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 109m 33s (- -110m 29s), Epoch: [3/20], Step: [2000/2015], Train Loss: 2.6200781583786013\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252]\n",
            "Time: 111m 36s (- -111m 30s), Epoch: [4/20], Step: [100/2015], Train Loss: 2.344979348182678\n",
            "Time: 113m 21s (- -113m 12s), Epoch: [4/20], Step: [200/2015], Train Loss: 2.3078741335868838\n",
            "Time: 115m 8s (- -115m 14s), Epoch: [4/20], Step: [300/2015], Train Loss: 2.3485884404182436\n",
            "Time: 116m 55s (- -117m 21s), Epoch: [4/20], Step: [400/2015], Train Loss: 2.3413527703285215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.37477848851518 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happened to the lives of a refugee , and I soon realize that it s very hard to be hard , because people from North Korea in China s <UNK> , are considered refugees in China , were considered to be', 'And in that day , when you read these <UNK> , the children of the baby who didn t have anything to be on the top , because he didn t have anything to eat in two weeks .', 'In school , we spent a lot of time to learn about how the rest of the Cold War II , but it s not about learning a lot of the world , except the United States , and Japan is our enemies .', 'I can t talk about how I had to get away from the North , how do I get away from the <UNK> of the <UNK> , so how long I was sent to live in China with a <UNK> .', 'Although I ve been asking this way the world outside , but I still think I m going to live in my life in <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 118m 54s (- -119m 20s), Epoch: [4/20], Step: [500/2015], Train Loss: 2.341391797065735\n",
            "Time: 120m 40s (- -121m 31s), Epoch: [4/20], Step: [600/2015], Train Loss: 2.347625362873077\n",
            "Time: 122m 25s (- -123m 44s), Epoch: [4/20], Step: [700/2015], Train Loss: 2.334087470769882\n",
            "Time: 124m 14s (- -125m 55s), Epoch: [4/20], Step: [800/2015], Train Loss: 2.358257761001587\n",
            "Time: 126m 0s (- -126m 7s), Epoch: [4/20], Step: [900/2015], Train Loss: 2.3602662563323973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 18.630040302764606 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happened to the life of a refugee refugee , and the <UNK> of the <UNK> , but I soon realized that it was difficult , because the refugee came from North Korea in China was considered to be <UNK> citizens .', 'And in that time when you read these lines , there were five people of the baby who didn t have to be on the other , because the house didn t have anything to eat in two weeks .', 'In school , we spent a lot of time to learn about the rest of the Cold War II , but not to learn a lot of the outside world , except the United States , Korea and Japan are our enemies .', 'I can t talk to the idea of how I ve been able to hide out from the North , how do you say in the <UNK> , because I m sent to live in the Chinese , to live with a professional person .', 'Although I ve been wondering that I didn t know the world outside the other , but I still think I m going to live in life in <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 127m 57s (- -128m 9s), Epoch: [4/20], Step: [1000/2015], Train Loss: 2.345759756565094\n",
            "Time: 129m 43s (- -130m 23s), Epoch: [4/20], Step: [1100/2015], Train Loss: 2.344395673274994\n",
            "Time: 131m 29s (- -132m 36s), Epoch: [4/20], Step: [1200/2015], Train Loss: 2.3665045428276064\n",
            "Time: 133m 16s (- -134m 50s), Epoch: [4/20], Step: [1300/2015], Train Loss: 2.35975017786026\n",
            "Time: 135m 2s (- -135m 3s), Epoch: [4/20], Step: [1400/2015], Train Loss: 2.3527770495414733\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.172401002500287 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been so excited about what happened to a refugee person from the <UNK> , which is , but I soon realize that it s difficult , because people are refugees , because people are refugees , because people are <UNK> in China , are being', 'And in that time , when you read it , even though the people who are not at all over the time , because the house didn t have anything to eat in two weeks .', 'In school , we spend a lot of time to learn about the rest of the <UNK> War II , but not to learn a lot about the world , except the United States , Korea and Japan are our enemies .', 'I can t talk about how I ve got to hide about myself from the future , like how to do it in the Chinese , because the famine is that I was sent to live in China to live with a <UNK> .', 'Though I used to wonder , I ve been asking myself , but I still think I m going to live in the lives of my life in <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 136m 59s (- -137m 6s), Epoch: [4/20], Step: [1500/2015], Train Loss: 2.3432939720153807\n",
            "Time: 138m 46s (- -139m 19s), Epoch: [4/20], Step: [1600/2015], Train Loss: 2.3417982840538025\n",
            "Time: 140m 31s (- -141m 33s), Epoch: [4/20], Step: [1700/2015], Train Loss: 2.3487310552597047\n",
            "Time: 142m 19s (- -143m 45s), Epoch: [4/20], Step: [1800/2015], Train Loss: 2.3517938685417175\n",
            "Time: 144m 6s (- -145m 58s), Epoch: [4/20], Step: [1900/2015], Train Loss: 2.362763285636902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.20122717232046 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so excited about what happened to a refugee refugee from North Korea , and I realized that it was difficult , but I realized that it was difficult , because people were born , because of the refugee North Korea in China , are <UNK>', 'In this case , when you read these lines , all the five people of the baby who didn t have any other way , because the house was not going to be eaten in two weeks .', 'In school , we spend a lot of time to learn about the rest of the <UNK> <UNK> , but not to learn a lot of the outside world , except the United States , and Japan is our enemies .', 'I can t talk about how I ve been talking about how to get out of the way to say , in the <UNK> , because of that famine , I m sending Chinese to live in China to live with a professional relatives .', 'Although I ve been wondering , I knew that I was living in the outside world , but I still think I m going to live in the entire life of <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 146m 5s (- -147m 58s), Epoch: [4/20], Step: [2000/2015], Train Loss: 2.3371864342689515\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046]\n",
            "Time: 148m 6s (- -147m 22s), Epoch: [5/20], Step: [100/2015], Train Loss: 2.0617164266109467\n",
            "Time: 149m 53s (- -150m 51s), Epoch: [5/20], Step: [200/2015], Train Loss: 2.049697074890137\n",
            "Time: 151m 40s (- -152m 49s), Epoch: [5/20], Step: [300/2015], Train Loss: 2.0705888831615447\n",
            "Time: 153m 27s (- -154m 55s), Epoch: [5/20], Step: [400/2015], Train Loss: 2.067497662305832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.103048509897953 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happened to life s life from North Korea , and then I realized that it was difficult , but I realized that it was difficult , because refugees was very difficult , because the refugees was so hard to go into', 'In that person wrote when you read these lines , all the five five people s family have no longer on the top anymore , because the house has nothing to eat in two weeks .', 'In school , we spend a lot of time learning about the life of the <UNK> II , but not to learn a lot of the outside world , except the United States , and Japan is our enemies .', 'I can t speak specifically about myself from North Korea , how to <UNK> out of the <UNK> , how powerful I was sent to China to live with a long term to live with a long term .', 'Although I ve been wondering who doesn t know the world outside , but I still think I m going to live life at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 155m 25s (- -156m 53s), Epoch: [5/20], Step: [500/2015], Train Loss: 2.0953902685642243\n",
            "Time: 157m 14s (- -157m 1s), Epoch: [5/20], Step: [600/2015], Train Loss: 2.095082380771637\n",
            "Time: 159m 1s (- -159m 11s), Epoch: [5/20], Step: [700/2015], Train Loss: 2.104841389656067\n",
            "Time: 160m 47s (- -161m 24s), Epoch: [5/20], Step: [800/2015], Train Loss: 2.108525266647339\n",
            "Time: 162m 34s (- -163m 36s), Epoch: [5/20], Step: [900/2015], Train Loss: 2.115828613042831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.12001854734058 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so nervous from what happened to a refugee from North Korea , which was going to be , but I realized that it was hard to be hard , because from North Koreans was so hard to go into China s <UNK> , and so', 'And in that , when I was writing this <UNK> , he had read these <UNK> , and I didn t have any more to eat in two weeks .', 'In school , we spent a lot of time to learn about how the work of the chairman Kim , but not to learn a lot of the outside world , except the United States , Korea and Japan is our enemies .', 'I can t speak to the specific idea that I ve been around from North Korea , and how famines I m sending to China is that the <UNK> of the <UNK> , because he s sent to live with a <UNK> .', 'Although I used to wonder how the outside world is , but I still think I ll live a whole life in <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 164m 30s (- -165m 39s), Epoch: [5/20], Step: [1000/2015], Train Loss: 2.0983297526836395\n",
            "Time: 166m 18s (- -167m 50s), Epoch: [5/20], Step: [1100/2015], Train Loss: 2.1051673281192778\n",
            "Time: 168m 3s (- -168m 4s), Epoch: [5/20], Step: [1200/2015], Train Loss: 2.096990587711334\n",
            "Time: 169m 48s (- -170m 19s), Epoch: [5/20], Step: [1300/2015], Train Loss: 2.1259265625476838\n",
            "Time: 171m 34s (- -172m 33s), Epoch: [5/20], Step: [1400/2015], Train Loss: 2.1130559027194975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.429060830741058 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happened to life s life from North Korea , how would I soon realize it s very difficult , but I soon realized it was difficult , because refugees was very difficult , because refugees from North Korea was considered the', 'And in that time , when you read these lines , all the five people of the kid who didn t have any of the same family , because the house didn t have anything to eat in two weeks .', 'In school , we spent a lot of time to learn about the rest of the <UNK> War II , but not to learn a lot about the world outside , except the United States , and Japan is our enemies .', 'I can t talk to a specific talk about how I escaped the North , how could only say in the <UNK> of <UNK> because of the starvation I was sent to China to live with a <UNK> .', 'Although I ve been asking myself , I knew how the outside world outside , but I still think I m going to live my life in <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 173m 31s (- -174m 35s), Epoch: [5/20], Step: [1500/2015], Train Loss: 2.1166273653507233\n",
            "Time: 175m 18s (- -176m 48s), Epoch: [5/20], Step: [1600/2015], Train Loss: 2.098463340997696\n",
            "Time: 177m 4s (- -177m 2s), Epoch: [5/20], Step: [1700/2015], Train Loss: 2.132683435678482\n",
            "Time: 178m 49s (- -179m 16s), Epoch: [5/20], Step: [1800/2015], Train Loss: 2.1193626773357392\n",
            "Time: 180m 34s (- -181m 31s), Epoch: [5/20], Step: [1900/2015], Train Loss: 2.1295483899116516\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.535279395010622 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happened to the lives of a refugee , but I soon realized that it was very difficult , but I soon realized it was very difficult , because refugees was dangerous , because refugees were <UNK> in China s <UNK> .', 'And so there s a <UNK> when you read these lines , all the five people s families who are not on the top of this , because the house has nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the rest of the chairman of the <UNK> <UNK> , but not to learn a lot about the outside world , except the United States , Korea and Japan are our enemies .', 'I can t talk about myself in my own feet away from North Korea , how to be able to say in the <UNK> , because I m sending Chinese to live with a poor person .', 'Although I ve had a clue that I had no idea how the outside world outside , but I still think I m going to live my life in <UNK> , until everything is changing .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 182m 34s (- -183m 30s), Epoch: [5/20], Step: [2000/2015], Train Loss: 2.137136303186417\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622]\n",
            "Time: 184m 38s (- -183m 12s), Epoch: [6/20], Step: [100/2015], Train Loss: 1.8565235352516174\n",
            "Time: 186m 26s (- -186m 29s), Epoch: [6/20], Step: [200/2015], Train Loss: 1.8548520910739899\n",
            "Time: 188m 13s (- -188m 24s), Epoch: [6/20], Step: [300/2015], Train Loss: 1.8792708277702332\n",
            "Time: 190m 0s (- -190m 27s), Epoch: [6/20], Step: [400/2015], Train Loss: 1.8623106956481934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.344630418020166 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happens to life from the outside , and then I soon realize that it s difficult to be difficult , but I soon realized it s difficult , because refugees are dangerous , because the refugees from North Korea in China', 'And so there s when you write this little boy , and I didn t have to read this on the other , because the house was not going to be eaten for two weeks .', 'In school , we spent a lot of time to learn about the rest of the Cold War II , but not to learn a lot of the outside of the outside world , except for Korea and Japan are our enemies .', 'I can t talk about how I ve got to hide out of the river , how to be able to say in <UNK> because of how famine I m sent to China to live with a professional relatives .', 'Though I ve ever wondered the world outside of the other life , but I still think I m going to live in the entire life of <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 191m 56s (- -192m 26s), Epoch: [6/20], Step: [500/2015], Train Loss: 1.8781508362293244\n",
            "Time: 193m 41s (- -194m 37s), Epoch: [6/20], Step: [600/2015], Train Loss: 1.8950611221790314\n",
            "Time: 195m 27s (- -196m 49s), Epoch: [6/20], Step: [700/2015], Train Loss: 1.8983384335041047\n",
            "Time: 197m 15s (- -198m 59s), Epoch: [6/20], Step: [800/2015], Train Loss: 1.8930661845207215\n",
            "Time: 199m 2s (- -199m 10s), Epoch: [6/20], Step: [900/2015], Train Loss: 1.9216615200042724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.34086018427795 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happened to the life of a refugee , and it s a very difficult time , but I soon realized that it was difficult , because the refugee North Korea was so difficult , because the refugee North Korea in China', 'And in that moment , when you read these lines , the family of the kid who didn t have to have been on the top of this , because the house had nothing to eat in two weeks .', 'At school , we spent a lot of time to learn about the work of the chairman World War II , but not to learn many of the world , except the United States , Korea and Japan are our enemies .', 'I can t talk about myself in my own , and how to hide myself from North Korea , how powerful I m going to be in Chinese <UNK> because of China to live with a <UNK> of a <UNK> .', 'Although I had wondered who had no idea the world outside , but I still think I would live the entire life in <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 200m 59s (- -201m 12s), Epoch: [6/20], Step: [1000/2015], Train Loss: 1.9328264999389648\n",
            "Time: 202m 46s (- -203m 24s), Epoch: [6/20], Step: [1100/2015], Train Loss: 1.919315801858902\n",
            "Time: 204m 32s (- -205m 38s), Epoch: [6/20], Step: [1200/2015], Train Loss: 1.9148978328704833\n",
            "Time: 206m 19s (- -207m 50s), Epoch: [6/20], Step: [1300/2015], Train Loss: 1.9321985995769502\n",
            "Time: 208m 5s (- -208m 3s), Epoch: [6/20], Step: [1400/2015], Train Loss: 1.9459702718257903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.337118859501036 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had been unthinkable to the life of a refugee refugee , like , but I soon realized it was difficult , but I realized it was difficult , because refugees was very difficult , because refugees were <UNK> , because the refugee North Koreans went into', 'And there was a time when she was reading these lines , and my family were not there in this country , because the house was not there in two weeks .', 'In school , we spent a lot of time to learn about the rest of the chairman of the Cold War II , but not so much about the outside world , except the United States , and Japan was our enemies .', 'I couldn t speak specifically about myself out of the way , and how to get away from North Korea , how could I say in <UNK> because of the Chinese famine , and live in China , to live with a <UNK> .', 'Although I ve been wondering , I ve been wondering , I ve been wondering , but I still think I m going to live my entire life in <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 210m 2s (- -210m 6s), Epoch: [6/20], Step: [1500/2015], Train Loss: 1.9551568865776061\n",
            "Time: 211m 49s (- -212m 18s), Epoch: [6/20], Step: [1600/2015], Train Loss: 1.929237940311432\n",
            "Time: 213m 36s (- -214m 30s), Epoch: [6/20], Step: [1700/2015], Train Loss: 1.953650802373886\n",
            "Time: 215m 22s (- -216m 44s), Epoch: [6/20], Step: [1800/2015], Train Loss: 1.966584758758545\n",
            "Time: 217m 8s (- -218m 58s), Epoch: [6/20], Step: [1900/2015], Train Loss: 1.951790715456009\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.726164272913383 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had a <UNK> to a refugee person from the North , and then I realized that it was very difficult , but I realized it was very difficult , because the refugee came from North Koreans , were considered refugees , because the refugee came from', 'And in that time , when she was going to read these lines , they were all over five percent of the baby who didn t have anything to eat in two weeks .', 'In school , we spend a lot of time to learn about the rest of the world s life , but not to many of the world s world , except the United States , South Korea and Japan is our enemies .', 'I can t speak specifically about my own , and how to get away from North Korea , how powerful I was being sent to China to live with a <UNK> .', 'Although I had wonder , I knew that I had no idea of the world outside , but I still thought I d live my life at <UNK> <UNK> , until all of these suddenly suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 219m 6s (- -220m 59s), Epoch: [6/20], Step: [2000/2015], Train Loss: 1.9537723827362061\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383]\n",
            "Time: 221m 9s (- -219m 2s), Epoch: [7/20], Step: [100/2015], Train Loss: 1.7028202176094056\n",
            "Time: 222m 56s (- -222m 10s), Epoch: [7/20], Step: [200/2015], Train Loss: 1.6986923587322236\n",
            "Time: 224m 41s (- -224m 3s), Epoch: [7/20], Step: [300/2015], Train Loss: 1.695651432275772\n",
            "Time: 226m 25s (- -226m 8s), Epoch: [7/20], Step: [400/2015], Train Loss: 1.716880089044571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.32138746461635 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had a <UNK> to come with the life of a refugee refugee , like , but I soon realize that it was difficult , but I soon realized that it was difficult , because the refugees from North Korea <UNK> to China s immigrants is considered', 'And so there was a <UNK> when she was reading these lines , and my family were not about five percent of the children s lives , because the house was not so much to eat in two weeks .', 'In school , we spend a lot of time to learn about the rest of the Cold War II , but not to learn so much about the world outside II , except for Korea and Japan are our enemies .', 'I can t speak specifically about my job as a matter of how to hide the North Korea that I was sent to China to live with a <UNK> of a <UNK> .', 'Although I ve ever wondered , I knew the world outside of the other , but I still thought I d lived the lives in <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 228m 22s (- -228m 4s), Epoch: [7/20], Step: [500/2015], Train Loss: 1.7196457016468047\n",
            "Time: 230m 9s (- -230m 13s), Epoch: [7/20], Step: [600/2015], Train Loss: 1.727235929965973\n",
            "Time: 231m 55s (- -232m 24s), Epoch: [7/20], Step: [700/2015], Train Loss: 1.7599261558055879\n",
            "Time: 233m 41s (- -234m 36s), Epoch: [7/20], Step: [800/2015], Train Loss: 1.7705839931964875\n",
            "Time: 235m 27s (- -236m 48s), Epoch: [7/20], Step: [900/2015], Train Loss: 1.7537607955932617\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.429517050007647 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> what happened to a refugee from North Korea , how well I was soon realized that it was difficult , but I realized that it was difficult , because people from North Korea North Korea <UNK> in China s <UNK> , was considered a', 'And so there was a <UNK> when she was reading these lines , and my family were not about it on the ground , because the house didn t have anything to eat in two weeks .', 'In school , we spent a lot of time to learn about the rest of the chairman of Kim II Kim , but not to learn a lot about the outside world , except the United States , South Korea and Japan are our enemies .', 'I couldn t talk about how I had escaped hiding from North Korea and the <UNK> , how I was to send that in <UNK> because of his face to live with a long term .', 'Although I had wondered how the world outside the outside world , but I still think I d lived the lives of my life at <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 237m 22s (- -238m 51s), Epoch: [7/20], Step: [1000/2015], Train Loss: 1.7708887457847595\n",
            "Time: 239m 7s (- -239m 5s), Epoch: [7/20], Step: [1100/2015], Train Loss: 1.7728872978687287\n",
            "Time: 240m 55s (- -241m 16s), Epoch: [7/20], Step: [1200/2015], Train Loss: 1.788774688243866\n",
            "Time: 242m 40s (- -243m 30s), Epoch: [7/20], Step: [1300/2015], Train Loss: 1.7851657092571258\n",
            "Time: 244m 28s (- -245m 42s), Epoch: [7/20], Step: [1400/2015], Train Loss: 1.7922867512702942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.492243093958223 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so passionate about what happened to a refugee person from North Korea , which was , but I soon realized that it was very difficult , because the refugee people from North Korea , the North Pole , the refugee North Koreans , was considered', 'And in that time , when she wrote these lines , all the five children of whom were not on the top of this , because the house was not there for dinner .', 'In school , we spent a lot of time to learn about the rest of the spectrum Kim <UNK> , but not to learn so much about the outside world , except the United States , Korea and Japan .', 'I can t talk to the concrete about myself hiding out of the <UNK> , how I can tell you that in <UNK> because of the famine of starvation I m sending to live in China with a <UNK> .', 'Although I had wonder who was wondering , I was thinking , but I still think I m going to live in the entire life at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 246m 22s (- -247m 47s), Epoch: [7/20], Step: [1500/2015], Train Loss: 1.7872923040390014\n",
            "Time: 248m 8s (- -248m 0s), Epoch: [7/20], Step: [1600/2015], Train Loss: 1.8038247096538544\n",
            "Time: 249m 53s (- -250m 14s), Epoch: [7/20], Step: [1700/2015], Train Loss: 1.8013761413097382\n",
            "Time: 251m 39s (- -252m 29s), Epoch: [7/20], Step: [1800/2015], Train Loss: 1.7810795712471008\n",
            "Time: 253m 25s (- -254m 42s), Epoch: [7/20], Step: [1900/2015], Train Loss: 1.8232702660560607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.95198221733016 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been unthinkable all about the lives of a refugee speaker from North Korea , which is how much it is , but I soon realized it s difficult , because refugees are <UNK> , because the refugee North Koreans is considered <UNK> .', 'And in that time , when you write reading these lines , my family were not there in the air , because the house had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the philosophy of the <UNK> Kim of the outside Wall , except the United States , and Japan and Japan is our enemy .', 'I can t talk about how I escaped from North Korea , how to say in these <UNK> <UNK> because of my face , I was sent to China to live with a <UNK> person .', 'And although I had never been wondering , I knew that I was going to live my life in <UNK> <UNK> , until everything suddenly was suddenly in <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 255m 23s (- -256m 43s), Epoch: [7/20], Step: [2000/2015], Train Loss: 1.8078478348255158\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016]\n",
            "Time: 257m 21s (- -255m 12s), Epoch: [8/20], Step: [100/2015], Train Loss: 1.5788678085803987\n",
            "Time: 259m 6s (- -258m 11s), Epoch: [8/20], Step: [200/2015], Train Loss: 1.5577323305606843\n",
            "Time: 260m 51s (- -260m 0s), Epoch: [8/20], Step: [300/2015], Train Loss: 1.5799263536930084\n",
            "Time: 262m 38s (- -262m 1s), Epoch: [8/20], Step: [400/2015], Train Loss: 1.5869789838790893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.75933936158385 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been unthinkable with the life of a refugee person , and it s like me soon it s very hard , but I soon realized that it s hard to be difficult , because people from North Korea <UNK> in China is considered <UNK> .', 'And in those same <UNK> When read these lines , they were all threatened , because the whole family was not on the earth to eat two weeks .', 'In school , we spend a lot of time learning about the rest of the spectrum of Kim <UNK> , but not to learn so much about the world outside of the outside War , and Japan and Japan is our enemy .', 'I can t speak specifically to speak about myself from North Korea , how I ve been to just say in <UNK> because of the famine that I m sent to China to live with a <UNK> person .', 'Although I had never been wondering , I knew the outside world outside , but I still thought I would live a whole life at <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 264m 33s (- -265m 58s), Epoch: [8/20], Step: [500/2015], Train Loss: 1.591220144033432\n",
            "Time: 266m 17s (- -266m 9s), Epoch: [8/20], Step: [600/2015], Train Loss: 1.6094828402996064\n",
            "Time: 268m 2s (- -268m 20s), Epoch: [8/20], Step: [700/2015], Train Loss: 1.612627568244934\n",
            "Time: 269m 46s (- -270m 33s), Epoch: [8/20], Step: [800/2015], Train Loss: 1.6159301161766053\n",
            "Time: 271m 31s (- -272m 46s), Epoch: [8/20], Step: [900/2015], Train Loss: 1.6293412172794342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.96892625408528 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been unthinkable with the life of a refugee from the North , and the <UNK> , but I soon realized that it s very hard , because people from North Korea <UNK> , who s very hard to Northern China s immigrants are considered a', 'And in that time when she was reading these lines , all the five people who were not there on the earth were five percent of the baby , because the house had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the rest of the Cold War War II , but not to learn a lot of the world outside the outside , except for Korea and Japan are our enemies .', 'I can t talk specifically about my own <UNK> , and how to <UNK> the <UNK> , and just say in <UNK> , I m going to go to China to live with a stranger .', 'Although I ve been wondering , I knew the world outside , but I still think I m going to live my life at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 273m 29s (- -274m 47s), Epoch: [8/20], Step: [1000/2015], Train Loss: 1.6347821176052093\n",
            "Time: 275m 16s (- -276m 58s), Epoch: [8/20], Step: [1100/2015], Train Loss: 1.6579421961307526\n",
            "Time: 277m 3s (- -277m 10s), Epoch: [8/20], Step: [1200/2015], Train Loss: 1.6465863192081451\n",
            "Time: 278m 50s (- -279m 22s), Epoch: [8/20], Step: [1300/2015], Train Loss: 1.6662315845489502\n",
            "Time: 280m 35s (- -281m 36s), Epoch: [8/20], Step: [1400/2015], Train Loss: 1.667303183078766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.622852791504858 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been unthinkable in the life of a refugee from the North , and the <UNK> soon will realize that it s very difficult , because refugees is dangerous , because refugees from North Korea <UNK> , because refugees are <UNK> of the <UNK> of the', 'And in that time , when you read the back of these lines , all of you had no longer on this earth , because my house had nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the <UNK> of the <UNK> War II , but not to learn a lot of the world outside La <UNK> , except in South Korea and Japan are our enemies .', 'I can t speak specifically to how I escaped from North Korea the North Ocean , how powerful I was sent to China was to live in Chinese <UNK> to live with a <UNK> of <UNK> .', 'Though I ve been asking , I ve been asking , I still wonder that I m going to live my life at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 282m 31s (- -283m 39s), Epoch: [8/20], Step: [1500/2015], Train Loss: 1.6734172010421753\n",
            "Time: 284m 17s (- -285m 52s), Epoch: [8/20], Step: [1600/2015], Train Loss: 1.6837221813201904\n",
            "Time: 286m 4s (- -286m 5s), Epoch: [8/20], Step: [1700/2015], Train Loss: 1.6577831280231476\n",
            "Time: 287m 49s (- -288m 19s), Epoch: [8/20], Step: [1800/2015], Train Loss: 1.682752501964569\n",
            "Time: 289m 35s (- -290m 33s), Epoch: [8/20], Step: [1900/2015], Train Loss: 1.688077710866928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.913059310120946 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been so passionate about the lives of a refugee person , and I soon realized how it s very difficult , but I soon realized it s very difficult , because refugees are dangerous , because refugees from North Korea China is considered to be', 'And so there s a <UNK> when you read these lines , all the five five people who are not on this earth , because the house has nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the philosophy of the <UNK> World War II , but not to learn a lot of the outside world , except the United States , Korea and Japan was our enemies .', 'I can t speak specifically about myself in the North Ocean that <UNK> up in the <UNK> of hunger that famine me on the Chinese , so how famine I was sent to China to live with a <UNK> .', 'Although I ve been asking , I ve been asking , I still knew that I would live lives at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 291m 31s (- -292m 37s), Epoch: [8/20], Step: [2000/2015], Train Loss: 1.6885614478588105\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946]\n",
            "Time: 293m 32s (- -291m 23s), Epoch: [9/20], Step: [100/2015], Train Loss: 1.4541558527946472\n",
            "Time: 295m 18s (- -294m 9s), Epoch: [9/20], Step: [200/2015], Train Loss: 1.4559979665279388\n",
            "Time: 297m 5s (- -297m 54s), Epoch: [9/20], Step: [300/2015], Train Loss: 1.4432102715969086\n",
            "Time: 298m 50s (- -299m 54s), Epoch: [9/20], Step: [400/2015], Train Loss: 1.4765755999088288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.8204747842089 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been unthinkable coming to the lives of a refugee from the <UNK> , and the <UNK> soon I soon realized how it is , because refugees is extremely dangerous , because refugees from North Korea China is considered a little <UNK> of the time .', 'And there was a time when she was reading these lines , all the five five people who were not there anymore , because they didn t have any time to go to dinner for two weeks .', 'In school , we spend a lot of time to learn about the life of the Cold War II , but not learn many of the world outside the outside War , except in South Korea and Japan is our enemies .', 'I can t speak specifically to how I hide out of North Korea , how to hide how the North Pole is , I m going to send China for the <UNK> of a <UNK> .', 'Though I was supposed to wonder how the world outside , but I still think I d lived the lives of my life at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 300m 46s (- -301m 49s), Epoch: [9/20], Step: [500/2015], Train Loss: 1.4979748916625977\n",
            "Time: 302m 32s (- -303m 57s), Epoch: [9/20], Step: [600/2015], Train Loss: 1.4710227572917938\n",
            "Time: 304m 20s (- -304m 5s), Epoch: [9/20], Step: [700/2015], Train Loss: 1.4991789805889129\n",
            "Time: 306m 5s (- -306m 17s), Epoch: [9/20], Step: [800/2015], Train Loss: 1.5151822185516357\n",
            "Time: 307m 53s (- -308m 27s), Epoch: [9/20], Step: [900/2015], Train Loss: 1.5325608706474305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 21.01076791534983 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been <UNK> what happened to the life of a refugee from the <UNK> , but I soon realized that it was difficult , it was very difficult because the refugee North Koreans was so hard , because the refugee North Koreans went into China s', 'And then , when she wrote reading these lines , they were all over the five , because his family had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the philosophy of the world s life World War II , but not to learn a lot about the external world , except for Korea and Japan is our enemy .', 'I can t talk to a certain thing that I ve had from the North Sea , how can only say in <UNK> because of his hunger , I m sent to China to live with a <UNK> person .', 'Though I ve ever wondered , I ve been wondering , but I still think I m going to live my life at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 309m 49s (- -310m 28s), Epoch: [9/20], Step: [1000/2015], Train Loss: 1.519618537425995\n",
            "Time: 311m 37s (- -312m 39s), Epoch: [9/20], Step: [1100/2015], Train Loss: 1.5309972202777862\n",
            "Time: 313m 23s (- -314m 52s), Epoch: [9/20], Step: [1200/2015], Train Loss: 1.5308700954914094\n",
            "Time: 315m 9s (- -315m 5s), Epoch: [9/20], Step: [1300/2015], Train Loss: 1.5576044404506684\n",
            "Time: 316m 53s (- -317m 20s), Epoch: [9/20], Step: [1400/2015], Train Loss: 1.5581949520111085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.64128421707714 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been unthinkable in the life of a refugee from North Korea s , but I soon realized it was difficult , it was difficult , because refugees was very difficult , because refugees was so difficult , because the refugee North Koreans went into China', 'And in that writing when she read these lines , my family were not about the five of their lives , because the house had nothing to do to eat in two weeks .', 'In school , we spend a lot of time to learn about the work of the <UNK> <UNK> <UNK> , but not to learn so much about the world outside of the outside , except in conclusion , in Mexico and Japan are our enemies .', 'I can t speak specifically about myself from North Korea from North Korea , how do I say in <UNK> because of the hunger that I m sent to live in China with a <UNK> .', 'Although I d been wondering , I knew the outside world , but I still think I d lived a whole life in <UNK> , until all suddenly suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 318m 48s (- -319m 24s), Epoch: [9/20], Step: [1500/2015], Train Loss: 1.5728148460388183\n",
            "Time: 320m 33s (- -321m 38s), Epoch: [9/20], Step: [1600/2015], Train Loss: 1.5736285483837127\n",
            "Time: 322m 18s (- -323m 52s), Epoch: [9/20], Step: [1700/2015], Train Loss: 1.5810494637489318\n",
            "Time: 324m 6s (- -324m 4s), Epoch: [9/20], Step: [1800/2015], Train Loss: 1.5566812646389008\n",
            "Time: 325m 52s (- -326m 17s), Epoch: [9/20], Step: [1900/2015], Train Loss: 1.5946787548065187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.820391228298924 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of solved what happened to the lives of a refugee , but I soon realized it was very difficult , but I soon realized it was very difficult , but I soon realized that it was very difficult , because the refugee North Koreans', 'And in that , when she wrote this line , my family was not about five percent of the age of the kid who didn t have any more to eat in two weeks .', 'In school , we spent a lot of time to learn about the <UNK> of the <UNK> War II , but not to learn so much about the world outside of the outside , except in Korea and Japan was our enemies .', 'I can t speak specifically about my work that backs up from North Korea in <UNK> because of the <UNK> , and starvation that I m sent to the Chinese to live with a long term .', 'And I ve been asking myself , I ve been asking myself , but I still think I m going to live my life at <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 327m 49s (- -328m 20s), Epoch: [9/20], Step: [2000/2015], Train Loss: 1.603601177930832\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924]\n",
            "Time: 329m 49s (- -327m 28s), Epoch: [10/20], Step: [100/2015], Train Loss: 1.3647971260547638\n",
            "Time: 331m 35s (- -330m 3s), Epoch: [10/20], Step: [200/2015], Train Loss: 1.3588657760620118\n",
            "Time: 333m 20s (- -333m 46s), Epoch: [10/20], Step: [300/2015], Train Loss: 1.3754769384860992\n",
            "Time: 335m 6s (- -335m 43s), Epoch: [10/20], Step: [400/2015], Train Loss: 1.3778290963172912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.393057728241306 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so passionate about a life of a refugee refugee , and it s very difficult to say , but I soon realized that it was difficult , but I realized that it was difficult , because refugees were very difficult from North Korea , and', 'And so there was a When she was going to read these lines , all the five five people who were not there anymore , because both houses hadn t been to eat in two weeks .', 'In school , we spend a lot of time to learn about the work of the puppet Kim II , but not to learn so much about the outside world , except the United States , and Japan is our enemies .', 'I can t speak specifically about having to hide how to hide with North Korea , how can only say that in <UNK> years from that face to China to live with a long way of living and live with a long distance .', 'Although I was wondering , I knew that I knew that I was going to live my life in <UNK> <UNK> , until everything suddenly was changing .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 337m 3s (- -337m 36s), Epoch: [10/20], Step: [500/2015], Train Loss: 1.3856454658508301\n",
            "Time: 338m 50s (- -339m 43s), Epoch: [10/20], Step: [600/2015], Train Loss: 1.4008194720745086\n",
            "Time: 340m 36s (- -341m 53s), Epoch: [10/20], Step: [700/2015], Train Loss: 1.4057805943489075\n",
            "Time: 342m 22s (- -342m 3s), Epoch: [10/20], Step: [800/2015], Train Loss: 1.4246281492710113\n",
            "Time: 344m 7s (- -344m 15s), Epoch: [10/20], Step: [900/2015], Train Loss: 1.433928142786026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.5932495949785 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was <UNK> what happened to a refugee from North Korea , and it was very difficult , but I soon realized that it was difficult , because the refugee Soviet North Koreans went into China s <UNK> , was considered the migrant of his life ,', 'And there s a little bit when you read these lines , all the five five people who have no longer left this way , because both home is not having to eat in two weeks .', 'In school , we spent a lot of time to learn about the <UNK> of Kim , the <UNK> <UNK> , but not to learn many , many , many , and Japan is our enemies .', 'I can t speak specifically about myself from North Korea from North Korea . What can I say in these <UNK> of <UNK> , because I m sent to China to live with a long distance of their <UNK> .', 'Although I had never been wondering , I knew , I was thinking , but I still think I d lived all my life at <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 346m 3s (- -346m 16s), Epoch: [10/20], Step: [1000/2015], Train Loss: 1.4290867161750793\n",
            "Time: 347m 49s (- -348m 28s), Epoch: [10/20], Step: [1100/2015], Train Loss: 1.43723428606987\n",
            "Time: 349m 33s (- -350m 44s), Epoch: [10/20], Step: [1200/2015], Train Loss: 1.4574389481544494\n",
            "Time: 351m 18s (- -352m 57s), Epoch: [10/20], Step: [1300/2015], Train Loss: 1.4441010534763337\n",
            "Time: 353m 4s (- -353m 10s), Epoch: [10/20], Step: [1400/2015], Train Loss: 1.4569580972194671\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.247145853502477 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of taken to what happened to a refugee from North Korea , how would I soon realize that it was difficult , but I soon realized it was difficult , because the refugee people from North Korea <UNK> were considered a <UNK> migrant immigrant', 'And there was a <UNK> when she was read these flows , all the five five children who were not on the earth , because the house had nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the puppet of the Cold War II , but not to learn a lot about the external world , except the United States , and Japan was our enemies .', 'I can t speak specifically about myself from North Korea from North Korea , how the North is just saying that in <UNK> , I was sent to Chinese to live with a <UNK> .', 'Even though I was wondering , I was wondering , but I still thought I d lived a life in <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 354m 59s (- -355m 14s), Epoch: [10/20], Step: [1500/2015], Train Loss: 1.4633824741840362\n",
            "Time: 356m 43s (- -357m 30s), Epoch: [10/20], Step: [1600/2015], Train Loss: 1.4750519907474517\n",
            "Time: 358m 28s (- -359m 43s), Epoch: [10/20], Step: [1700/2015], Train Loss: 1.4830340957641601\n",
            "Time: 360m 15s (- -361m 56s), Epoch: [10/20], Step: [1800/2015], Train Loss: 1.4740937745571137\n",
            "Time: 362m 2s (- -362m 9s), Epoch: [10/20], Step: [1900/2015], Train Loss: 1.4846405971050263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.37393989736967 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I d been hiding what happened to a refugee situation from North Korea , and I realized that it was difficult to be very difficult , because the refugee displaced asylum , which was very dangerous , because the refugee North Koreans went into China s <UNK>', 'And so there s a little kid who wrote these flows , and they re my five friends who are not on the verge , because even my house has nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the flight of the Cold War II , but not learn so much about the outside world , except the United States , Korea and Japan is our enemies .', 'I cannot speak specifically about my job as a matter of how I escaped the North Koreans , just to say , in Chinese years , I was sent to China to live with a long way of being a <UNK> .', 'Though I d been wondering , I knew the world was how I thought I d lived in the entire life in <UNK> <UNK> , until everything suddenly suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 363m 58s (- -364m 12s), Epoch: [10/20], Step: [2000/2015], Train Loss: 1.4874945342540742\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967]\n",
            "Time: 365m 59s (- -363m 39s), Epoch: [11/20], Step: [100/2015], Train Loss: 1.2803919172286988\n",
            "Time: 367m 43s (- -366m 6s), Epoch: [11/20], Step: [200/2015], Train Loss: 1.270534497499466\n",
            "Time: 369m 27s (- -369m 46s), Epoch: [11/20], Step: [300/2015], Train Loss: 1.2861195051670073\n",
            "Time: 371m 13s (- -371m 42s), Epoch: [11/20], Step: [400/2015], Train Loss: 1.2980123114585878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.662545584619938 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I ve been looking at what happened to life s life , and it s going to be like , but I soon realized it s difficult , because refugees is so hard , because refugees is so hard , because the refugee North Koreans went into', 'And there s a key blog when she read these flows , and the whole five percent of the child who s not on this earth is because the house has nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the narrative of Kim La <UNK> , but not so much about the outside world , except the United States , South Korea and Japan are our enemies .', 'I cannot speak specifically about myself from North Korea from North Korea , how can only say in these <UNK> <UNK> because I m sending to Chinese to live with a long way to live a relative person .', 'Although I ve been asking , though I don t know the world outside , but I still think I m going to live my life at <UNK> <UNK> , until everything suddenly changes .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 373m 8s (- -373m 36s), Epoch: [11/20], Step: [500/2015], Train Loss: 1.3017594599723816\n",
            "Time: 374m 56s (- -375m 41s), Epoch: [11/20], Step: [600/2015], Train Loss: 1.3210815322399139\n",
            "Time: 376m 42s (- -377m 50s), Epoch: [11/20], Step: [700/2015], Train Loss: 1.3173403429985047\n",
            "Time: 378m 27s (- -378m 0s), Epoch: [11/20], Step: [800/2015], Train Loss: 1.3402852249145507\n",
            "Time: 380m 12s (- -380m 12s), Epoch: [11/20], Step: [900/2015], Train Loss: 1.3310812723636627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.032047031084147 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so <UNK> what happened to life with the life of a refugee , but I soon realized that it was difficult , it was difficult , because the refugee North <UNK> went from North Korea <UNK> , because the refugee North Koreans went into China', 'And so there was a revelation when she was reading these lines , all my family five , because both of them had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the <UNK> of the <UNK> War II , but not in many of the outside world , except the United States , South Korea and Japan are our enemies .', 'I couldn t speak specifically about having been hiding out of North Korea , just to say in <UNK> years , so how hunger I was sent to China to live with a <UNK> of long term <UNK> .', 'Although I had no wonder , I knew the other life outside , but I was thinking , I would live the lives of my life in <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 382m 9s (- -382m 13s), Epoch: [11/20], Step: [1000/2015], Train Loss: 1.3384831857681274\n",
            "Time: 383m 55s (- -384m 25s), Epoch: [11/20], Step: [1100/2015], Train Loss: 1.3517868900299073\n",
            "Time: 385m 40s (- -386m 39s), Epoch: [11/20], Step: [1200/2015], Train Loss: 1.3771527576446534\n",
            "Time: 387m 27s (- -388m 50s), Epoch: [11/20], Step: [1300/2015], Train Loss: 1.3710214865207673\n",
            "Time: 389m 12s (- -389m 3s), Epoch: [11/20], Step: [1400/2015], Train Loss: 1.3743843924999237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.96555524601572 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was constantly coming to life with a refugee from the <UNK> , and it would take me to realize that it was difficult , but I realized it was difficult , because the refugee North Koreans went into China s <UNK> , was considered a <UNK>', 'And so there s a <UNK> when you read these lines , all the five five children who are not on this earth , because the house has nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the work of the Cold War II , but not to learn a lot of external world , except the United States , South Korea and Japan was our enemies .', 'I can t speak specifically about myself out of North Korea and the <UNK> of the river , just to say in <UNK> because I m sending to live with Chinese to live a long way to live a relative person .', 'Though I was telling you , I knew that I had no idea the world outside , but I was thinking , I d lived even in <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 391m 9s (- -391m 6s), Epoch: [11/20], Step: [1500/2015], Train Loss: 1.3715105068683624\n",
            "Time: 392m 54s (- -393m 19s), Epoch: [11/20], Step: [1600/2015], Train Loss: 1.3850115275382995\n",
            "Time: 394m 42s (- -395m 31s), Epoch: [11/20], Step: [1700/2015], Train Loss: 1.3971905052661895\n",
            "Time: 396m 29s (- -397m 43s), Epoch: [11/20], Step: [1800/2015], Train Loss: 1.4074196565151214\n",
            "Time: 398m 17s (- -399m 54s), Epoch: [11/20], Step: [1900/2015], Train Loss: 1.4060429728031159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.904359010168914 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so passionate about the life of a refugee person in the North , and it would take me that it was difficult , but I soon realized it was difficult , because the refugee North Koreans went into China s <UNK> , was considered a', 'And in that sense when she reads these lines , there are my children five people who are not on this earth , because even my house has nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the semester of the Cold War II , but not to learn a lot about the world outside War II , except in South Korea , in South Korea , in South Korea , in', 'I can t speak specifically about my work that has gone out of North Korea and just say in <UNK> years because I m sent to the Chinese , so how to live with a long term .', 'Though I was able to wonder how the outer world was , but I still think that I will live my life in <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 400m 13s (- -401m 58s), Epoch: [11/20], Step: [2000/2015], Train Loss: 1.3928918635845184\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914]\n",
            "Time: 402m 14s (- -399m 46s), Epoch: [12/20], Step: [100/2015], Train Loss: 1.1966431587934494\n",
            "Time: 404m 0s (- -402m 0s), Epoch: [12/20], Step: [200/2015], Train Loss: 1.198677053451538\n",
            "Time: 405m 45s (- -405m 35s), Epoch: [12/20], Step: [300/2015], Train Loss: 1.2160479140281677\n",
            "Time: 407m 30s (- -407m 30s), Epoch: [12/20], Step: [400/2015], Train Loss: 1.217797999382019\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.797170018112023 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of going to be all about a refugee from North Korea s life refugee , and it would soon realize that it was difficult , but I soon realized it was difficult , because the refugee North Koreans went into China s migrant <UNK>', 'And so there was a time when she was reading these lines , all my family five people who were not there anymore , because he had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the <UNK> of the Cold War II , but not to learn a lot of the outside world , except the United States , and Japan is our enemies .', 'I can t speak specifically about my job from North Korea from the North Pole , how can only tell you in Chinese years because of the hunger I m sent to live in China with a <UNK> of a <UNK> .', 'And although I d been wondering , I knew that I was going to live in my life outside of how the rest of my life at <UNK> , until everything suddenly was changing .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 409m 26s (- -409m 22s), Epoch: [12/20], Step: [500/2015], Train Loss: 1.2254979568719864\n",
            "Time: 411m 13s (- -411m 27s), Epoch: [12/20], Step: [600/2015], Train Loss: 1.2341789400577545\n",
            "Time: 413m 1s (- -413m 33s), Epoch: [12/20], Step: [700/2015], Train Loss: 1.2458624839782715\n",
            "Time: 414m 47s (- -415m 43s), Epoch: [12/20], Step: [800/2015], Train Loss: 1.2484596741199494\n",
            "Time: 416m 36s (- -417m 51s), Epoch: [12/20], Step: [900/2015], Train Loss: 1.2596959030628205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.31708449573569 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was constantly coming from what was going to life from North Korea s life , and it was very difficult , but I soon realized it was difficult , because the refugee displaced visited , who were <UNK> from North Koreans , was considered a <UNK>', 'And so there was a <UNK> when she got these flows , and they were all five of her five children who had no longer on the earth , because both were nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the work of the puppet Kim War II , but not to learn a lot about the outside world , except the United States , and Japan is our enemies .', 'I can t speak specifically about being hiding from North Korea , how to do that in <UNK> because of just being <UNK> from hunger to live a relative person .', 'Although I had never wondered the rest of the world outside , but I still think that I d lived even at <UNK> lives in <UNK> <UNK> , until everything suddenly was changing .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 418m 33s (- -419m 51s), Epoch: [12/20], Step: [1000/2015], Train Loss: 1.2909837424755097\n",
            "Time: 420m 19s (- -420m 3s), Epoch: [12/20], Step: [1100/2015], Train Loss: 1.2832951354980469\n",
            "Time: 422m 6s (- -422m 14s), Epoch: [12/20], Step: [1200/2015], Train Loss: 1.282934432029724\n",
            "Time: 423m 53s (- -424m 26s), Epoch: [12/20], Step: [1300/2015], Train Loss: 1.2932122611999513\n",
            "Time: 425m 40s (- -426m 37s), Epoch: [12/20], Step: [1400/2015], Train Loss: 1.3094306945800782\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.276212316829994 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> all the time to be from the North , and the <UNK> of a refugee from North Korea is , but I soon realized it was difficult , because the refugee displaced spotted , the <UNK> spotted , was considered the illegal migrant <UNK>', 'And so there was a revelation when she was reading these lines , including five of his friends , because both of them had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the semester of the Cold War II , but not learn so much about the world outside La , except in South Korea , in South Korea , in Korea and Japan .', 'I can t speak specifically about my own <UNK> , how I escaped the North Korea saying , in <UNK> , because I was sent to Chinese to live with a <UNK> and live up to the <UNK> .', 'Although I had wonder how the world outside , but I still thought I d lived all my life at <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 427m 37s (- -428m 39s), Epoch: [12/20], Step: [1500/2015], Train Loss: 1.3061111044883729\n",
            "Time: 429m 23s (- -430m 52s), Epoch: [12/20], Step: [1600/2015], Train Loss: 1.3051951551437377\n",
            "Time: 431m 10s (- -431m 4s), Epoch: [12/20], Step: [1700/2015], Train Loss: 1.3101040530204773\n",
            "Time: 432m 57s (- -433m 17s), Epoch: [12/20], Step: [1800/2015], Train Loss: 1.3240605247020723\n",
            "Time: 434m 43s (- -435m 30s), Epoch: [12/20], Step: [1900/2015], Train Loss: 1.3275971806049347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.33572077292906 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I d been so passionate about the life of a refugee refugee , and it would soon realize that it was difficult , but I soon realized it was difficult , because refugee refugee North Koreans was so hard , because from North Korea China was considered', 'It was when I wrote this line , as my family was five , who was not at this point , because even if you hadn t had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the beautiful scale Star Trek II , but not learn so much about the world outside of the outside War , in Tanzania , in Korea , in Tanzania , in our enemies , to', 'I can t speak specifically about being hiding out of North Korea , just to say in <UNK> years , because I m sent to China , so how to live with a relative person .', 'Though I was once asked to wonder the difference , but I still think I d live even in <UNK> <UNK> , until everything suddenly was going to live lives , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 436m 39s (- -437m 33s), Epoch: [12/20], Step: [2000/2015], Train Loss: 1.3365359044075011\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906]\n",
            "Time: 438m 41s (- -435m 41s), Epoch: [13/20], Step: [100/2015], Train Loss: 1.1317982417345047\n",
            "Time: 440m 28s (- -439m 43s), Epoch: [13/20], Step: [200/2015], Train Loss: 1.1283133465051651\n",
            "Time: 442m 15s (- -441m 13s), Epoch: [13/20], Step: [300/2015], Train Loss: 1.144743385910988\n",
            "Time: 444m 1s (- -443m 5s), Epoch: [13/20], Step: [400/2015], Train Loss: 1.1574341213703156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.27641499216065 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was <UNK> from what was going to be the life of a refugee person s <UNK> , which was , but I soon realized that it was difficult , because the refugee refugee North Museum of Chinese <UNK> were considered to be migrant migrant migrant migrant', 'And so there was a <UNK> when she got these flows of all these <UNK> , all the five five children who were not there anymore , because he had nothing to eat in two weeks .', 'In school , we spend a lot of time learning about the life of the <UNK> World War II , but not so much about the outside world , except the United States , and Japan was our enemies .', 'I can t speak specifically about my own <UNK> , how to hide with North Korea , how is it only <UNK> <UNK> , because I was sent to Chinese to live with a long way to live and live a long way to live a relative', 'Although I had no idea how the world outside , but I was thinking that I was going to live my life in <UNK> <UNK> , until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 445m 56s (- -446m 57s), Epoch: [13/20], Step: [500/2015], Train Loss: 1.166254215836525\n",
            "Time: 447m 42s (- -447m 1s), Epoch: [13/20], Step: [600/2015], Train Loss: 1.1676698875427247\n",
            "Time: 449m 28s (- -449m 9s), Epoch: [13/20], Step: [700/2015], Train Loss: 1.1856655335426332\n",
            "Time: 451m 13s (- -451m 20s), Epoch: [13/20], Step: [800/2015], Train Loss: 1.1907345521450043\n",
            "Time: 452m 59s (- -453m 31s), Epoch: [13/20], Step: [900/2015], Train Loss: 1.1952047669887542\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.216380737321632 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of going on with the life of a refugee from North Korea s <UNK> , and I soon realized it was incredibly difficult , because the refugee displaced person coming from North Korea China was considered a very difficult input of the market .', 'And so there was a <UNK> when she got these lines , all the five five who had no longer on earth , because even my house had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the work of the Cold War II , but not to learn so much about the outside world , except the United States , and Japan is our enemies .', 'I can t speak specifically about I ve gone out of North Korea as only can be said in <UNK> years because of the hunger I m sending to Chinese to catch a relative person .', 'And I was wondering that I knew how the outer world outside , but I was thinking , I still think I d lived all my life at the <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 454m 56s (- -455m 30s), Epoch: [13/20], Step: [1000/2015], Train Loss: 1.1920952165126801\n",
            "Time: 456m 39s (- -457m 45s), Epoch: [13/20], Step: [1100/2015], Train Loss: 1.2134984648227691\n",
            "Time: 458m 25s (- -459m 56s), Epoch: [13/20], Step: [1200/2015], Train Loss: 1.2039953565597534\n",
            "Time: 460m 11s (- -460m 9s), Epoch: [13/20], Step: [1300/2015], Train Loss: 1.2243910360336303\n",
            "Time: 461m 58s (- -462m 21s), Epoch: [13/20], Step: [1400/2015], Train Loss: 1.2277117669582367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.354874479547842 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of taken to what happened to life of a refugee from North Korea s , but I soon realized that it was difficult , not very difficult because the refugee refugee refugee North Korea in China is considered <UNK> .', 'And so there was a <UNK> when she got these lines , all my children , my children no longer just for two weeks later , because even my house had nothing to eat in two weeks .', 'At school , we spent a lot of time to learn about the act of the semester , the World War II , but not many of the world outside of the outside , except for Korea , and Japan was our enemies .', 'I cannot speak specifically about having been hiding out of North Korea , just to point out how I m sent to live in <UNK> because of the hunger I m sent to live with a poor person .', 'Though I was wondering , I knew the person outside the world outside , but I still think that I d lived even at <UNK> <UNK> , which was until everything suddenly .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 463m 53s (- -464m 25s), Epoch: [13/20], Step: [1500/2015], Train Loss: 1.2444627785682678\n",
            "Time: 465m 41s (- -466m 36s), Epoch: [13/20], Step: [1600/2015], Train Loss: 1.2544583976268768\n",
            "Time: 467m 27s (- -468m 48s), Epoch: [13/20], Step: [1700/2015], Train Loss: 1.2384538125991822\n",
            "Time: 469m 14s (- -469m 0s), Epoch: [13/20], Step: [1800/2015], Train Loss: 1.2514498543739319\n",
            "Time: 471m 1s (- -471m 13s), Epoch: [13/20], Step: [1900/2015], Train Loss: 1.2653695464134216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.17697708322979 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> all the time to happen to a refugee from North Korea , and it would take me to realize that it was difficult , but I soon realized it was difficult , because refugees were very difficult , because the refugee North Koreans went', 'And there was a time when she was when she got these lines , all the five five grandchildren who didn t have any more to eat in the end , because the house didn t have any two weeks for dinner .', 'In school , we spend a lot of time to learn about the work of the Middle War II , but not to learn a lot about the outside world , except the United States , and Japan is our enemies .', 'I can t speak specifically about my own feet away from North Korea in the eyes of only saying , in <UNK> because of the famine that hunger I m sent to China to live with a <UNK> person .', 'And although I have ever wondered the outside world outside , but I still think I d lived all my life at <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 472m 59s (- -473m 14s), Epoch: [13/20], Step: [2000/2015], Train Loss: 1.266369367837906\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011, 1.1317982417345047, 1.1283133465051651, 1.144743385910988, 1.1574341213703156, 1.166254215836525, 1.1676698875427247, 1.1856655335426332, 1.1907345521450043, 1.1952047669887542, 1.1920952165126801, 1.2134984648227691, 1.2039953565597534, 1.2243910360336303, 1.2277117669582367, 1.2444627785682678, 1.2544583976268768, 1.2384538125991822, 1.2514498543739319, 1.2653695464134216, 1.266369367837906]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906, 20.27641499216065, 20.216380737321632, 20.354874479547842, 20.17697708322979]\n",
            "Time: 475m 0s (- -471m 44s), Epoch: [14/20], Step: [100/2015], Train Loss: 1.0771349084377289\n",
            "Time: 476m 48s (- -475m 34s), Epoch: [14/20], Step: [200/2015], Train Loss: 1.0768746554851532\n",
            "Time: 478m 34s (- -477m 1s), Epoch: [14/20], Step: [300/2015], Train Loss: 1.0786463844776153\n",
            "Time: 480m 22s (- -480m 49s), Epoch: [14/20], Step: [400/2015], Train Loss: 1.0847132247686386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.758027813975364 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of <UNK> to what was going to life from North Korea s life s life , and it would soon realize that it was difficult , but I soon realized it was difficult , because Muslims from North Korea <UNK> were considered <UNK> of', 'And there was a <UNK> when she got these lines , all my five and family five , because both of them had nothing to eat in two weeks .', 'At school , we spend a lot of time to learn about the world s life playing Kim , but not to many of the world outside of the outside War , and Japan is our enemy .', 'I can t speak specifically about my own , and then I have to speak in the North , just in <UNK> , because I m sent to the Chinese , to live with a <UNK> person .', 'Although I had never been wondering , I was not aware of the world as I , but I still think that I would live even at <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 482m 18s (- -482m 39s), Epoch: [14/20], Step: [500/2015], Train Loss: 1.0968105632066727\n",
            "Time: 484m 6s (- -484m 42s), Epoch: [14/20], Step: [600/2015], Train Loss: 1.1058124971389771\n",
            "Time: 485m 53s (- -486m 47s), Epoch: [14/20], Step: [700/2015], Train Loss: 1.1172779703140259\n",
            "Time: 487m 41s (- -488m 55s), Epoch: [14/20], Step: [800/2015], Train Loss: 1.128718490600586\n",
            "Time: 489m 25s (- -489m 6s), Epoch: [14/20], Step: [900/2015], Train Loss: 1.1289762383699418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.547064444942563 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was constantly going to happen to the life of a refugee from North Korea s <UNK> , how would I soon realized it was difficult , but I soon realized it was difficult , because the refugee displaced asylum from North Korea <UNK> were considered <UNK>', 'And there was a <UNK> when she got these lines , and my family were not out in the air anymore , because both of them had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the World War II , but not in many of the world outside La <UNK> , except in South Korea , in Korea , is our enemies .', 'I can t speak specifically about my own <UNK> , how to hide my years just from North Korea hunger that famine that I was sent to China to live with a <UNK> <UNK> .', 'Although I had been wondering , I knew the world outside , but I still think I d lived all the lives in <UNK> <UNK> , until everything suddenly was changing .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 491m 23s (- -491m 6s), Epoch: [14/20], Step: [1000/2015], Train Loss: 1.1466530978679657\n",
            "Time: 493m 9s (- -493m 17s), Epoch: [14/20], Step: [1100/2015], Train Loss: 1.1516231000423431\n",
            "Time: 494m 55s (- -495m 29s), Epoch: [14/20], Step: [1200/2015], Train Loss: 1.1583639776706696\n",
            "Time: 496m 42s (- -497m 40s), Epoch: [14/20], Step: [1300/2015], Train Loss: 1.162167848944664\n",
            "Time: 498m 28s (- -499m 52s), Epoch: [14/20], Step: [1400/2015], Train Loss: 1.1651501476764679\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.00396122200077 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of going on to the life of a refugee refugee from North Korea s <UNK> , what would I say soon to him , but I soon realized it was difficult , because the refugee displaced asylum was considered a <UNK> of migrant immigrants', 'And so there was a meeting when she was reading these lines , all my children who were not at this earth , because my home was nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the real world work of Kim II Kim , but not to learn so much about the world outside La <UNK> , except for South America and Japan of our enemies .', 'I can t tell you how I ve got to go out of North Korea , and just how I ve got to say in <UNK> because of that , and I m sent to the Chinese , to live with a long way to live away', 'Even though I have no idea of what the world outside , but I still think I m going to live in life at <UNK> <UNK> , until everything suddenly has changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 500m 24s (- -501m 55s), Epoch: [14/20], Step: [1500/2015], Train Loss: 1.1662205207347869\n",
            "Time: 502m 11s (- -502m 7s), Epoch: [14/20], Step: [1600/2015], Train Loss: 1.1798181706666946\n",
            "Time: 503m 59s (- -504m 18s), Epoch: [14/20], Step: [1700/2015], Train Loss: 1.177376961708069\n",
            "Time: 505m 45s (- -506m 31s), Epoch: [14/20], Step: [1800/2015], Train Loss: 1.2049704551696778\n",
            "Time: 507m 30s (- -508m 45s), Epoch: [14/20], Step: [1900/2015], Train Loss: 1.1935699737071992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.889402490340114 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of going off to what happened to a refugee from the <UNK> North Korea , but I realized that it was difficult to be very difficult , because the refugee displaced people from North Korea <UNK> , because from North Korea included Koreans in', 'And so there was when she was 15 minutes away , and my five percent of the kid who was not there anymore , because he had nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the work of the puppet Kim II , but not without learning much about the outside world , except the United States , and Japan is our enemies .', 'I cannot speak specifically about my job as being possible away from North Korea . What is only possible in these <UNK> because I m sent to China to live with a <UNK> from a distance of their <UNK> .', 'Although I had never wondered the world outside , I still think I d lived all my life at the <UNK> <UNK> , until everything suddenly strikes the lives of <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 509m 28s (- -510m 46s), Epoch: [14/20], Step: [2000/2015], Train Loss: 1.1994604063034058\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011, 1.1317982417345047, 1.1283133465051651, 1.144743385910988, 1.1574341213703156, 1.166254215836525, 1.1676698875427247, 1.1856655335426332, 1.1907345521450043, 1.1952047669887542, 1.1920952165126801, 1.2134984648227691, 1.2039953565597534, 1.2243910360336303, 1.2277117669582367, 1.2444627785682678, 1.2544583976268768, 1.2384538125991822, 1.2514498543739319, 1.2653695464134216, 1.266369367837906, 1.0771349084377289, 1.0768746554851532, 1.0786463844776153, 1.0847132247686386, 1.0968105632066727, 1.1058124971389771, 1.1172779703140259, 1.128718490600586, 1.1289762383699418, 1.1466530978679657, 1.1516231000423431, 1.1583639776706696, 1.162167848944664, 1.1651501476764679, 1.1662205207347869, 1.1798181706666946, 1.177376961708069, 1.2049704551696778, 1.1935699737071992, 1.1994604063034058]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906, 20.27641499216065, 20.216380737321632, 20.354874479547842, 20.17697708322979, 19.758027813975364, 20.547064444942563, 20.00396122200077, 19.889402490340114]\n",
            "Time: 511m 30s (- -507m 36s), Epoch: [15/20], Step: [100/2015], Train Loss: 1.0256272649765015\n",
            "Time: 513m 17s (- -511m 16s), Epoch: [15/20], Step: [200/2015], Train Loss: 1.016500694155693\n",
            "Time: 515m 3s (- -514m 39s), Epoch: [15/20], Step: [300/2015], Train Loss: 1.0240982788801194\n",
            "Time: 516m 49s (- -516m 28s), Epoch: [15/20], Step: [400/2015], Train Loss: 1.0425837457180023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.826998127852992 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had this kind of place that happened to a refugee from North Korea s <UNK> , but I soon realized that it was difficult , not only were difficult , because the refugee refugee North Koreans , was considered a <UNK> , because the refugee North', 'And there was a little girl when she got these flows , all my family five , who didn t have any five year old kid in the air .', 'In school , we spent a lot of time to learn about the act of the <UNK> World War II , but not in many of the world outside of the outside , except for South America and Japan of our enemies .', 'I can t tell you how I ve got away from North Korea . What can I say in these <UNK> because of my hunger , I m sent to China to live with a long way to live a relative person .', 'And although I had no wonder , I knew that I had no doubt about the outside world , but I still think I would live even at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 518m 43s (- -518m 19s), Epoch: [15/20], Step: [500/2015], Train Loss: 1.0330818921327591\n",
            "Time: 520m 30s (- -520m 21s), Epoch: [15/20], Step: [600/2015], Train Loss: 1.0517603093385697\n",
            "Time: 522m 15s (- -522m 29s), Epoch: [15/20], Step: [700/2015], Train Loss: 1.0554947292804717\n",
            "Time: 523m 59s (- -524m 39s), Epoch: [15/20], Step: [800/2015], Train Loss: 1.07287029504776\n",
            "Time: 525m 46s (- -526m 48s), Epoch: [15/20], Step: [900/2015], Train Loss: 1.0862455934286117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.80718103706736 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so <UNK> to what was happening to a refugee from North Korea s <UNK> , and I soon realized that it was difficult to be difficult , because refugee people from North Korea <UNK> and <UNK> are as a result of illegal immigrants , being', 'And there was a key blog that she was going to read these lines , all the five five , because the house didn t have any left to eat in two weeks .', 'In school , we spent a lot of time learning about the real life scale Kim War II , but not in many of our world , except American , South Korea and Japan was our enemies .', 'I cannot speak specifically about my job as being possible from North Korea , how is it only possible that in <UNK> years from my face to live with a <UNK> person .', 'Though I was telling myself , I knew how the outer world was , but I still thought I d lived all my life at the <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 527m 45s (- -528m 46s), Epoch: [15/20], Step: [1000/2015], Train Loss: 1.0919886207580567\n",
            "Time: 529m 30s (- -530m 58s), Epoch: [15/20], Step: [1100/2015], Train Loss: 1.0880117952823638\n",
            "Time: 531m 16s (- -531m 9s), Epoch: [15/20], Step: [1200/2015], Train Loss: 1.0976157772541046\n",
            "Time: 533m 4s (- -533m 20s), Epoch: [15/20], Step: [1300/2015], Train Loss: 1.1085583406686783\n",
            "Time: 534m 51s (- -535m 31s), Epoch: [15/20], Step: [1400/2015], Train Loss: 1.110286311507225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.623789182629668 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was so <UNK> what happened to a man from the North Korea . And obviously I realized it was extremely difficult , because refugees was very difficult , because refugees came from North Korea China was declared a migrant migrant .', 'And so there was a little girl when she got these lines , including my five family , my children no longer just serving the earth in two weeks .', 'In school , we spend a lot of time learning about the life of the Cold War II , but not in many of the past outside of the outside War II , South America and Japan are our enemies .', 'I can t speak specifically about my job as to say in <UNK> , which is , how did I get out of the <UNK> , because of the famine , I was sent to Chinese to live with a long way from living with a poor', 'Though I was telling myself that I was never going to know the world outside , but I still think I d lived even in my life at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 536m 46s (- -537m 34s), Epoch: [15/20], Step: [1500/2015], Train Loss: 1.1180457311868668\n",
            "Time: 538m 34s (- -539m 45s), Epoch: [15/20], Step: [1600/2015], Train Loss: 1.1102939385175705\n",
            "Time: 540m 21s (- -541m 57s), Epoch: [15/20], Step: [1700/2015], Train Loss: 1.1238849884271622\n",
            "Time: 542m 8s (- -542m 9s), Epoch: [15/20], Step: [1800/2015], Train Loss: 1.1205213755369186\n",
            "Time: 543m 53s (- -544m 23s), Epoch: [15/20], Step: [1900/2015], Train Loss: 1.1373492884635925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 20.054036834532578 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was constantly going away from the <UNK> of the future , and the soon , but I soon realized that it was difficult , not only were quite difficult , because from North Korea North Koreans came from North Korea <UNK> , who were seen as', 'And so there was a time when she was able to read these lines , including my family , who s five of them who had nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the puppet of the <UNK> War II , but not to learn so much about the world outside War II , in South Korea , in South Korea , in Tanzania , in Japan ,', 'I can t tell what I ve experienced from North Korea down to just how you might say that in <UNK> , because of that hunger I m sent to live a Chinese <UNK> .', 'Although I had never been , I knew that I had no idea the world outside , but I still think that I would live all my life at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 545m 51s (- -546m 25s), Epoch: [15/20], Step: [2000/2015], Train Loss: 1.1493624395132065\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011, 1.1317982417345047, 1.1283133465051651, 1.144743385910988, 1.1574341213703156, 1.166254215836525, 1.1676698875427247, 1.1856655335426332, 1.1907345521450043, 1.1952047669887542, 1.1920952165126801, 1.2134984648227691, 1.2039953565597534, 1.2243910360336303, 1.2277117669582367, 1.2444627785682678, 1.2544583976268768, 1.2384538125991822, 1.2514498543739319, 1.2653695464134216, 1.266369367837906, 1.0771349084377289, 1.0768746554851532, 1.0786463844776153, 1.0847132247686386, 1.0968105632066727, 1.1058124971389771, 1.1172779703140259, 1.128718490600586, 1.1289762383699418, 1.1466530978679657, 1.1516231000423431, 1.1583639776706696, 1.162167848944664, 1.1651501476764679, 1.1662205207347869, 1.1798181706666946, 1.177376961708069, 1.2049704551696778, 1.1935699737071992, 1.1994604063034058, 1.0256272649765015, 1.016500694155693, 1.0240982788801194, 1.0425837457180023, 1.0330818921327591, 1.0517603093385697, 1.0554947292804717, 1.07287029504776, 1.0862455934286117, 1.0919886207580567, 1.0880117952823638, 1.0976157772541046, 1.1085583406686783, 1.110286311507225, 1.1180457311868668, 1.1102939385175705, 1.1238849884271622, 1.1205213755369186, 1.1373492884635925, 1.1493624395132065]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906, 20.27641499216065, 20.216380737321632, 20.354874479547842, 20.17697708322979, 19.758027813975364, 20.547064444942563, 20.00396122200077, 19.889402490340114, 19.826998127852992, 19.80718103706736, 19.623789182629668, 20.054036834532578]\n",
            "Time: 547m 54s (- -543m 34s), Epoch: [16/20], Step: [100/2015], Train Loss: 0.9637722766399384\n",
            "Time: 549m 39s (- -547m 5s), Epoch: [16/20], Step: [200/2015], Train Loss: 0.9707902467250824\n",
            "Time: 551m 25s (- -550m 24s), Epoch: [16/20], Step: [300/2015], Train Loss: 0.985478184223175\n",
            "Time: 553m 13s (- -552m 9s), Epoch: [16/20], Step: [400/2015], Train Loss: 0.9792842918634415\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.89549220765484 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of going off to me of a refugee s life from North Korea s <UNK> , and it s very difficult , but I soon realized it was difficult , because the refugee North Koreans went into China s <UNK> of <UNK> , were', 'It was when she wrote this line of <UNK> and all of this , physically <UNK> my five , who didn t have any more .', 'In school , we spend a lot of time to learn about the act of the puppet World War II , but not in many of the world outside the country , in South Korea , in South Korea , in Korea , Japan is our enemies', 'I can t speak specifically about having been hiding out of North Korea . What can I say in these <UNK> because of my hunger and famine , I was sent to Chinese , to live with a <UNK> of cash .', 'And although I had been wondering , I knew that I had no idea the world outside , but I still think I d lived my whole life at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 555m 11s (- -555m 55s), Epoch: [16/20], Step: [500/2015], Train Loss: 0.9981485533714295\n",
            "Time: 556m 58s (- -557m 57s), Epoch: [16/20], Step: [600/2015], Train Loss: 1.0104245352745056\n",
            "Time: 558m 44s (- -558m 3s), Epoch: [16/20], Step: [700/2015], Train Loss: 1.0134312742948532\n",
            "Time: 560m 29s (- -560m 12s), Epoch: [16/20], Step: [800/2015], Train Loss: 1.0052074098587036\n",
            "Time: 562m 15s (- -562m 22s), Epoch: [16/20], Step: [900/2015], Train Loss: 1.030664069056511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.583525370316547 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of out of a refugee from the <UNK> North Korea , and I realized that it was difficult to be very difficult , but I soon realized it was difficult , because the refugee North Koreans went into China s equivalent of North Korea', 'And so there was a key kid when she was reading these lines , all my family were not in the air anymore , because both of you had nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the narrative of the Cold War II , but not in many of the world outside La <UNK> , except in South Korea , in Korea , and Japan was our enemies .', 'I can t speak specifically about having a <UNK> of how I escaped from North Korea hunger that famine that I was sent to China to live with a <UNK> person .', 'And although I have asked , I once asked if the outer world , but I think I would live even at <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 564m 11s (- -564m 22s), Epoch: [16/20], Step: [1000/2015], Train Loss: 1.0216612654924393\n",
            "Time: 565m 58s (- -566m 32s), Epoch: [16/20], Step: [1100/2015], Train Loss: 1.0307461977005006\n",
            "Time: 567m 46s (- -568m 42s), Epoch: [16/20], Step: [1200/2015], Train Loss: 1.043236049413681\n",
            "Time: 569m 32s (- -570m 53s), Epoch: [16/20], Step: [1300/2015], Train Loss: 1.0465580159425736\n",
            "Time: 571m 20s (- -571m 4s), Epoch: [16/20], Step: [1400/2015], Train Loss: 1.054484602212906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.90490636793742 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of swept to what happened to a man from North Korea s <UNK> in the <UNK> , but I realized it was difficult not that she was difficult , because the refugee displaced North Koreans went into China s migrant was declared of the', 'It was when she wrote this line , as my family had no longer , five percent of her children s father was not going to eat in two weeks , because both girls were not had any given two weeks for dinner .', 'In school , we spent a lot of time teaching mathematics how the world works II , but not in many of the world outside of the outside of the outside , except in South Korea , in Tanzania , Japan .', 'I can t speak specifically about my job as being possible away from North Korea . What is only possible in these years because of hunger that hunger I m sent to live Chinese with a long term distance relative .', 'Although I had never wondered the world outside , but I still think I d live even in the lives in <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 573m 17s (- -573m 5s), Epoch: [16/20], Step: [1500/2015], Train Loss: 1.065860338807106\n",
            "Time: 575m 3s (- -575m 18s), Epoch: [16/20], Step: [1600/2015], Train Loss: 1.052193101644516\n",
            "Time: 576m 48s (- -577m 31s), Epoch: [16/20], Step: [1700/2015], Train Loss: 1.0624741929769517\n",
            "Time: 578m 33s (- -579m 46s), Epoch: [16/20], Step: [1800/2015], Train Loss: 1.0811631059646607\n",
            "Time: 580m 21s (- -581m 56s), Epoch: [16/20], Step: [1900/2015], Train Loss: 1.0921888011693954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.81899936107871 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> what was happening to a refugee from North Korea s life , and it would make me realize it was not difficult , because the refugees came from North Korea <UNK> , because the refugees came from North Korea <UNK> , who was equally', 'And so there was a <UNK> when she got these lines , and my family had no longer five man else s lives on the earth I had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the work of the Cold War II , but not to learn so much about the world outside War , which is the United States , South Korea and Japan and our conclusion .', 'I cannot speak specifically about my job as being possible away from North Korea . What is only possible in these famine since I was being sent to catch Chinese to live with a long old friend .', 'Although I had never wondered the difference , but I still think that I would live entire life at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 582m 17s (- -583m 59s), Epoch: [16/20], Step: [2000/2015], Train Loss: 1.0966322916746138\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011, 1.1317982417345047, 1.1283133465051651, 1.144743385910988, 1.1574341213703156, 1.166254215836525, 1.1676698875427247, 1.1856655335426332, 1.1907345521450043, 1.1952047669887542, 1.1920952165126801, 1.2134984648227691, 1.2039953565597534, 1.2243910360336303, 1.2277117669582367, 1.2444627785682678, 1.2544583976268768, 1.2384538125991822, 1.2514498543739319, 1.2653695464134216, 1.266369367837906, 1.0771349084377289, 1.0768746554851532, 1.0786463844776153, 1.0847132247686386, 1.0968105632066727, 1.1058124971389771, 1.1172779703140259, 1.128718490600586, 1.1289762383699418, 1.1466530978679657, 1.1516231000423431, 1.1583639776706696, 1.162167848944664, 1.1651501476764679, 1.1662205207347869, 1.1798181706666946, 1.177376961708069, 1.2049704551696778, 1.1935699737071992, 1.1994604063034058, 1.0256272649765015, 1.016500694155693, 1.0240982788801194, 1.0425837457180023, 1.0330818921327591, 1.0517603093385697, 1.0554947292804717, 1.07287029504776, 1.0862455934286117, 1.0919886207580567, 1.0880117952823638, 1.0976157772541046, 1.1085583406686783, 1.110286311507225, 1.1180457311868668, 1.1102939385175705, 1.1238849884271622, 1.1205213755369186, 1.1373492884635925, 1.1493624395132065, 0.9637722766399384, 0.9707902467250824, 0.985478184223175, 0.9792842918634415, 0.9981485533714295, 1.0104245352745056, 1.0134312742948532, 1.0052074098587036, 1.030664069056511, 1.0216612654924393, 1.0307461977005006, 1.043236049413681, 1.0465580159425736, 1.054484602212906, 1.065860338807106, 1.052193101644516, 1.0624741929769517, 1.0811631059646607, 1.0921888011693954, 1.0966322916746138]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906, 20.27641499216065, 20.216380737321632, 20.354874479547842, 20.17697708322979, 19.758027813975364, 20.547064444942563, 20.00396122200077, 19.889402490340114, 19.826998127852992, 19.80718103706736, 19.623789182629668, 20.054036834532578, 19.89549220765484, 19.583525370316547, 19.90490636793742, 19.81899936107871]\n",
            "Time: 584m 20s (- -579m 30s), Epoch: [17/20], Step: [100/2015], Train Loss: 0.9203345298767089\n",
            "Time: 586m 6s (- -584m 49s), Epoch: [17/20], Step: [200/2015], Train Loss: 0.9083132916688919\n",
            "Time: 587m 50s (- -586m 6s), Epoch: [17/20], Step: [300/2015], Train Loss: 0.9210938769578934\n",
            "Time: 589m 35s (- -589m 52s), Epoch: [17/20], Step: [400/2015], Train Loss: 0.9283190816640854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.46388975441724 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was <UNK> about a refugee from North Korea , and it would be , but I realized it was difficult to be so difficult , because the refugee people from North Korea <UNK> and <UNK> , because the refugee North Koreans went to the nation of', 'And so there was a <UNK> when she got these lines , all my family , who was not in this room anymore , because both my friends had nothing to eat in two weeks .', 'At school , we spend a lot of time to learn about the <UNK> , the chairman of the <UNK> War , but not to learn so much about the world outside War , to the United States , South Korea , and Japan is our enemies', 'I cannot speak specifically about my job as being possible from North Korea . What is only possible in these <UNK> since I m being sent to live Chinese to live with a <UNK> person .', 'Although I had never asked the question of the world outside , but I still thought I d live even in the lives of <UNK> , until it suddenly suddenly suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 591m 33s (- -591m 37s), Epoch: [17/20], Step: [500/2015], Train Loss: 0.9450959467887878\n",
            "Time: 593m 20s (- -593m 38s), Epoch: [17/20], Step: [600/2015], Train Loss: 0.9438889718055725\n",
            "Time: 595m 7s (- -595m 43s), Epoch: [17/20], Step: [700/2015], Train Loss: 0.9621766060590744\n",
            "Time: 596m 55s (- -597m 49s), Epoch: [17/20], Step: [800/2015], Train Loss: 0.9732212197780609\n",
            "Time: 598m 42s (- -599m 57s), Epoch: [17/20], Step: [900/2015], Train Loss: 0.9726406127214432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.461308456188345 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of physically coming to a refugee from North Korea s life refugee <UNK> , and it was very soon I realized it was difficult , not only profoundly , because the refugee North Koreans went into China s migrant <UNK> is considered a <UNK>', 'And so there was a <UNK> when she got these lines , all my family , who was not there for my five , because he had nothing to eat in two weeks .', 'In school , we spent a lot of time to learn about the puppet of the Middle War II , but not in many of the world outside the country , the United States , South Korea , in Korea , for the Japan , in Korea', 'I can t speak specifically about having been hiding out of North Korea . What can I say in these <UNK> because of hunger I m sent to live Chinese and live with a <UNK> person .', 'Although I d been wondering , I knew the moment of life outside my life , but I still think that I d lived a life in <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 600m 37s (- -601m 59s), Epoch: [17/20], Step: [1000/2015], Train Loss: 0.978558498620987\n",
            "Time: 602m 26s (- -602m 6s), Epoch: [17/20], Step: [1100/2015], Train Loss: 0.9772282874584198\n",
            "Time: 604m 13s (- -604m 17s), Epoch: [17/20], Step: [1200/2015], Train Loss: 1.002185475230217\n",
            "Time: 605m 58s (- -606m 29s), Epoch: [17/20], Step: [1300/2015], Train Loss: 1.0019132256507874\n",
            "Time: 607m 46s (- -608m 39s), Epoch: [17/20], Step: [1400/2015], Train Loss: 1.0063286679983139\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.286918080594983 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> what happened to a refugee from North Korea s <UNK> , how would it was not that I realized it was difficult , but I realized it was difficult , because the refugee oil Chinese <UNK> , was considered a higher marriage .', 'And there was a <UNK> when she got these lines and I was in my five foot family s five children , because even my house had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the act of the Middle World War II , but not to learn many of the world outside of the outside , except for South Korea , in Korea , for the Japanese and Japan', 'I can t tell about my being pulled out of North Korea , how do you get out of <UNK> , because of that hunger I m sent to life with a long term away to live a long way to live .', 'Although I used to wonder , I knew that I knew I was living in my life in <UNK> <UNK> , until everything suddenly was changing lives , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 609m 42s (- -610m 41s), Epoch: [17/20], Step: [1500/2015], Train Loss: 1.0201069366931916\n",
            "Time: 611m 29s (- -612m 53s), Epoch: [17/20], Step: [1600/2015], Train Loss: 1.023816528916359\n",
            "Time: 613m 15s (- -613m 6s), Epoch: [17/20], Step: [1700/2015], Train Loss: 1.033327819108963\n",
            "Time: 615m 1s (- -615m 19s), Epoch: [17/20], Step: [1800/2015], Train Loss: 1.036825453042984\n",
            "Time: 616m 48s (- -617m 31s), Epoch: [17/20], Step: [1900/2015], Train Loss: 1.0390199398994446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.91074921071965 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> what happened to be a refugee <UNK> from North Korea s degree , but I realized it was difficult not that she was difficult , because the refugee displaced visited of China had been considered for <UNK> in the Chinese , were considered his', 'And so there was a <UNK> when she got these lines , all the five five , five children who had no longer left this earth , because he had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the act of the puppet World War II , but not in many ways , except the United States , South Korea and Japan is our enemies .', 'I cannot speak specifically about my job as being possible away from North Korea . What can I say in <UNK> years because of my face as being sent to live with a <UNK> person .', 'Though I was telling you , I knew that I knew that I was living in the whole life in <UNK> , until everything suddenly was changing the lives of <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 618m 44s (- -619m 34s), Epoch: [17/20], Step: [2000/2015], Train Loss: 1.0325741153955459\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011, 1.1317982417345047, 1.1283133465051651, 1.144743385910988, 1.1574341213703156, 1.166254215836525, 1.1676698875427247, 1.1856655335426332, 1.1907345521450043, 1.1952047669887542, 1.1920952165126801, 1.2134984648227691, 1.2039953565597534, 1.2243910360336303, 1.2277117669582367, 1.2444627785682678, 1.2544583976268768, 1.2384538125991822, 1.2514498543739319, 1.2653695464134216, 1.266369367837906, 1.0771349084377289, 1.0768746554851532, 1.0786463844776153, 1.0847132247686386, 1.0968105632066727, 1.1058124971389771, 1.1172779703140259, 1.128718490600586, 1.1289762383699418, 1.1466530978679657, 1.1516231000423431, 1.1583639776706696, 1.162167848944664, 1.1651501476764679, 1.1662205207347869, 1.1798181706666946, 1.177376961708069, 1.2049704551696778, 1.1935699737071992, 1.1994604063034058, 1.0256272649765015, 1.016500694155693, 1.0240982788801194, 1.0425837457180023, 1.0330818921327591, 1.0517603093385697, 1.0554947292804717, 1.07287029504776, 1.0862455934286117, 1.0919886207580567, 1.0880117952823638, 1.0976157772541046, 1.1085583406686783, 1.110286311507225, 1.1180457311868668, 1.1102939385175705, 1.1238849884271622, 1.1205213755369186, 1.1373492884635925, 1.1493624395132065, 0.9637722766399384, 0.9707902467250824, 0.985478184223175, 0.9792842918634415, 0.9981485533714295, 1.0104245352745056, 1.0134312742948532, 1.0052074098587036, 1.030664069056511, 1.0216612654924393, 1.0307461977005006, 1.043236049413681, 1.0465580159425736, 1.054484602212906, 1.065860338807106, 1.052193101644516, 1.0624741929769517, 1.0811631059646607, 1.0921888011693954, 1.0966322916746138, 0.9203345298767089, 0.9083132916688919, 0.9210938769578934, 0.9283190816640854, 0.9450959467887878, 0.9438889718055725, 0.9621766060590744, 0.9732212197780609, 0.9726406127214432, 0.978558498620987, 0.9772282874584198, 1.002185475230217, 1.0019132256507874, 1.0063286679983139, 1.0201069366931916, 1.023816528916359, 1.033327819108963, 1.036825453042984, 1.0390199398994446, 1.0325741153955459]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906, 20.27641499216065, 20.216380737321632, 20.354874479547842, 20.17697708322979, 19.758027813975364, 20.547064444942563, 20.00396122200077, 19.889402490340114, 19.826998127852992, 19.80718103706736, 19.623789182629668, 20.054036834532578, 19.89549220765484, 19.583525370316547, 19.90490636793742, 19.81899936107871, 19.46388975441724, 19.461308456188345, 19.286918080594983, 19.91074921071965]\n",
            "Time: 620m 45s (- -615m 26s), Epoch: [18/20], Step: [100/2015], Train Loss: 0.8713813960552216\n",
            "Time: 622m 32s (- -620m 34s), Epoch: [18/20], Step: [200/2015], Train Loss: 0.8731583619117737\n",
            "Time: 624m 20s (- -623m 44s), Epoch: [18/20], Step: [300/2015], Train Loss: 0.8852085411548615\n",
            "Time: 626m 7s (- -625m 26s), Epoch: [18/20], Step: [400/2015], Train Loss: 0.8930940163135529\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.478388755972986 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of taken with the life of a refugee from North Korea s <UNK> , and I realized that it was difficult to be so difficult , because the refugee people from North Korea <UNK> China s <UNK> <UNK> , were considered a clear immigrant', 'And there was a little kid when she got these lines , all the five five who made no longer on earth , because he had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the work of the puppet World War II , but not in many of the world outside the outside War II , South Korea , in Korea , to the conclusion of our enemies .', 'I can t tell you how I ve got from North Korea down , and how can I say that in <UNK> when I m sent to China to live with a poor person .', 'Although I had never wondered the difference , but I still think I d spend a life of life at <UNK> <UNK> , until they re all about to be living in <UNK> , until everything suddenly has changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 628m 3s (- -627m 12s), Epoch: [18/20], Step: [500/2015], Train Loss: 0.9012843263149262\n",
            "Time: 629m 50s (- -629m 12s), Epoch: [18/20], Step: [600/2015], Train Loss: 0.900028076171875\n",
            "Time: 631m 37s (- -631m 16s), Epoch: [18/20], Step: [700/2015], Train Loss: 0.92364770591259\n",
            "Time: 633m 23s (- -633m 23s), Epoch: [18/20], Step: [800/2015], Train Loss: 0.9159180790185928\n",
            "Time: 635m 9s (- -635m 33s), Epoch: [18/20], Step: [900/2015], Train Loss: 0.925900535583496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.685447468788432 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> what happened to a man in prison from North Korea s <UNK> , what I realized it was difficult to do so difficult , because the refugees was so difficult , because the refugee oil founded hits the Chinese , were considered his ,', 'And there was 15 minutes she had her to read all those five five children who have no longer on earth , because they had nothing to eat in two weeks .', 'In school , we spend a lot of time to learn about the <UNK> narrative of Kim II , but not in many ways , except the United States of America , in South Korea , in Tanzania , Japan is our enemy .', 'I can t speak specifically about having personally escaped from North Korea under just to say that in <UNK> , so how to speak , from Chinese , to live with a long way to live a relative person .', 'Though I had , I knew , I knew , I knew that I was living in the entire life at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 637m 3s (- -637m 34s), Epoch: [18/20], Step: [1000/2015], Train Loss: 0.9332020634412765\n",
            "Time: 638m 48s (- -639m 46s), Epoch: [18/20], Step: [1100/2015], Train Loss: 0.9414683157205581\n",
            "Time: 640m 33s (- -641m 58s), Epoch: [18/20], Step: [1200/2015], Train Loss: 0.9502524375915528\n",
            "Time: 642m 20s (- -642m 9s), Epoch: [18/20], Step: [1300/2015], Train Loss: 0.9455750477313996\n",
            "Time: 644m 9s (- -644m 18s), Epoch: [18/20], Step: [1400/2015], Train Loss: 0.9784943777322769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.542868504897584 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of going out to what happened to a man of <UNK> <UNK> , but I soon realized it was difficult , because the refugee people were going to be , but I realized it was difficult , because the refugee North Koreans went to', 'And so there was a key kid when she got these <UNK> , my family had no longer five girls on the earth , because both you didn t have any of these to dinner in two weeks .', 'In school , we spent a lot of time teaching mathematics how to work on the <UNK> World War II , but not in many of the world outside the United States , South Korea and Japan was our enemy .', 'I cannot speak specifically about my job as being possible to <UNK> up and down to just say in awe of being <UNK> , because I was sent to Chinese , to live with a long term away .', 'Although I was asked to wonder , the world outside , but I still thought that I would live all my life at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 646m 6s (- -646m 19s), Epoch: [18/20], Step: [1500/2015], Train Loss: 0.9598257005214691\n",
            "Time: 647m 53s (- -648m 30s), Epoch: [18/20], Step: [1600/2015], Train Loss: 0.9760297030210495\n",
            "Time: 649m 39s (- -650m 43s), Epoch: [18/20], Step: [1700/2015], Train Loss: 0.9826265573501587\n",
            "Time: 651m 26s (- -652m 54s), Epoch: [18/20], Step: [1800/2015], Train Loss: 0.9917564535140991\n",
            "Time: 653m 11s (- -653m 9s), Epoch: [18/20], Step: [1900/2015], Train Loss: 0.9873139816522598\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.47578202276559 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was <UNK> what was happening to the life of a refugee man s life going forward , but I soon realized it was difficult , because the refugee people were going to be so difficult , because these refugees were considered , because from North Korea', 'And there was a <UNK> when she got these lines , all my family , who was not at this point , because he had nothing to eat in two weeks .', 'In school , we spent a lot of time teaching mathematics Kim II , but we didn t learn so much about the world outside II , except for the U.S. , North America , South Korea , and Japan was our enemies .', 'I cannot speak specifically about my job as being possible from North Korea down to just being one point in <UNK> because of that famine I m sent to live with China to live a long way to live .', 'Even though I had actually wondered what the world outside was like , but I still think I d lived all my life at <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 655m 8s (- -655m 10s), Epoch: [18/20], Step: [2000/2015], Train Loss: 0.9908147156238556\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011, 1.1317982417345047, 1.1283133465051651, 1.144743385910988, 1.1574341213703156, 1.166254215836525, 1.1676698875427247, 1.1856655335426332, 1.1907345521450043, 1.1952047669887542, 1.1920952165126801, 1.2134984648227691, 1.2039953565597534, 1.2243910360336303, 1.2277117669582367, 1.2444627785682678, 1.2544583976268768, 1.2384538125991822, 1.2514498543739319, 1.2653695464134216, 1.266369367837906, 1.0771349084377289, 1.0768746554851532, 1.0786463844776153, 1.0847132247686386, 1.0968105632066727, 1.1058124971389771, 1.1172779703140259, 1.128718490600586, 1.1289762383699418, 1.1466530978679657, 1.1516231000423431, 1.1583639776706696, 1.162167848944664, 1.1651501476764679, 1.1662205207347869, 1.1798181706666946, 1.177376961708069, 1.2049704551696778, 1.1935699737071992, 1.1994604063034058, 1.0256272649765015, 1.016500694155693, 1.0240982788801194, 1.0425837457180023, 1.0330818921327591, 1.0517603093385697, 1.0554947292804717, 1.07287029504776, 1.0862455934286117, 1.0919886207580567, 1.0880117952823638, 1.0976157772541046, 1.1085583406686783, 1.110286311507225, 1.1180457311868668, 1.1102939385175705, 1.1238849884271622, 1.1205213755369186, 1.1373492884635925, 1.1493624395132065, 0.9637722766399384, 0.9707902467250824, 0.985478184223175, 0.9792842918634415, 0.9981485533714295, 1.0104245352745056, 1.0134312742948532, 1.0052074098587036, 1.030664069056511, 1.0216612654924393, 1.0307461977005006, 1.043236049413681, 1.0465580159425736, 1.054484602212906, 1.065860338807106, 1.052193101644516, 1.0624741929769517, 1.0811631059646607, 1.0921888011693954, 1.0966322916746138, 0.9203345298767089, 0.9083132916688919, 0.9210938769578934, 0.9283190816640854, 0.9450959467887878, 0.9438889718055725, 0.9621766060590744, 0.9732212197780609, 0.9726406127214432, 0.978558498620987, 0.9772282874584198, 1.002185475230217, 1.0019132256507874, 1.0063286679983139, 1.0201069366931916, 1.023816528916359, 1.033327819108963, 1.036825453042984, 1.0390199398994446, 1.0325741153955459, 0.8713813960552216, 0.8731583619117737, 0.8852085411548615, 0.8930940163135529, 0.9012843263149262, 0.900028076171875, 0.92364770591259, 0.9159180790185928, 0.925900535583496, 0.9332020634412765, 0.9414683157205581, 0.9502524375915528, 0.9455750477313996, 0.9784943777322769, 0.9598257005214691, 0.9760297030210495, 0.9826265573501587, 0.9917564535140991, 0.9873139816522598, 0.9908147156238556]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906, 20.27641499216065, 20.216380737321632, 20.354874479547842, 20.17697708322979, 19.758027813975364, 20.547064444942563, 20.00396122200077, 19.889402490340114, 19.826998127852992, 19.80718103706736, 19.623789182629668, 20.054036834532578, 19.89549220765484, 19.583525370316547, 19.90490636793742, 19.81899936107871, 19.46388975441724, 19.461308456188345, 19.286918080594983, 19.91074921071965, 19.478388755972986, 19.685447468788432, 19.542868504897584, 19.47578202276559]\n",
            "Time: 657m 11s (- -651m 23s), Epoch: [19/20], Step: [100/2015], Train Loss: 0.8273465424776077\n",
            "Time: 658m 57s (- -656m 20s), Epoch: [19/20], Step: [200/2015], Train Loss: 0.8207612466812134\n",
            "Time: 660m 42s (- -659m 29s), Epoch: [19/20], Step: [300/2015], Train Loss: 0.8382474106550216\n",
            "Time: 662m 29s (- -661m 9s), Epoch: [19/20], Step: [400/2015], Train Loss: 0.854357956647873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.236260797465214 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> to be coming to a refugee from the <UNK> of <UNK> , but I realized it was difficult to do so difficult , because these individuals from North Korea <UNK> <UNK> , who was <UNK> China in China s <UNK> .', 'And there was a <UNK> when she read all these lines , but even five of my children were not in the air , because he had nothing to eat in two weeks .', 'In school , we spend a lot of time learning about the real life of Kim World War II , but not in many of the world outside the outside War , in Tanzania , in Tanzania , Japan and Japan are enemies of us .', 'I cannot speak specifically about my job as being possible away from North Korea . What can I say in awe of those years because of my hunger as I m leaving Chinese to live with a long way to live away from a long line away', 'Although I d been wondering , I knew the life of the world outside , but I still think I d lived all my life at the <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 664m 27s (- -664m 52s), Epoch: [19/20], Step: [500/2015], Train Loss: 0.8462153422832489\n",
            "Time: 666m 12s (- -666m 54s), Epoch: [19/20], Step: [600/2015], Train Loss: 0.8608196163177491\n",
            "Time: 667m 58s (- -668m 58s), Epoch: [19/20], Step: [700/2015], Train Loss: 0.8780097371339798\n",
            "Time: 669m 45s (- -669m 4s), Epoch: [19/20], Step: [800/2015], Train Loss: 0.8745519310235977\n",
            "Time: 671m 31s (- -671m 13s), Epoch: [19/20], Step: [900/2015], Train Loss: 0.8908000582456589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.078162142621448 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> , and what happened to a man in North Korea , is , but I soon realized it was difficult , because the refugee people were difficult , because these refugees were failing from North Korea <UNK> were declared migrant migrant migrant migrant .', 'And there was a <UNK> when she got these lines , all my family in town who were there in his own five , because both girls had nothing to eat in two weeks .', 'In school , we spent a lot of time teaching mathematics playing the World War II , but not yet , more of the United States of the outside world , except in South Korea , in South Korea , in South Korea , in South Korea', 'I cannot speak specifically about my job as being possible away from North Korea .', 'Even though I had actually wondered the world outside , but I still think that I would live the lives of <UNK> <UNK> , until everything suddenly was changing the lives of <UNK> , until everything suddenly was changing .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 673m 26s (- -673m 13s), Epoch: [19/20], Step: [1000/2015], Train Loss: 0.8930481308698655\n",
            "Time: 675m 15s (- -675m 21s), Epoch: [19/20], Step: [1100/2015], Train Loss: 0.90189009308815\n",
            "Time: 677m 0s (- -677m 33s), Epoch: [19/20], Step: [1200/2015], Train Loss: 0.908477395772934\n",
            "Time: 678m 48s (- -679m 42s), Epoch: [19/20], Step: [1300/2015], Train Loss: 0.9080121779441833\n",
            "Time: 680m 34s (- -681m 54s), Epoch: [19/20], Step: [1400/2015], Train Loss: 0.9138901251554489\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.30583561170699 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I had <UNK> , so in a refugee from North Korea , it would be , but I realized it was difficult , because the refugee people from North Korea <UNK> , because these were <UNK> prisoners from North Korea , were considered a great deal of', 'And there was a kid who said she read that these <UNK> , all my family in five , you didn t have any more than to eat in two weeks .', 'In school , we spend a lot of time learning about the work of the Cold War II , but not in many of the world outside War II , South Korea , South Korea , South Korea and Japan are our enemies .', 'I cannot speak specifically about my own , how did I get out of the North , and just say in awe of those years because of hunger I m sent to live with a <UNK> person .', 'Although I d been wondering , I knew the <UNK> world outside , but I still think that I d lived even at <UNK> <UNK> , until everything changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 682m 31s (- -683m 55s), Epoch: [19/20], Step: [1500/2015], Train Loss: 0.9314822494983673\n",
            "Time: 684m 18s (- -684m 7s), Epoch: [19/20], Step: [1600/2015], Train Loss: 0.9394758558273315\n",
            "Time: 686m 4s (- -686m 19s), Epoch: [19/20], Step: [1700/2015], Train Loss: 0.9424504691362381\n",
            "Time: 687m 50s (- -688m 32s), Epoch: [19/20], Step: [1800/2015], Train Loss: 0.9368116080760955\n",
            "Time: 689m 37s (- -690m 44s), Epoch: [19/20], Step: [1900/2015], Train Loss: 0.9503107053041459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation Score: 19.287486481956538 \n",
            " source sentence ['Tôi đã khôngtưởng được những gì xảy đến với cuộcsống của một người tịnạn từ Bắc TriềuTiên thì sẽ như thếnào , nhưng tôi sớm nhận ra rằng nó khôngnhững rất khókhăn , màcòn vôcùng nguyhiểm , vì những người tịnạn từ Bắc TriềuTiên vào TrungQuốc đều bị coi là dân nhậpcư tráiphép .', 'Trong đó có viết Khi chị đọc được những dòng này thì cả giađình 5 người của em đã không còn trên cõiđời này nữa , bởivì cả nhà em đã không có gì để ăn trong hai tuần .', 'Ở trường , chúngtôi dành rất nhiều thờigian để học về cuộcđời của chủtịch Kim II <UNK> , nhưng lại không học nhiều về thếgiới bênngoài , ngoạitrừ việc HoaKỳ , HànQuốc và NhậtBản là kẻthù của chúngtôi .', 'Tôi khôngthể nói cụthể về việc mình đã trốn khỏi Bắc TriềuTiên như thếnào chỉ cóthể nói rằng trong những nămtháng <UNK> vì nạn đói ấy tôi được gửi sang TrungQuốc để sống với một người họhàng xa .', 'Mặcdù tôi đã từng tựhỏi không biết thếgiới bênngoài kia như thếnào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộcđời ở <UNK> <UNK> , cho tới khi tấtcả mọi thứ độtnhiên thayđổi .'] \n",
            " predicted sentence ['I was kind of going off to what happened to life of a refugee from North Korea s degree of <UNK> , but I realized it was difficult not that it was difficult , because the refugee people were coming from North Korea <UNK> and <UNK> were', 'And so there was a <UNK> when she got these lines and my family , and my five years wasn t that my whole family was not going to have anything to eat in two weeks .', 'In school , we spent a lot of time teaching mathematics playing the World War II , but not in many of the past the outside world , except the United States , and Japan was our enemies .', 'I can t tell how I escaped from North Korea , how did I get out of the North , and only in those years because of my hunger I m sent to live Chinese with a <UNK> person .', 'And yet I knew I had no wonder knowing how the outer world was , but I was thinking that I was going to live lives in <UNK> <UNK> , until everything suddenly changed .'] \n",
            " Reference sentence: ['I had no idea what life was going to be like as a North Korean refugee , but I soon learned it s not only extremely difficult , it s also very dangerous , since North Korean refugees are considered in China as illegal migrants .', 'It read , When you read this , all five family members will not exist in this world , because we haven t eaten for the past two weeks .', 'In school , we spent a lot of time studying the history of Kim <UNK> <UNK> , but we never learned much about the outside world , except that America , South Korea , Japan are the enemies .', 'I can t reveal many details 91 about 93 how I left North Korea , but I only can say that during the ugly years of the famine I was sent to China to live with distant relatives .', 'Although I often wondered about the outside world , I thought I would spend my entire life in North Korea , until everything suddenly changed .']\n",
            "Time: 691m 34s (- -692m 46s), Epoch: [19/20], Step: [2000/2015], Train Loss: 0.9491790747642517\n",
            "plot_losses: [6.619141621589661, 5.680650668144226, 5.354693050384522, 5.147682757377624, 5.000887427330017, 4.875171036720276, 4.784134893417359, 4.674350028038025, 4.570793943405151, 4.425696096420288, 4.341190090179444, 4.23148211479187, 4.1684769344329835, 4.08700380563736, 4.019237139225006, 3.9547698307037353, 3.8860162687301636, 3.842606201171875, 3.771799964904785, 3.73540559053421, 3.562603545188904, 3.4861389136314394, 3.486001591682434, 3.4452125430107117, 3.4079955887794493, 3.379240102767944, 3.342082092761993, 3.3137521028518675, 3.280788285732269, 3.2668155312538145, 3.2598270320892335, 3.201135313510895, 3.1885945987701416, 3.1689483237266542, 3.159568283557892, 3.126814968585968, 3.100325117111206, 3.102081768512726, 3.0608720707893373, 3.0492928194999696, 2.778776776790619, 2.733776319026947, 2.7686165308952333, 2.7396368527412416, 2.7224786949157713, 2.7188721823692323, 2.7184589552879332, 2.71845632314682, 2.715833249092102, 2.705645899772644, 2.7103427267074585, 2.665152521133423, 2.662909860610962, 2.667311899662018, 2.669655153751373, 2.6586928224563597, 2.668819844722748, 2.641269471645355, 2.634045372009277, 2.6200781583786013, 2.344979348182678, 2.3078741335868838, 2.3485884404182436, 2.3413527703285215, 2.341391797065735, 2.347625362873077, 2.334087470769882, 2.358257761001587, 2.3602662563323973, 2.345759756565094, 2.344395673274994, 2.3665045428276064, 2.35975017786026, 2.3527770495414733, 2.3432939720153807, 2.3417982840538025, 2.3487310552597047, 2.3517938685417175, 2.362763285636902, 2.3371864342689515, 2.0617164266109467, 2.049697074890137, 2.0705888831615447, 2.067497662305832, 2.0953902685642243, 2.095082380771637, 2.104841389656067, 2.108525266647339, 2.115828613042831, 2.0983297526836395, 2.1051673281192778, 2.096990587711334, 2.1259265625476838, 2.1130559027194975, 2.1166273653507233, 2.098463340997696, 2.132683435678482, 2.1193626773357392, 2.1295483899116516, 2.137136303186417, 1.8565235352516174, 1.8548520910739899, 1.8792708277702332, 1.8623106956481934, 1.8781508362293244, 1.8950611221790314, 1.8983384335041047, 1.8930661845207215, 1.9216615200042724, 1.9328264999389648, 1.919315801858902, 1.9148978328704833, 1.9321985995769502, 1.9459702718257903, 1.9551568865776061, 1.929237940311432, 1.953650802373886, 1.966584758758545, 1.951790715456009, 1.9537723827362061, 1.7028202176094056, 1.6986923587322236, 1.695651432275772, 1.716880089044571, 1.7196457016468047, 1.727235929965973, 1.7599261558055879, 1.7705839931964875, 1.7537607955932617, 1.7708887457847595, 1.7728872978687287, 1.788774688243866, 1.7851657092571258, 1.7922867512702942, 1.7872923040390014, 1.8038247096538544, 1.8013761413097382, 1.7810795712471008, 1.8232702660560607, 1.8078478348255158, 1.5788678085803987, 1.5577323305606843, 1.5799263536930084, 1.5869789838790893, 1.591220144033432, 1.6094828402996064, 1.612627568244934, 1.6159301161766053, 1.6293412172794342, 1.6347821176052093, 1.6579421961307526, 1.6465863192081451, 1.6662315845489502, 1.667303183078766, 1.6734172010421753, 1.6837221813201904, 1.6577831280231476, 1.682752501964569, 1.688077710866928, 1.6885614478588105, 1.4541558527946472, 1.4559979665279388, 1.4432102715969086, 1.4765755999088288, 1.4979748916625977, 1.4710227572917938, 1.4991789805889129, 1.5151822185516357, 1.5325608706474305, 1.519618537425995, 1.5309972202777862, 1.5308700954914094, 1.5576044404506684, 1.5581949520111085, 1.5728148460388183, 1.5736285483837127, 1.5810494637489318, 1.5566812646389008, 1.5946787548065187, 1.603601177930832, 1.3647971260547638, 1.3588657760620118, 1.3754769384860992, 1.3778290963172912, 1.3856454658508301, 1.4008194720745086, 1.4057805943489075, 1.4246281492710113, 1.433928142786026, 1.4290867161750793, 1.43723428606987, 1.4574389481544494, 1.4441010534763337, 1.4569580972194671, 1.4633824741840362, 1.4750519907474517, 1.4830340957641601, 1.4740937745571137, 1.4846405971050263, 1.4874945342540742, 1.2803919172286988, 1.270534497499466, 1.2861195051670073, 1.2980123114585878, 1.3017594599723816, 1.3210815322399139, 1.3173403429985047, 1.3402852249145507, 1.3310812723636627, 1.3384831857681274, 1.3517868900299073, 1.3771527576446534, 1.3710214865207673, 1.3743843924999237, 1.3715105068683624, 1.3850115275382995, 1.3971905052661895, 1.4074196565151214, 1.4060429728031159, 1.3928918635845184, 1.1966431587934494, 1.198677053451538, 1.2160479140281677, 1.217797999382019, 1.2254979568719864, 1.2341789400577545, 1.2458624839782715, 1.2484596741199494, 1.2596959030628205, 1.2909837424755097, 1.2832951354980469, 1.282934432029724, 1.2932122611999513, 1.3094306945800782, 1.3061111044883729, 1.3051951551437377, 1.3101040530204773, 1.3240605247020723, 1.3275971806049347, 1.3365359044075011, 1.1317982417345047, 1.1283133465051651, 1.144743385910988, 1.1574341213703156, 1.166254215836525, 1.1676698875427247, 1.1856655335426332, 1.1907345521450043, 1.1952047669887542, 1.1920952165126801, 1.2134984648227691, 1.2039953565597534, 1.2243910360336303, 1.2277117669582367, 1.2444627785682678, 1.2544583976268768, 1.2384538125991822, 1.2514498543739319, 1.2653695464134216, 1.266369367837906, 1.0771349084377289, 1.0768746554851532, 1.0786463844776153, 1.0847132247686386, 1.0968105632066727, 1.1058124971389771, 1.1172779703140259, 1.128718490600586, 1.1289762383699418, 1.1466530978679657, 1.1516231000423431, 1.1583639776706696, 1.162167848944664, 1.1651501476764679, 1.1662205207347869, 1.1798181706666946, 1.177376961708069, 1.2049704551696778, 1.1935699737071992, 1.1994604063034058, 1.0256272649765015, 1.016500694155693, 1.0240982788801194, 1.0425837457180023, 1.0330818921327591, 1.0517603093385697, 1.0554947292804717, 1.07287029504776, 1.0862455934286117, 1.0919886207580567, 1.0880117952823638, 1.0976157772541046, 1.1085583406686783, 1.110286311507225, 1.1180457311868668, 1.1102939385175705, 1.1238849884271622, 1.1205213755369186, 1.1373492884635925, 1.1493624395132065, 0.9637722766399384, 0.9707902467250824, 0.985478184223175, 0.9792842918634415, 0.9981485533714295, 1.0104245352745056, 1.0134312742948532, 1.0052074098587036, 1.030664069056511, 1.0216612654924393, 1.0307461977005006, 1.043236049413681, 1.0465580159425736, 1.054484602212906, 1.065860338807106, 1.052193101644516, 1.0624741929769517, 1.0811631059646607, 1.0921888011693954, 1.0966322916746138, 0.9203345298767089, 0.9083132916688919, 0.9210938769578934, 0.9283190816640854, 0.9450959467887878, 0.9438889718055725, 0.9621766060590744, 0.9732212197780609, 0.9726406127214432, 0.978558498620987, 0.9772282874584198, 1.002185475230217, 1.0019132256507874, 1.0063286679983139, 1.0201069366931916, 1.023816528916359, 1.033327819108963, 1.036825453042984, 1.0390199398994446, 1.0325741153955459, 0.8713813960552216, 0.8731583619117737, 0.8852085411548615, 0.8930940163135529, 0.9012843263149262, 0.900028076171875, 0.92364770591259, 0.9159180790185928, 0.925900535583496, 0.9332020634412765, 0.9414683157205581, 0.9502524375915528, 0.9455750477313996, 0.9784943777322769, 0.9598257005214691, 0.9760297030210495, 0.9826265573501587, 0.9917564535140991, 0.9873139816522598, 0.9908147156238556, 0.8273465424776077, 0.8207612466812134, 0.8382474106550216, 0.854357956647873, 0.8462153422832489, 0.8608196163177491, 0.8780097371339798, 0.8745519310235977, 0.8908000582456589, 0.8930481308698655, 0.90189009308815, 0.908477395772934, 0.9080121779441833, 0.9138901251554489, 0.9314822494983673, 0.9394758558273315, 0.9424504691362381, 0.9368116080760955, 0.9503107053041459, 0.9491790747642517]\n",
            "validation_scores: [3.61856925760017, 7.007239430993643, 8.60345393722872, 11.345141070743452, 12.912563664463216, 14.413157602219174, 15.239655385296158, 16.318305424244944, 17.655139184606092, 17.53099017656127, 17.716875052278873, 19.17102691009252, 19.37477848851518, 18.630040302764606, 20.172401002500287, 20.20122717232046, 19.103048509897953, 20.12001854734058, 20.429060830741058, 20.535279395010622, 20.344630418020166, 20.34086018427795, 20.337118859501036, 20.726164272913383, 20.32138746461635, 20.429517050007647, 20.492243093958223, 20.95198221733016, 20.75933936158385, 20.96892625408528, 20.622852791504858, 20.913059310120946, 20.8204747842089, 21.01076791534983, 20.64128421707714, 20.820391228298924, 20.393057728241306, 20.5932495949785, 20.247145853502477, 20.37393989736967, 20.662545584619938, 20.032047031084147, 19.96555524601572, 20.904359010168914, 19.797170018112023, 20.31708449573569, 20.276212316829994, 20.33572077292906, 20.27641499216065, 20.216380737321632, 20.354874479547842, 20.17697708322979, 19.758027813975364, 20.547064444942563, 20.00396122200077, 19.889402490340114, 19.826998127852992, 19.80718103706736, 19.623789182629668, 20.054036834532578, 19.89549220765484, 19.583525370316547, 19.90490636793742, 19.81899936107871, 19.46388975441724, 19.461308456188345, 19.286918080594983, 19.91074921071965, 19.478388755972986, 19.685447468788432, 19.542868504897584, 19.47578202276559, 19.236260797465214, 19.078162142621448, 19.30583561170699, 19.287486481956538]\n",
            "Time: 693m 35s (- -687m 20s), Epoch: [20/20], Step: [100/2015], Train Loss: 0.7903525459766388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-0724401ace84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m trainIters(encoder1, decoder1, n_iters=n_epochs, print_every=print_every, plot_every=plot_every, evaluate_every=evaluate_every, learning_rate_encoder=lr_rate_en, learning_rate_decoder=lr_rate_de, lr_decay=lr_decay, gamma_encoder=gamma_decoder,\n\u001b[0;32m----> 8\u001b[0;31m           gamma_decoder=gamma_decoder)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-ff73ea150991>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, lr_decay, gamma_encoder, gamma_decoder, print_every, plot_every, learning_rate_encoder, learning_rate_decoder, evaluate_every)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             loss = train(input_tensor, target_tensor, len1, len2, encoder,\n\u001b[0;32m---> 30\u001b[0;31m                          decoder, encoder_optimizer, decoder_optimizer)\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-b87157332c9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, input_lengths, target_lengths, encoder, decoder, encoder_optimizer, decoder_optimizer, clip)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0;32m---> 32\u001b[0;31m                     decoder_input, decoder_hidden, encoder_outputs)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-e4bafff9dafe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Get current hidden state from input word and last hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Calculate attention from current RNN state and all encoder outputs;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hx, batch_sizes)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mbatch_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvariable_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             dropout_ts)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8_R-uE7bf0nM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Perform bean-search  (only a small part is shown because some experiments were run on other matchines)"
      ]
    },
    {
      "metadata": {
        "id": "dZRY9Qa4qGsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1261
        },
        "outputId": "e40508f5-bc6b-4f2e-acc3-eaa70b33d928"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "n_layers = 1\n",
        "teacher_forcing_ratio = 1\n",
        "n_iters = 20\n",
        "source_vocab_size = 19000\n",
        "target_vocab_size = 22000\n",
        "\n",
        "\n",
        "encoder_current = EncoderRNN(input_lang.n_words, hidden_size,n_layers=n_layers).to(device)\n",
        "decoder_current = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, dropout=dropout_p, n_layers=n_layers).to(device)\n",
        "checkpoint = torch.load(\"/content/drive/My Drive/saved_model/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                    .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size, target_vocab_size,lr_decay,teacher_forcing_ratio))\n",
        "encoder_current.load_state_dict(checkpoint['encoder'])\n",
        "decoder_current.load_state_dict(checkpoint['decoder'])\n",
        "encoder_current.eval()\n",
        "decoder_current.eval()\n",
        "\n",
        "test_score_greedy, _ = test_model(encoder_current, decoder_current, val_loader)\n",
        "#greedy_score_vocab.append(test_score_greedy)\n",
        "beam_list = []\n",
        "for beam_size in range(2,15):\n",
        "    print(\"beam_size: \",beam_size)\n",
        "    test_score_beam, _ = test_model(encoder_current, decoder_current, val_loader, search_method='beam')\n",
        "    print('beam score:', test_score_beam)\n",
        "    beam_list.append(test_score_beam)\n",
        "torch.save({'beam_score': beam_list,'greedy_score': test_score_greedy},\n",
        "           \"/content/drive/My Drive/saved_scores/val_scores/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size,\n",
        "                        target_vocab_size,lr_decay,teacher_forcing_ratio))   \n",
        "\n",
        "#beam_score_vocab.append(max(beam_list))\n",
        "print('greedy score', test_score_greedy)\n",
        "print ('best beam score', max(beam_list))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam_size:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.399772061300844\n",
            "beam_size:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.885072497554653\n",
            "beam_size:  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.546221110841913\n",
            "beam_size:  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.65936273827937\n",
            "beam_size:  6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.456229301064628\n",
            "beam_size:  7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.496652097545763\n",
            "beam_size:  8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.44000686678932\n",
            "beam_size:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.41382166796132\n",
            "beam_size:  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.405179069327094\n",
            "beam_size:  11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.450878508909064\n",
            "beam_size:  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.497319801972438\n",
            "beam_size:  13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.354444615665944\n",
            "beam_size:  14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 22.248742513670873\n",
            "greedy score 21.01076791534983\n",
            "best beam score 22.885072497554653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZLdxIWmg6WG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        },
        "outputId": "d1f8155a-034d-4180-9a63-51d834cb887b"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 512\n",
        "n_layers = 4\n",
        "teacher_forcing_ratio = 1\n",
        "n_iters = 20\n",
        "source_vocab_size = 19000\n",
        "target_vocab_size = 22000\n",
        "\n",
        "\n",
        "encoder_current = EncoderRNN(input_lang.n_words, hidden_size,n_layers=n_layers).to(device)\n",
        "decoder_current = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, dropout=dropout_p, n_layers=n_layers).to(device)\n",
        "checkpoint = torch.load(\"/content/drive/My Drive/saved_model/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                    .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size, target_vocab_size,lr_decay,teacher_forcing_ratio))\n",
        "encoder_current.load_state_dict(checkpoint['encoder'])\n",
        "decoder_current.load_state_dict(checkpoint['decoder'])\n",
        "encoder_current.eval()\n",
        "decoder_current.eval()\n",
        "\n",
        "test_score_greedy, _ = test_model(encoder_current, decoder_current, val_loader)\n",
        "#greedy_score_vocab.append(test_score_greedy)\n",
        "beam_list = []\n",
        "for beam_size in range(2,15):\n",
        "    print(\"beam_size: \",beam_size)\n",
        "    test_score_beam, _ = test_model(encoder_current, decoder_current, val_loader, search_method='beam')\n",
        "    print('beam score:', test_score_beam)\n",
        "    beam_list.append(test_score_beam)\n",
        "torch.save({'beam_score': beam_list,'greedy_score': test_score_greedy},\n",
        "           \"/content/drive/My Drive/saved_scores/val_scores/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size,\n",
        "                        target_vocab_size,lr_decay,teacher_forcing_ratio))   \n",
        "\n",
        "#beam_score_vocab.append(max(beam_list))\n",
        "print('greedy score', test_score_greedy)\n",
        "print ('best beam score', max(beam_list))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam_size:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.0200259980087\n",
            "beam_size:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.072811265650447\n",
            "beam_size:  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.99351362572711\n",
            "beam_size:  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.02216066738348\n",
            "beam_size:  6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.11758900618767\n",
            "beam_size:  7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.080467559019997\n",
            "beam_size:  8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.916414433116906\n",
            "beam_size:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.95214562843179\n",
            "beam_size:  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.025171392615462\n",
            "beam_size:  11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.115452745514105\n",
            "beam_size:  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.024113695683727\n",
            "beam_size:  13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.967285503023753\n",
            "beam_size:  14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 25.006884301677115\n",
            "greedy score 24.23407561373281\n",
            "best beam score 25.11758900618767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wsbN85nY6PmH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        },
        "outputId": "cd26b798-00db-4488-e762-d6706a338518"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "n_layers = 2\n",
        "teacher_forcing_ratio = 1\n",
        "n_iters = 20\n",
        "source_vocab_size = 19000\n",
        "target_vocab_size = 22000\n",
        "\n",
        "\n",
        "encoder_current = EncoderRNN(input_lang.n_words, hidden_size,n_layers=n_layers).to(device)\n",
        "decoder_current = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, dropout=dropout_p, n_layers=n_layers).to(device)\n",
        "checkpoint = torch.load(\"/content/drive/My Drive/saved_model/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                    .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size, target_vocab_size,lr_decay,teacher_forcing_ratio))\n",
        "encoder_current.load_state_dict(checkpoint['encoder'])\n",
        "decoder_current.load_state_dict(checkpoint['decoder'])\n",
        "encoder_current.eval()\n",
        "decoder_current.eval()\n",
        "\n",
        "test_score_greedy, _ = test_model(encoder_current, decoder_current, val_loader)\n",
        "#greedy_score_vocab.append(test_score_greedy)\n",
        "beam_list = []\n",
        "for beam_size in range(2,15):\n",
        "    print(\"beam_size: \",beam_size)\n",
        "    test_score_beam, _ = test_model(encoder_current, decoder_current, val_loader, search_method='beam')\n",
        "    print('beam score:', test_score_beam)\n",
        "    beam_list.append(test_score_beam)\n",
        "torch.save({'beam_score': beam_list,'greedy_score': test_score_greedy},\n",
        "           \"/content/drive/My Drive/saved_scores/val_scores/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size,\n",
        "                        target_vocab_size,lr_decay,teacher_forcing_ratio))   \n",
        "\n",
        "#beam_score_vocab.append(max(beam_list))\n",
        "print('greedy score', test_score_greedy)\n",
        "print ('best beam score', max(beam_list))\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam_size:  2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.20619219216039\n",
            "beam_size:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.8231000567948\n",
            "beam_size:  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.766487520543293\n",
            "beam_size:  5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.700467929617023\n",
            "beam_size:  6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.740932906564048\n",
            "beam_size:  7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.853968026223058\n",
            "beam_size:  8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.749179468647075\n",
            "beam_size:  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.56300847004859\n",
            "beam_size:  10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.567635148335448\n",
            "beam_size:  11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.65821253999541\n",
            "beam_size:  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.660476419760016\n",
            "beam_size:  13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.540302073934885\n",
            "beam_size:  14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "beam score: 24.562848997472386\n",
            "greedy score 22.94583172963565\n",
            "best beam score 24.853968026223058\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A3uXnn0ABMRI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plot beam on validation set"
      ]
    },
    {
      "metadata": {
        "id": "jiYyZEE3-UgU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = torch.load(\n",
        "           \"/content/drive/My Drive/saved_scores/val_scores/LSTM_attnIsTrue_hiddenSize512_nLayer2_batchSize64_epoch20_srcVocSize19000_tgtVocSize22000_lrDecayFalse_teacherF1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UJkG2y63Cbw8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "0c38d206-96d6-4952-adfa-9ed115902be6"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(range(1,15),[score['greedy_score']]+score['beam_score'])\n",
        "plt.xlabel('beam width')\n",
        "plt.ylabel('BLEU score')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0,0.5,'BLEU score')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8f35671cc0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "NLszAj0oCupK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Plot training loss for GRU/LSTM to compare speed"
      ]
    },
    {
      "metadata": {
        "id": "kteP61XhBapr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score1 = torch.load(\n",
        "           \"/content/drive/My Drive/saved_scores/LSTM_attnIsTrue_hiddenSize512_nLayer2_batchSize64_epoch20_srcVocSize19000_tgtVocSize22000_lrDecayFalse_teacherF1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bK_0qylOT9kA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score2 = torch.load(\n",
        "           \"/content/drive/My Drive/saved_scores/attnIsTrue_hiddenSize512_nLayer2_batchSize64_epoch20_srcVocSize20000_tgtVocSize32000_lrDecayFalse_teacherF1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fzv0j3zuFDx4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This is the loss of each epoch from self-attention\n",
        "score3 = [4.545370580658081,\n",
        " 3.537611612251827,\n",
        " 3.0360496749953616,\n",
        " 2.672862425115373,\n",
        " 2.394343628107555,\n",
        " 2.1719146188289393,\n",
        " 1.9884819965513922,\n",
        " 1.8350102686692797,\n",
        " 1.707337155985454,\n",
        " 1.5958151286556606,\n",
        " 1.4975455459148164,\n",
        " 1.4124447373170703,\n",
        " 1.3285594105247467,\n",
        " 1.2610603803207003,\n",
        " 1.1966657795603315]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3tqfVcOxC1J_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "0a187f96-f221-49dd-c566-c504e0192458"
      },
      "cell_type": "code",
      "source": [
        "CHAR_GRU_H512_L4 = score1['plot_losses']\n",
        "CHAR_LSTM_H512_L4 = score2['plot_losses']\n",
        "\n",
        "def average_loss(losses):\n",
        "    average_losses = []\n",
        "    for i in range(0,320,20):\n",
        "        average_losses.append(np.mean(losses[i:i+20]))\n",
        "    return average_losses\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "# this locator puts ticks at regular intervals\n",
        "loc = ticker.MultipleLocator(base=1)\n",
        "ax.xaxis.set_major_locator(loc)\n",
        "ax.set_xlim(xmin = 0,xmax = 14)\n",
        "plt.plot(average_loss(score1['plot_losses']), marker = '*',label = 'GRU_H512_L2')\n",
        "plt.plot(average_loss(score2['plot_losses']), marker = '*',label = 'LSTM_H512_L2')\n",
        "plt.plot(score3, marker='*', label='self_attention')\n",
        "\n",
        "plt.xlabel(\"Epoches\")\n",
        "\n",
        "plt.title(\"Training Loss Over Steps\")\n",
        "plt.legend()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8ee8c8fd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmLtAIBDMQIS5CwQCwQzk/wOsEsCxSAtrYwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8ee8f73898>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "W5lBux2pD-Dl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluate on test set"
      ]
    },
    {
      "metadata": {
        "id": "fvTiCHSoD-tc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e55653b9-799d-4612-dae0-1e6afff0ff0e"
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 1024\n",
        "n_layers = 2\n",
        "teacher_forcing_ratio = 1\n",
        "n_iters = 20\n",
        "source_vocab_size = 19000\n",
        "target_vocab_size = 22000\n",
        "beam_size = 7\n",
        "\n",
        "encoder_current = EncoderRNN(input_lang.n_words, hidden_size,n_layers=n_layers).to(device)\n",
        "decoder_current = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, dropout=dropout_p, n_layers=n_layers).to(device)\n",
        "checkpoint = torch.load(\"/content/drive/My Drive/saved_model/LSTM_attnIs{}_hiddenSize{}_nLayer{}_batchSize{}_epoch{}_srcVocSize{}_tgtVocSize{}_lrDecay{}_teacherF{}\"\\\n",
        "                    .format(Attention,hidden_size,n_layers,BATCH_SIZE,n_iters,source_vocab_size, target_vocab_size,lr_decay,teacher_forcing_ratio))\n",
        "encoder_current.load_state_dict(checkpoint['encoder'])\n",
        "decoder_current.load_state_dict(checkpoint['decoder'])\n",
        "encoder_current.eval()\n",
        "decoder_current.eval()\n",
        "\n",
        "test_score_beam, predicted_sentence = test_model(encoder_current, decoder_current, test_loader, search_method='beam')\n",
        "\n",
        "print ('test score', test_score_beam)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:root:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:root:If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test score 21.309190297919464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jnaut8XNEKJ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}