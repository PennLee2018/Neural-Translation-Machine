{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lang1_lines = open('data/iwslt-zh-en/train.tok.zh', encoding = 'utf-8').read().\\\n",
    "                    strip().split('\\n')\n",
    "    lang2_lines = open('data/iwslt-zh-en/train.tok.en', encoding = 'utf-8').read().\\\n",
    "                    strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [(lang1_lines[i] +',' + lang2_lines[i]).split(',',1) for i in range(len(lang1_lines))]\n",
    "\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 213376 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 88917\n",
      "chi 69125\n",
      "['这 是 次 连接 导线 的 飞行   它 拥有 翅膀  ', 'This was a fly by wire . It has wings .']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'chi')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "# emb_size = 256\n",
    "# max(len(pair[1].split(\" \")) for pair in pairs)\n",
    "MAX_LENGTH = max(len(pair[0].split(\" \")) for pair in pairs)\n",
    "dropout_p = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(2, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "#     def initHidden(self):\n",
    "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    \n",
    "    encoder_hidden = ((encoder_hidden[0] + encoder_hidden[1])/2).unsqueeze(0)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    print(plot_losses)\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "#     with torch.no_grad():\n",
    "#         input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "#         input_length = input_tensor.size()[0]\n",
    "#         encoder_hidden = encoder.initHidden()\n",
    "\n",
    "#         encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
    "\n",
    "#         for ei in range(input_length):\n",
    "#             encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "#                                                      encoder_hidden)\n",
    "#             encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "#         decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        \n",
    "#         decoder_hidden = encoder_hidden\n",
    "\n",
    "#         decoded_words = []\n",
    "#         decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "#         for di in range(max_length):\n",
    "#             decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "#                 decoder_input, decoder_hidden, encoder_outputs)\n",
    "#             decoder_attentions[di] = decoder_attention.data\n",
    "#             topv, topi = decoder_output.data.topk(1)\n",
    "#             if topi.item() == EOS_token:\n",
    "#                 decoded_words.append('<EOS>')\n",
    "#                 break\n",
    "#             else:\n",
    "#                 decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "#             decoder_input = topi.squeeze().detach()\n",
    "\n",
    "#         return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def evaluateRandomly(encoder, decoder, n=10):\n",
    "#     for i in range(n):\n",
    "#         pair = random.choice(pairs)\n",
    "#         print('>', pair[0])\n",
    "#         print('=', pair[1])\n",
    "#         output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "#         output_sentence = ' '.join(output_words)\n",
    "#         print('<', output_sentence)\n",
    "#         print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 0m 7s) (1 25%) 11.1166\n",
      "0m 4s (- 0m 4s) (2 50%) 11.0986\n",
      "0m 7s (- 0m 2s) (3 75%) 11.0920\n",
      "0m 9s (- 0m 0s) (4 100%) 11.0614\n",
      "[11.116584190955528, 11.098579915364583, 11.09195533165565, 11.061416625976562]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11188e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD8CAYAAABAWd66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11188e7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(input_lang.n_words,hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 4, print_every=1,plot_every=1)\n",
    "\n",
    "torch.save(encoder1.state_dict(), \"saved_model/encoder_hiddenSize{}\".format(hidden_size))\n",
    "torch.save(attn_decoder1.state_dict(), \"saved_model/attn_decoder_hiddenSize{}\".format(hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
